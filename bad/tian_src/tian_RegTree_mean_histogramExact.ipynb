{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os \n",
    "import sys \n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "from datetime  import datetime \n",
    "\n",
    "import numpy as np  # learn \n",
    "import pandas as pd # learn\n",
    "from pandas import *\n",
    "from numpy import *\n",
    "\n",
    "from scipy import stats # look at scipy\n",
    "from scipy import linalg\n",
    "from scipy import *\n",
    "\n",
    " \n",
    "import matplotlib as mplt # learn matplolib \n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "import seaborn as sns \n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc={\"figure.figsize\": (14, 6)})\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import sklearn as sk\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import random\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#BT data\n",
    "\n",
    "# dta_RDD = sc.textFile(\"file:///home/tguo/data/tian-test/udpjitter-H1april2014_regTree.csv\")\n",
    "\n",
    "dta_RDD = sc.textFile(\"hdfs://computer61.ant-net/user/tguo/udpjitter-H1april2014_regTree_mllib.csv\")\n",
    "\n",
    "\n",
    "dta_splited = dta_RDD.map(lambda line: line.split(\",\")).map(lambda r:\n",
    "                                                                (float(r[0]),int( float(r[1])), \n",
    "                                                                 int(float(r[2])),int(float(r[3])),\n",
    "                                                                 int(float(r[4])),int(float(r[5])),\n",
    "                                                                 int(float(r[6])),int(float(r[7])) ))\n",
    "# dta_splited.first()\n",
    "\n",
    "dta= dta_splited\n",
    "b\n",
    "dta.cache()\n",
    "dta.first()\n",
    "\n",
    "# print dta.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dta_withNodeIdx = dta_splited.map( lambda r: (0, [r[0], r[1], r[2],r[3],r[4],r[5],r[6],r[7] ] ))\n",
    "\n",
    "# print dta_withNodeIdx.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for test: randomly assign node index to data points and then aggregate the information of each node\n",
    "# dta_withRandomNodeIdx = dta_splited.map( lambda r: (random.randint(0, 7), \n",
    "#                                                     [r[0], r[1], r[2],r[3],r[4],r[5],r[6],r[7] ] ))\n",
    "\n",
    "# print dta_withRandomNodeIdx.first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-14.0, 1, 3, 4, 0, 0, 0, 0)\n",
      "7009728\n"
     ]
    }
   ],
   "source": [
    "#Flight data\n",
    "\n",
    "dta_flight = sc.textFile(\"file:///home/tguo/data/tian-test/2008_flight_mllib.csv\")\n",
    "\n",
    "dta_flight = dta_flight.cache()\n",
    "# dta.first()\n",
    "\n",
    "# dta_csv.iloc[0]\n",
    "\n",
    "\n",
    "dta_splited = dta_flight.map(lambda line: line.split(\",\")).map(lambda r:\n",
    "                                                                 (float(r[0]),int( float(r[1])), \n",
    "                                                                  int(float(r[2])),int(float(r[3])),\n",
    "                                                                  int(float(r[4])),int(float(r[5])),\n",
    "                                                                  int(float(r[6])),int(float(r[7])) ))\n",
    "# # dta_splited.first()\n",
    "\n",
    "dta= dta_splited\n",
    "\n",
    "# dta.cache()\n",
    "print dta.first()\n",
    "print dta.count()\n",
    "\n",
    "\n",
    "# dta_filtered = dta.filter(lambda line: line[0] <2000 )\n",
    "# dta_filtered.cache()\n",
    "# dta_filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxdepth=5\n",
    "numFeatures=7\n",
    "# numFeatureValues = 30\n",
    "numRandomNode=63\n",
    "\n",
    "# assign data to new nodes\n",
    "# def search_nodeToData(line, tree):\n",
    "#     for i in range(0,maxdepth):\n",
    "#         if line[0]>10:\n",
    "#             tmp=line[0]\n",
    "#         else:\n",
    "#             tmp=line[0]\n",
    "#     return random.randint(0, numRandomNode)        \n",
    "\n",
    "# dta_test = dta.map( lambda line: (search_nodeToData(line, tree),(line\n",
    "#                                                                 )) ) \n",
    "\n",
    "# dta_test=node_dta.zipWithIndex(dta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tmprdd= dta.map(lambda line: line[2])\n",
    "\n",
    "# print tmprdd.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fadf\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# for debug\n",
    "a={1:2}\n",
    "# a.update({3:2})\n",
    "a.update( { 3:{1:[2]}  }  )\n",
    "a.update( { 1:{3:[2]}  }  )\n",
    "\n",
    "\n",
    "a[3][1].append(4)\n",
    "if 1 in a[3]:\n",
    "    print 'fadf'\n",
    "    \n",
    "    \n",
    "for i in a[1].keys():\n",
    "    print i\n",
    "    \n",
    "def func(tmp_dict):\n",
    "    tmop_dict.update({ 1:{ 1:[3]  }   })\n",
    "    tmop_dict.update({ 1:{ 1:[3]  }   })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# basic functions in two-pass method\n",
    "\n",
    "def ini_combiner(dvAndfeatures):\n",
    "    # each feature-value has a triple [cnt, mean, variance]\n",
    "    \n",
    "    tmp_feature={}\n",
    "    Y= dvAndfeatures[0]\n",
    "    \n",
    "    for i in range(0,numFeatures):\n",
    "        feature_val=dvAndfeatures[i+1]\n",
    "        tmp_feature.update( {i:{feature_val: [ 1, Y, 0] }   }   )\n",
    "    return tmp_feature\n",
    "\n",
    "def update_combiner(featureToValueToStat,dvAndfeatures):\n",
    "    \n",
    "    tmp_featureToValueToStat= featureToValueToStat\n",
    "    \n",
    "    Y= dvAndfeatures[0]\n",
    "    for i in range(0, numFeatures):\n",
    "        feature_val=dvAndfeatures[i+1]\n",
    "    \n",
    "        if feature_val in featureToValueToStat[i]:\n",
    "            preCnt= featureToValueToStat[i][feature_val][0]\n",
    "            preMean= featureToValueToStat[i][feature_val][1]\n",
    "            preVar= featureToValueToStat[i][feature_val][2]\n",
    "         \n",
    "            tmp_featureToValueToStat[i][feature_val][2]=featureToValueToStat[i][feature_val][2] + preCnt *1.0/ (preCnt+1)*( preMean- Y)*( preMean- Y)\n",
    "        \n",
    "            tmp_featureToValueToStat[i][feature_val][1]= (1.0* preMean * preCnt + Y )/( 1.0*preCnt+1)\n",
    "        \n",
    "            tmp_featureToValueToStat[i][feature_val][0] =  preCnt  +1\n",
    "        else:\n",
    "            tmp_featureToValueToStat[i].update( {feature_val: [ 1, Y, 0] } )\n",
    "            \n",
    "    return tmp_featureToValueToStat\n",
    "\n",
    "    \n",
    "def merge_combiner(featureToValueToStat_1, featureToValueToStat_2):  \n",
    "# merged in featureToValueToStat_1\n",
    "    \n",
    "    for i in range(0, numFeatures):\n",
    "        \n",
    "        for j in featureToValueToStat_1[i].keys():\n",
    "            \n",
    "            feature_val=j\n",
    "            \n",
    "            if feature_val in featureToValueToStat_2[i]:\n",
    "                \n",
    "                featureToValueToStat_1[i][j][2] =1.0* featureToValueToStat_1[i][j][2] + \\\n",
    "                featureToValueToStat_2[i][j][2] + \\\n",
    "                ( (1.0* featureToValueToStat_1[i][j][0]*featureToValueToStat_2[i][j][0] ) / (1.0*featureToValueToStat_1[i][j][0]+featureToValueToStat_2[i][j][0])* \\\n",
    "                 (1.0*featureToValueToStat_1[i][j][1] - featureToValueToStat_2[i][j][1])* (1.0*featureToValueToStat_1[i][j][1] - featureToValueToStat_2[i][j][1]))     \n",
    "                \n",
    "                \n",
    "                featureToValueToStat_1[i][j][1] = (featureToValueToStat_1[i][j][1]*featureToValueToStat_1[i][j][0]+ \\\n",
    "                                           featureToValueToStat_2[i][j][1]*featureToValueToStat_2[i][j][0]*1.0)\\\n",
    "                                           /(1.0* featureToValueToStat_1[i][j][0]+featureToValueToStat_2[i][j][0]  )\n",
    "                    \n",
    "                featureToValueToStat_1[i][j][0] = featureToValueToStat_1[i][j][0]+featureToValueToStat_2[i][j][0]\n",
    "    \n",
    "    \n",
    "    for i in range(0, numFeatures):\n",
    "        \n",
    "        for j in featureToValueToStat_2[i].keys():\n",
    "            \n",
    "            feature_val=j\n",
    "            \n",
    "            if feature_val not in featureToValueToStat_1[i]:\n",
    "                \n",
    "                tmp_cnt=featureToValueToStat_2[i][j][0]\n",
    "                tmp_mean=featureToValueToStat_2[i][j][1]\n",
    "                tmp_var=featureToValueToStat_2[i][j][2]\n",
    "                \n",
    "                featureToValueToStat_1[i].update({feature_val: [ tmp_cnt, tmp_mean, tmp_var] })\n",
    "                    \n",
    "    \n",
    "    return  featureToValueToStat_1\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# two-pass method \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# assign data to new nodes\n",
    "def search_nodeToData(line, tree):\n",
    "    for i in range(0,maxdepth):\n",
    "        if line[0]>10:\n",
    "            tmp=line[0]\n",
    "        else:\n",
    "            tmp=line[0]\n",
    "    return random.randint(0, numRandomNode)        \n",
    "\n",
    "dta_test = dta.map( lambda line: (search_nodeToData(line, tree),(line\n",
    "                                                                )) ) \n",
    "\n",
    "\n",
    "nodeStatis = dta_test.combineByKey(lambda line:ini_combiner(line),\n",
    "                             lambda nodeToFeatureToValue, line: (update_combiner(nodeToFeatureToValue,line)),\n",
    "                             lambda nodeToFeatureToValue_x,nodeToFeatureToValue_y: (merge_combiner( nodeToFeatureToValue_x, nodeToFeatureToValue_y))\n",
    "                            )\n",
    "\n",
    "local_NodeFeatureValue_statis = nodeStatis.collect()\n",
    "\n",
    "\n",
    "# run your code\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "\n",
    "print elapsed, 'sec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "# for debug\n",
    "\n",
    "print len(local_NodeFeatureValue_statis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# basic functions in one-pass method\n",
    "\n",
    "# assign each data instance to the node\n",
    "def search_nodeToData(features, tree):\n",
    "        \n",
    "    nodeNum=len(tree)\n",
    "    \n",
    "    if nodeNum == 0:\n",
    "        return 0;\n",
    "    \n",
    "    current_nodeIdx=0\n",
    "    while current_nodeIdx< nodeNum:\n",
    "        split_feature= tree[current_nodeIdx][0]\n",
    "        split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "        if split_feature==-1:\n",
    "            return -1\n",
    "        \n",
    "        if (( 1<<features[split_feature]) &  split_valueSet) != 0 :\n",
    "            current_nodeIdx=current_nodeIdx*2+1\n",
    "        else:\n",
    "            current_nodeIdx=current_nodeIdx*2+2\n",
    "    \n",
    "#     return random.randint(0, 2)    \n",
    "    return current_nodeIdx - nodeNum\n",
    "\n",
    "\n",
    "def partition_combiner(list_dvAndfeatures):\n",
    "    # each feature-value has a triple [cnt, mean, variance]\n",
    "    \n",
    "    nodes_dict={}\n",
    "    tmpcnt=0\n",
    "    \n",
    "    for dvAndfeatures in list_dvAndfeatures:\n",
    "        \n",
    "        Y= dvAndfeatures[0]\n",
    "        \n",
    "        node= search_nodeToData(dvAndfeatures[1:numFeatures+1], \\\n",
    "                                currentNode_split_fromMaster.value)\n",
    "        \n",
    "        if node in nodes_dict.keys():\n",
    "            \n",
    "            # new added: sum of y in a node\n",
    "            nodes_dict[node]['sumY'] = nodes_dict[node]['sumY']+Y \n",
    "            nodes_dict[node]['count'] = nodes_dict[node]['count']+1 \n",
    "            \n",
    "        \n",
    "            for i in range(0,numFeatures):\n",
    "                \n",
    "                feature_val= dvAndfeatures[i+1]\n",
    "                \n",
    "                if feature_val not in nodes_dict[node][i]:\n",
    "                    nodes_dict[node][i].update( { feature_val: [1,Y,0]   }  )\n",
    "                else:\n",
    "                    preCnt= nodes_dict[node][i][feature_val][0]\n",
    "                    preMean= nodes_dict[node][i][feature_val][1]\n",
    "                    preVar= nodes_dict[node][i][feature_val][2]\n",
    "         \n",
    "                    nodes_dict[node][i][feature_val][2]=preVar + \\\n",
    "                    preCnt *1.0/ (preCnt+1)*( preMean- Y)*( preMean- Y)\n",
    "        \n",
    "                    nodes_dict[node][i][feature_val][1]=(1.0*preMean*preCnt+Y)/(1.0*preCnt+1)\n",
    "                    nodes_dict[node][i][feature_val][0] =  preCnt  +1\n",
    "            \n",
    "        else:\n",
    "            nodes_dict.update( {node: {}})  \n",
    "            \n",
    "            # new added: sum of y in a node\n",
    "            nodes_dict[node].update( { 'sumY': Y} )\n",
    "            nodes_dict[node].update( { 'count': 1} )\n",
    "            \n",
    "            for i in range(0,numFeatures):\n",
    "                feature_val= dvAndfeatures[i+1]\n",
    "                nodes_dict[node].update( { i: {}} )    \n",
    "                nodes_dict[node][i].update( { feature_val: [1,Y,0]   }  )\n",
    "                \n",
    "    return zip(nodes_dict.keys(), nodes_dict.values())\n",
    "# , nodes_dict.values()\n",
    "\n",
    "\n",
    "def merge_parttion_combiner(nodeToFeatureToValue_1,nodeToFeatureToValue_2 ):\n",
    "    \n",
    "    # new added: sum of Y in a node\n",
    "    nodeToFeatureToValue_1['sumY']= nodeToFeatureToValue_1['sumY']+\\\n",
    "    nodeToFeatureToValue_2['sumY']\n",
    "    nodeToFeatureToValue_1['count']= nodeToFeatureToValue_1['count']+\\\n",
    "    nodeToFeatureToValue_2['count'] \n",
    "    \n",
    "    \n",
    "    for i in range(0, numFeatures): #feature\n",
    "        for j in nodeToFeatureToValue_1[i].keys(): #feature value\n",
    "            feature_val=j\n",
    "            if feature_val in nodeToFeatureToValue_2[i].keys():\n",
    "                \n",
    "                \n",
    "                nodeToFeatureToValue_1[i][j][2] =1.0* nodeToFeatureToValue_1[i][j][2] + \\\n",
    "                nodeToFeatureToValue_2[i][j][2] + \\\n",
    "                ( (1.0* nodeToFeatureToValue_1[i][j][0]*nodeToFeatureToValue_2[i][j][0] ) / (1.0*nodeToFeatureToValue_1[i][j][0]+nodeToFeatureToValue_2[i][j][0])*\\\n",
    "                 (1.0*nodeToFeatureToValue_1[i][j][1] - nodeToFeatureToValue_2[i][j][1])* (1.0*nodeToFeatureToValue_1[i][j][1] - nodeToFeatureToValue_2[i][j][1]))   \n",
    "                \n",
    "                nodeToFeatureToValue_1[i][j][1] = (nodeToFeatureToValue_1[i][j][1]*nodeToFeatureToValue_1[i][j][0]*1.0+ \\\n",
    "                                                   nodeToFeatureToValue_2[i][j][1]*nodeToFeatureToValue_2[i][j][0]*1.0)/(1.0* nodeToFeatureToValue_1[i][j][0]+1.0*nodeToFeatureToValue_2[i][j][0])\n",
    "                    \n",
    "                nodeToFeatureToValue_1[i][j][0] = nodeToFeatureToValue_1[i][j][0]+nodeToFeatureToValue_2[i][j][0]\n",
    "    \n",
    "    for i in range(0, numFeatures):\n",
    "            for j in nodeToFeatureToValue_2[i].keys():\n",
    "            \n",
    "                feature_val=j\n",
    "            \n",
    "                if feature_val not in nodeToFeatureToValue_1[i].keys():\n",
    "                    tmp_cnt=nodeToFeatureToValue_2[i][j][0]\n",
    "                    tmp_mean=nodeToFeatureToValue_2[i][j][1]\n",
    "                    tmp_var=nodeToFeatureToValue_2[i][j][2]\n",
    "                    \n",
    "                    nodeToFeatureToValue_1[i].update({feature_val: [ tmp_cnt, tmp_mean, tmp_var] })\n",
    " \n",
    "    return  nodeToFeatureToValue_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a=(1,2,3,4)\n",
    "b=(2,4,5,7)\n",
    "\n",
    "tmparr=[]\n",
    "tmparr.append(a)\n",
    "tmparr.append(b)\n",
    "# print tmparr[:][2]\n",
    "# print type(a)\n",
    "\n",
    "col=[row[1] for row in tmparr]\n",
    "print col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split decision\n",
    "\n",
    "#implementation notes:\n",
    "# bit-operation to record the set of selected feature values in the left child\n",
    "\n",
    "\n",
    "def split_onOneFeature( statis_values,sumY_node, count_node, lrcount ,lrest  ):\n",
    "    \n",
    "    value_cnt= len(statis_values)\n",
    "    \n",
    "    leftY_sum=0\n",
    "    left_count=0\n",
    "    \n",
    "    rightY_sum= sumY_node\n",
    "    right_count= count_node\n",
    "    \n",
    "    bestSplitMetric=  (sumY_node /count_node)*(sumY_node)\n",
    "    #this is a bit-record variable\n",
    "    leftSplit_valueSet =0 \n",
    "    \n",
    "    \n",
    "    #..........test\n",
    "    \n",
    "#     tmpsum=0.0\n",
    "#     for i in range(0,value_cnt):\n",
    "#         tmpsum=tmpsum+  1.0*statis_values[i][1][0]*statis_values[i][1][1]\n",
    "    \n",
    "#     if tmpsum == sumY_node:\n",
    "#         print 'correct'\n",
    "#     else:\n",
    "#         print 'problem', tmpsum- sumY_node\n",
    "    \n",
    "    \n",
    "    #..................\n",
    "    \n",
    "    \n",
    "    for i in range(0,value_cnt):\n",
    "        \n",
    "        leftY_sum = leftY_sum + statis_values[i][1][1] * statis_values[i][1][0]\n",
    "        rightY_sum= sumY_node- leftY_sum\n",
    "        \n",
    "        left_count=left_count+ statis_values[i][1][0]\n",
    "        right_count=count_node-left_count\n",
    "        \n",
    "        current_feature_value= statis_values[i][0]\n",
    "        \n",
    "        leftMetric=0\n",
    "        rightMetric=0\n",
    "        if left_count !=0 :\n",
    "            leftMetric= leftY_sum*leftY_sum/left_count\n",
    "        if right_count !=0 :\n",
    "            rightMetric= rightY_sum*rightY_sum/right_count\n",
    "        \n",
    "        current_splitMetric= leftMetric + rightMetric\n",
    "        \n",
    "        if current_splitMetric > bestSplitMetric:\n",
    "            \n",
    "            \n",
    "            # for debug\n",
    "            lrcount.append(left_count)\n",
    "            lrcount.append(right_count)\n",
    "#             lrvar[0]= left\n",
    "            \n",
    "            lrest.append(leftY_sum/left_count)\n",
    "            lrest.append(rightY_sum/right_count)        \n",
    "         \n",
    "            bestSplitMetric=current_splitMetric   \n",
    "            #bit variable to record which value is chosen for the left child\n",
    "            leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "            \n",
    "#     if value_cnt==4:\n",
    "#             print leftSplit_valueSet        \n",
    "    \n",
    "#     print (sumY_node /count_node)*(sumY_node), bestSplitMetric\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "    return (bestSplitMetric, leftSplit_valueSet)\n",
    "\n",
    "# def split_onOneNode( statis_featureSplits  ):\n",
    "    \n",
    "#     vaue_cnt= len(statis_values)\n",
    "#     tmp_var_sum=0\n",
    "#     for i in range(0,vaue_cnt):\n",
    "#         tmp_var_sum = tmp_var_sum + statis_values[i][1][2]\n",
    "                \n",
    "#     return (value, values_leftChild)\n",
    "\n",
    "def find_bestSplit(local_aggre_nodes, current_NumNodes, nodes_tree, nodes_tree_test):\n",
    "    \n",
    "#     currentNode_split_fromMaster.unpersist(blocking = True)\n",
    "    \n",
    "    #debug\n",
    "    count_layer=[]\n",
    "    \n",
    "    nodes_layer=[]\n",
    "    current_idx_nodes =0\n",
    "    \n",
    "    lr_count=[]\n",
    "    \n",
    "    \n",
    "    if local_aggre_nodes[ current_idx_nodes ][0] == -1:\n",
    "        current_idx_nodes=1\n",
    "    \n",
    "    \n",
    "    for i in range(0, current_NumNodes):\n",
    "        \n",
    "        \n",
    "        if local_aggre_nodes[ current_idx_nodes ][0] != i:\n",
    "            nodes_layer.append( (-1,-1) )\n",
    "            nodes_tree.append( (-1,-1) )\n",
    "            \n",
    "            \n",
    "            continue\n",
    "            \n",
    "        \n",
    "        tmp_splitValue_oneFeature=[]\n",
    "        tmp_sumY_node = local_aggre_nodes[i][1]['sumY'] \n",
    "        tmp_count_node = local_aggre_nodes[i][1]['count'] \n",
    "    \n",
    "    \n",
    "        best_splitMetric_feature=(tmp_sumY_node /tmp_count_node)*\\\n",
    "        (tmp_sumY_node )\n",
    "        best_split_feature=-1\n",
    "        best_split_featureValueSet=-1\n",
    "        \n",
    "        best_leftEst=0\n",
    "        best_rightEst=0\n",
    "        \n",
    "    \n",
    "        for j in range(0,numFeatures):\n",
    "            \n",
    "            satistToValues = local_aggre_nodes[i][1][j] \n",
    "        \n",
    "#         print len(satistTovalues)\n",
    "        \n",
    "            sorted_valuesOnMean = sorted(satistToValues.items(), key= \\\n",
    "                                         lambda val: val[1][1] ) \n",
    "        \n",
    "#         if i==0 and j==0:\n",
    "#             print i,j\n",
    "#             print (sorted_valuesOnMean)\n",
    "\n",
    "            #debug\n",
    "            lr_count=[]\n",
    "            lr_est=[]\n",
    "\n",
    "             \n",
    "            split=split_onOneFeature( sorted_valuesOnMean, tmp_sumY_node,\\\n",
    "                                     tmp_count_node,  lr_count,lr_est )\n",
    "    \n",
    "            if split[0] > best_splitMetric_feature:\n",
    "            \n",
    "                best_splitMetric_feature=split[0]\n",
    "                \n",
    "                best_split_feature = j\n",
    "                best_split_featureValueSet= split[1]\n",
    "                \n",
    "                best_leftEst=lr_est[0]\n",
    "                best_rightEst=lr_est[1]\n",
    "                \n",
    "        \n",
    "#         print 'node split:',(tmp_sumY_node /tmp_count_node)*(tmp_sumY_node ), best_splitMetric_feature\n",
    "    \n",
    "        # for one-layer point assignment\n",
    "        nodes_layer.append((best_split_feature,best_split_featureValueSet,best_leftEst,best_rightEst))\n",
    "        \n",
    "        # for whole-tree point assignment\n",
    "        nodes_tree.append((best_split_feature,best_split_featureValueSet,best_leftEst,best_rightEst))\n",
    "        \n",
    "        \n",
    "        nodes_tree_test.append(best_leftEst)\n",
    "        nodes_tree_test.append(best_rightEst)\n",
    "        \n",
    "        \n",
    "        \n",
    "        current_idx_nodes= current_idx_nodes+1\n",
    "    \n",
    "#     print node_split\n",
    "        \n",
    "    return nodes_layer\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training error inspection \n",
    "def error_partition_combiner(list_dvAndfeatures):\n",
    "    # each feature-value has a triple [cnt, mean, variance]\n",
    "    \n",
    "    nodes_dict={}\n",
    "    tmpcnt=0\n",
    "    \n",
    "    for dvAndfeatures in list_dvAndfeatures:\n",
    "        \n",
    "        Y= dvAndfeatures[0]\n",
    "        \n",
    "        node= search_nodeToData(dvAndfeatures[1:numFeatures+1], \\\n",
    "                                currentNode_split_fromMaster.value)\n",
    "        \n",
    "        if node in nodes_dict.keys():\n",
    "            \n",
    "            # new added: sum of y in a node\n",
    "            nodes_dict[node]['sumY'] = nodes_dict[node]['sumY']+Y \n",
    "            nodes_dict[node]['count'] = nodes_dict[node]['count']+1 \n",
    "            \n",
    "            nodes_dict[node]['sumYsquare'] = nodes_dict[node]['sumYsquare']+ Y*Y\n",
    "            \n",
    "        else:\n",
    "            nodes_dict.update( {node: {}})  \n",
    "            \n",
    "            # new added: sum of y in a node\n",
    "            nodes_dict[node].update( { 'sumY': Y} )\n",
    "            nodes_dict[node].update( { 'count': 1} )\n",
    "            nodes_dict[node].update( {'sumYsquare': Y*Y} )\n",
    "            \n",
    "                \n",
    "    return zip(nodes_dict.keys(), nodes_dict.values())\n",
    "\n",
    "\n",
    "def error_merge_parttion_combiner(nodeToFeatureToValue_1,nodeToFeatureToValue_2 ):\n",
    "    \n",
    "    # new added: sum of Y in a node\n",
    "    nodeToFeatureToValue_1['sumY']= nodeToFeatureToValue_1['sumY']+ \\\n",
    "    nodeToFeatureToValue_2['sumY']\n",
    "    \n",
    "    nodeToFeatureToValue_1['count']= nodeToFeatureToValue_1['count']+\\\n",
    "    nodeToFeatureToValue_2['count']\n",
    "    \n",
    "    nodeToFeatureToValue_1['sumYsquare']= nodeToFeatureToValue_1['sumYsquare']+ \\\n",
    "    nodeToFeatureToValue_2['sumYsquare']\n",
    " \n",
    "    return  nodeToFeatureToValue_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before split: [0.014511236311689229]\n",
      "before split: [7009728]\n",
      "after split: [0.014349689349411918]\n",
      "after split: [3437083, 3572645]\n",
      "before split: [0.010798210451503318, 0.01776640938312934]\n",
      "before split: [3437083, 3572645]\n",
      "after split: [0.010704523535973601, 0.01756699078279276]\n",
      "after split: [2228171, 1208912, 2495854, 1076791]\n",
      "before split: [0.008470207290702155, 0.014822638553892788, 0.01428674455227415, 0.025170152097685614]\n",
      "before split: [2228171, 1208912, 2495854, 1076791]\n",
      "after split: [0.00841233901993484, 0.014717587106757895, 0.014219434668983032, 0.024912707152249143]\n",
      "after split: [1057594, 1170577, 946882, 262030, 1427773, 1068081, 831897, 244894]\n",
      "before split: [0.006656452258178919, 0.00999874922952591, 0.012196111741636973, 0.023829290486815244, 0.012844802687455784, 0.016056994206285077, 0.02144827882450455, 0.03668125816846736]\n",
      "before split: [1057594, 1170577, 946882, 262030, 1427773, 1068081, 831897, 244894]\n",
      "after split: [0.006601765099321311, 0.009922251568324374, 0.0121017012806927, 0.02336505657307039, 0.012742104099666158, 0.015898263638232775, 0.02123350921001995, 0.035853430946316996]\n",
      "after split: [498193, 559401, 467067, 703510, 441207, 505675, 187345, 74685, 698622, 729151, 576566, 491515, 613408, 218489, 147836, 97058]\n",
      "before split: [0.0051676440288772945, 0.007878968891319742, 0.007015801941645386, 0.011851871201000633, 0.009603006559520642, 0.014281839910927852, 0.01880497667781898, 0.034803875184249, 0.009450638652011048, 0.015895758382485095, 0.012379416439242036, 0.020026006747056103, 0.01844749395093793, 0.029055248739437826, 0.0272677092280359, 0.04893095939264604]\n",
      "before split: [498193, 559401, 467067, 703510, 441207, 505675, 187345, 74685, 698622, 729151, 576566, 491515, 613408, 218489, 147836, 97058]\n",
      "after split: [0.005140687668051109, 0.007827469522104672, 0.006971201240668753, 0.011767577868205622, 0.00945494400329662, 0.014132169857918733, 0.018623588447070114, 0.03432835109329471, 0.00941056251872634, 0.01582306059769447, 0.012313627284693067, 0.019916366369575514, 0.01828000556751285, 0.028367062261576655, 0.026989561279684736, 0.04838426572735507]\n",
      "after split: [300205, 197988, 391068, 168333, 344124, 122943, 563096, 140414, 362400, 78807, 390104, 115571, 82482, 104863, 31722, 42963, 207506, 491116, 649443, 79708, 172915, 403651, 287298, 204217, 340184, 273224, 136314, 82175, 50606, 97230, 80102, 16956]\n",
      "463.48358798 sec\n"
     ]
    }
   ],
   "source": [
    "# training  process\n",
    "\n",
    "# current_layer=1\n",
    "\n",
    "dta_test = dta\n",
    "nodeSplits_tree=[]\n",
    "\n",
    "nodes_tree_predict=[]\n",
    "\n",
    "#ini parameter\n",
    "currentNode_split_fromMaster = sc.broadcast([])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(0,maxdepth):\n",
    "    \n",
    "    current_NumNodes= int( math.pow(2,  i))\n",
    "    \n",
    "#     print 'current split decision:',currentNode_split_fromMaster.value\n",
    "    \n",
    "    \n",
    "      #----------------------- debug ---------------\n",
    "    \n",
    "    error_partitions = dta_test.mapPartitions( error_partition_combiner )\n",
    "\n",
    "    error_nodes =error_partitions.reduceByKey(lambda statis_partition_1, \\\n",
    "                                              statis_partition_2: \n",
    "                              error_merge_parttion_combiner(statis_partition_1,\\\n",
    "                                                      statis_partition_2 ))\n",
    "    \n",
    "    local_error_nodes= error_nodes.collect()\n",
    "    \n",
    "    local_sorted_error_nodes = sorted(local_error_nodes,key= lambda val:val[0])\n",
    "    \n",
    "#     print 'current number of nodes:', len(local_sorted_error_nodes)\n",
    "    tmpres=[]\n",
    "    tmpcnt=[]\n",
    "    for k in range(0, len(local_sorted_error_nodes)):   \n",
    "            \n",
    "        tmpvar=local_sorted_error_nodes[k][1]['sumYsquare']/local_sorted_error_nodes[k][1]['count']\\\n",
    "        -(local_sorted_error_nodes[k][1]['sumY']/local_sorted_error_nodes[k][1]['count'])*\\\n",
    "        (local_sorted_error_nodes[k][1]['sumY']/local_sorted_error_nodes[k][1]['count'])\n",
    "        \n",
    "        tmpres.append( tmpvar/ 100000.0 )\n",
    "        tmpcnt.append(local_sorted_error_nodes[k][1]['count'])\n",
    "        \n",
    "    print 'before split:',tmpres\n",
    "    print 'before split:', tmpcnt\n",
    "    \n",
    "    #---------------------------------------------\n",
    "    \n",
    "    \n",
    "    statis_partitions = dta_test.mapPartitions( partition_combiner )\n",
    "    \n",
    "    aggre_nodes =statis_partitions.reduceByKey(lambda statis_partition_1, statis_partition_2: \n",
    "                              merge_parttion_combiner(statis_partition_1,statis_partition_2 ))\n",
    "\n",
    "    local_aggre_nodes= aggre_nodes.collect()\n",
    "    \n",
    "    nodeSplits_layer=find_bestSplit(sorted(local_aggre_nodes,key= lambda val:val[0])\\\n",
    "                                    ,current_NumNodes,nodeSplits_tree, nodes_tree_predict)\n",
    "    \n",
    "\n",
    "    \n",
    "    #for whole tree\n",
    "    currentNode_split_fromMaster = sc.broadcast(nodeSplits_tree)\n",
    "    \n",
    "    \n",
    "    #----------------------- debug ---------------\n",
    "    \n",
    "    error_partitions = dta_test.mapPartitions( error_partition_combiner )\n",
    "\n",
    "    error_nodes =error_partitions.reduceByKey(lambda statis_partition_1, \\\n",
    "                                              statis_partition_2: \n",
    "                              error_merge_parttion_combiner(statis_partition_1,\\\n",
    "                                                      statis_partition_2 ))\n",
    "    \n",
    "    local_error_nodes= error_nodes.collect()\n",
    "   \n",
    "    \n",
    "    local_sorted_error_nodes = sorted(local_error_nodes,key= lambda val:val[0])\n",
    "    \n",
    "#     print 'current number of nodes:', len(local_sorted_error_nodes)\n",
    "    tmpres=[]\n",
    "    tmpcnt=[]\n",
    "    \n",
    "    for k in range(0, len(local_sorted_error_nodes)):\n",
    "        \n",
    "        tmpweight=0.0\n",
    "        \n",
    "        if len(local_sorted_error_nodes) > 1:\n",
    "            if (k%2) == 0 :\n",
    "                tmpweight= local_sorted_error_nodes[k][1]['count']+ \\\n",
    "                local_sorted_error_nodes[k+1][1]['count']\n",
    "            else:\n",
    "                tmpweight= local_sorted_error_nodes[k][1]['count']+ \\\n",
    "                local_sorted_error_nodes[k-1][1]['count']\n",
    "            weight=1.0*local_sorted_error_nodes[k][1]['count']/tmpweight\n",
    "            \n",
    "        else:\n",
    "            weight=1.0    \n",
    "            \n",
    "#       weight=1.0     \n",
    "            \n",
    "        tmpvar=local_sorted_error_nodes[k][1]['sumYsquare']/local_sorted_error_nodes[k][1]['count']\\\n",
    "        -(local_sorted_error_nodes[k][1]['sumY']/local_sorted_error_nodes[k][1]['count'])*\\\n",
    "        (local_sorted_error_nodes[k][1]['sumY']/local_sorted_error_nodes[k][1]['count'])\n",
    "        \n",
    "        tmpvar= weight*tmpvar/ 100000.0\n",
    "        \n",
    "        tmpres.append( tmpvar )\n",
    "        tmpcnt.append( local_sorted_error_nodes[k][1]['count'] )\n",
    "    \n",
    "    reduced_error=[]\n",
    "    for k in range(0, len(local_sorted_error_nodes),2):\n",
    "        reduced_error.append( tmpres[k]+tmpres[k+1]    )\n",
    "        \n",
    "    print 'after split:',reduced_error\n",
    "    print 'after split:',tmpcnt\n",
    "    \n",
    "    #---------------------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #for layer of nodes\n",
    "#     currentNode_split_fromMaster = sc.broadcast(nodeSplits_layer)\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "\n",
    "print elapsed,'sec'\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes_tree_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodeSplits_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nodes_tree_predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-6f67930d94b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mleaf_nodes_left\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnodes_tree_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmaxdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mleaf_nodes_right\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnodes_tree_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mleaf_nodes\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mnodes_tree_predict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mleaf_nodes_left\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mleaf_nodes_right\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleaf_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nodes_tree_predict' is not defined"
     ]
    }
   ],
   "source": [
    "leaf_nodes_left=int( len(nodes_tree_predict)- math.pow(2,maxdepth))\n",
    "leaf_nodes_right=len(nodes_tree_predict)\n",
    "    \n",
    "leaf_nodes= nodes_tree_predict[leaf_nodes_left:leaf_nodes_right]\n",
    "len(leaf_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[1,2,3,4,5,5]\n",
    "a[1:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing phase\n",
    "\n",
    "dta_test=dta\n",
    "dta_test_num=len(dta_test)\n",
    "\n",
    "err_square=0\n",
    "\n",
    "for i in range(0, dta_test_num):\n",
    "    node= search_nodeToData( dta_test[i][1:numFeatures+1], nodeSplits_tree)\n",
    "    err_square=err_square+(dta_test[i][0]-nodes_tree_predict[node])*(dta_test[i][0]-nodes_tree_predict[node])\n",
    "    \n",
    "print err_square/dta_test_num\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "labeledData=  dta_resetIdx.map(lambda line: LabeledPoint( \n",
    "        line[0],[line[1],line[2],line[3],\n",
    "        line[4], line[5],line[6],line[7]]))\n",
    "\n",
    "\n",
    "(datasubset, test_subset) = labeledData.randomSplit([0.005, 0.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3315067 0\n"
     ]
    }
   ],
   "source": [
    "tmpsum=0\n",
    "featurecnt=0\n",
    "\n",
    "for j in local_aggre_nodes[0][1][2].keys():\n",
    "    featurecnt= featurecnt+local_aggre_nodes[0][1][2][j][0]\n",
    "#     for i in local_aggre_nodes[0][1][0][j].keys():\n",
    "#         tmpsum=tmpsum + local_aggre_nodes[0][1][0][j][i]\n",
    "    \n",
    "print featurecnt,tmpsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'count': 3315067, 'sumY': 2758054210.0, 'sumYsquare': 161259522932250.0})"
      ]
     },
     "execution_count": 706,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def search_nodeToData(features, tree):\n",
    "        \n",
    "#     nodeNum=len(tree)\n",
    "    \n",
    "#     if nodeNum == 0:\n",
    "#         return 0;\n",
    "    \n",
    "#     current_nodeIdx=0\n",
    "#     while current_nodeIdx< nodeNum:\n",
    "#         split_feature= tree[current_nodeIdx][0]\n",
    "#         split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "#         if split_feature==-1:\n",
    "#             return -1\n",
    "        \n",
    "#         if (( 1<<features[split_feature]) &  split_valueSet) != 0 :\n",
    "#             current_nodeIdx=current_nodeIdx*2+1\n",
    "#         else:\n",
    "#             current_nodeIdx=current_nodeIdx*2+2\n",
    "    \n",
    "# #     return random.randint(0, 2)    \n",
    "#     return current_nodeIdx - nodeNum\n",
    "\n",
    "\n",
    "#test: one-pass method\n",
    "dta_test = dta\n",
    "\n",
    "#tree=currentNode_split_fromMaster.value()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "statis_partitions = dta_test.mapPartitions( partition_combiner )\n",
    "# local_statis_partition=statis_partitions.collect()\n",
    "aggre_nodes =statis_partitions.reduceByKey(lambda statis_partition_1, statis_partition_2: \n",
    "                              merge_parttion_combiner(statis_partition_1,statis_partition_2 ))\n",
    "local_aggre_nodes= aggre_nodes.collect()\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "\n",
    "print elapsed,'sec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# for debug\n",
    "\n",
    "                    #two-pass           one-pass\n",
    "1 node(root):      36.1043100357 sec , 35.2387490273 sec\n",
    "\n",
    "2 nodes:           36.1584570408 sec , 35.16437006 sec\n",
    "\n",
    "4 nodes:           36.6243078709 sec ,  35.6500589848 sec\n",
    "\n",
    "8 nodes:           36.7351138592 sec,   35.6860120296 sec\n",
    "\n",
    "16 nodes:          36.8523671627 sec,   35.8512570858 sec\n",
    " \n",
    "32 nodes:          37.0641298294 sec,   37.1946568489 sec\n",
    "    \n",
    "64 nodes:          37.4132461548 sec,   38.327684164 sec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, [2, 'a']), (2, [1, 'b']), (4, [8, 'c'])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2, [1, 'b']), (1, [2, 'a']), (4, [8, 'c'])]"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpdict={ 1:[2,'a'], 2:[1,'b'], 4:[8,'c']     }\n",
    "\n",
    "print tmpdict.items()\n",
    "\n",
    "sorted(tmpdict.items(), key= lambda val: val[1][0] )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 10, 2: 30}\n"
     ]
    }
   ],
   "source": [
    "# for debug \n",
    "tmparr=()\n",
    "tmparr={ 'faf':2,1:2,4:2}\n",
    "del tmparr['faf']\n",
    "tmparr\n",
    "\n",
    "tmpddict={ 1:10,2:30}\n",
    "\n",
    "def tmpfunc(tmpdict):\n",
    "    \n",
    "    ldict=tmpdict.copy()\n",
    "    ldict[1]=ldict[1]-10\n",
    "    return ldict\n",
    "\n",
    "tmpfunc(tmpddict)\n",
    "tmpfunc(tmpddict)\n",
    "\n",
    "print tmpddict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
