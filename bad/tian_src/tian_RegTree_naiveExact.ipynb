{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os \n",
    "import sys \n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "from datetime  import datetime \n",
    "\n",
    "import numpy as np  # learn \n",
    "import pandas as pd # learn\n",
    "from pandas import *\n",
    "from numpy import *\n",
    "\n",
    "from scipy import stats # look at scipy\n",
    "from scipy import linalg\n",
    "from scipy import *\n",
    "\n",
    " \n",
    "import matplotlib as mplt # learn matplolib \n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "import seaborn as sns \n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc={\"figure.figsize\": (14, 6)})\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import sklearn as sk\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import random\n",
    "import time\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328.0, 10, 10, 2, 2, 8, 1, 3)\n",
      "3315067\n"
     ]
    }
   ],
   "source": [
    "#BT data\n",
    "# dta_RDD = sc.textFile(\"file:///home/tguo/data/tian-test/udpjitter-H1april2014_regTree.csv\")\n",
    "\n",
    "dta_RDD = sc.textFile(\"hdfs://computer61.ant-net/user/tguo/udpjitter-H1april2014_regTree_mllib.csv\")\n",
    "\n",
    "#data format: dependent variable, feature values\n",
    "dta_splited = dta_RDD.map(lambda line: line.split(\",\")).map(lambda r:\n",
    "                                                                (float(r[0]),int( float(r[1])), \n",
    "                                                                 int(float(r[2])),int(float(r[3])),\n",
    "                                                                  int(float(r[4])),int(float(r[5])),\n",
    "                                                                 int(float(r[6])),int(float(r[7])) ))\n",
    "\n",
    "\n",
    "# dta_splited.first()\n",
    "dta= dta_splited\n",
    "dta.cache()\n",
    "print dta.first()\n",
    "print dta.count()\n",
    "\n",
    "# dta_filtered = dta.filter(lambda line: line[0] <2000 )\n",
    "# dta_filtered.cache()\n",
    "# dta_filtered.count()\n",
    "# print dta.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: [(78.676903536, 5, 13, 101, 1005)]\n",
      "2000000\n"
     ]
    }
   ],
   "source": [
    "# synthetic data\n",
    "dta_RDD = sc.textFile(\"file:///home/tguo/tian_src/synData_7f_6zeros_0.01percen.txt\")\n",
    "# synthetic_data_6zeros_0.01percen.txt\n",
    "\n",
    "dta_RDD.cache()\n",
    "\n",
    "dta_splited = dta_RDD.map(lambda line: line.split(\",\")).map(lambda r: ( float(r[7]),int(r[0]),int(r[1]),int(r[2])\\\n",
    "                                                                       ,int(r[3]) )\\\n",
    "                                                           ) \n",
    "# ( float(r[2]),float(r[0]),float(r[1]) ) \n",
    "\n",
    "dta= dta_splited\n",
    "dta.cache()\n",
    "\n",
    "print 'original:',dta.take(1)\n",
    "print dta.count()\n",
    "\n",
    "# re-set index of categorical features\n",
    "\n",
    "# feature_dist=[]\n",
    "\n",
    "# tmp1 = dta.map(lambda line: line[1]).distinct().collect()\n",
    "# feature_dist.append(tmp1)\n",
    "\n",
    "# tmp1 = dta.map(lambda line: line[2]).distinct().collect()\n",
    "# feature_dist.append(tmp1)\n",
    "\n",
    "# tmp1 = dta.map(lambda line: line[3]).distinct().collect()\n",
    "# feature_dist.append(tmp1)\n",
    "\n",
    "# fea_cnt= len(feature_dist )\n",
    "# fea_map=[]\n",
    "\n",
    "# for i in range(0, fea_cnt):\n",
    "#     tmpcnt = len(feature_dist[i])\n",
    "#     val_map= dict( zip(feature_dist[i], range(0,tmpcnt)) )\n",
    "#     fea_map.append(val_map)\n",
    "\n",
    "# def reset_index( line ):\n",
    "#     tmp=[]\n",
    "#     tmp.append(line[0])\n",
    "#     for i in range(1,4):\n",
    "#         tmp.append(fea_map[i-1][ line[i] ] )\n",
    "#     return tmp\n",
    "\n",
    "# dta = dta.map( lambda line: reset_index(line)  )\n",
    "\n",
    "# print 'feature value re-indexed:',dta.first()\n",
    "# print dta.count()\n",
    "\n",
    "\n",
    "\n",
    "# total_featureVal_set=[]\n",
    "# for i in range(0, numFeatures):\n",
    "#     featureValues = dta_train.map(lambda line: line[1+i]).distinct().collect()\n",
    "#     total_featureVal_set.append( featureValues)\n",
    "\n",
    "# print total_featureVal_set    \n",
    "    \n",
    "# print 'done'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# basic functions in one-pass method [on the cluster side]\n",
    "\n",
    "# assign each data instance to the node\n",
    "def search_nodeToData(features, tree):\n",
    "    nodeNum=len(tree)   \n",
    "    if nodeNum == 0:\n",
    "        return 0;\n",
    "    \n",
    "    current_nodeIdx=0\n",
    "    while current_nodeIdx< nodeNum:\n",
    "        split_feature= tree[current_nodeIdx][0]\n",
    "        split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "        # unbalanced tree grow\n",
    "        if split_feature==-1:\n",
    "            current_nodeIdx=current_nodeIdx*2+1+ random.randint(0, 1)\n",
    "            continue\n",
    "            \n",
    "        if (( 1<<features[split_feature]) &  split_valueSet) != 0 :\n",
    "            current_nodeIdx=current_nodeIdx*2+1\n",
    "        else:\n",
    "            current_nodeIdx=current_nodeIdx*2+2\n",
    "            \n",
    "    return current_nodeIdx - nodeNum\n",
    "\n",
    "def partition_dataToNode(list_dvAndfeatures):\n",
    "    dataToNode_map=[]\n",
    "    res=[]\n",
    "    for dvAndfeatures in list_dvAndfeatures:\n",
    "        \n",
    "        Y= dvAndfeatures[0]\n",
    "        node= search_nodeToData(dvAndfeatures[1:numFeatures+1], \\\n",
    "                                currentNode_split_fromMaster.value)\n",
    "        dataToNode_map.append(node)\n",
    "        res.append( (node, dvAndfeatures )   )\n",
    "    return res\n",
    "def dataToNode_assignment( data_rdd ):\n",
    "    dataToNode_map = data_rdd.mapPartitions( partition_dataToNode )\n",
    "    dataToNode_map.cache()\n",
    "    return dataToNode_map\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "# extract values for each feature\n",
    "def partition_featureValues(list_dvAndfeatures):\n",
    "\n",
    "    feature_valueSet={}\n",
    "    \n",
    "    for i in range(0,numFeatures):\n",
    "        feature_valueSet.update( {i: set()} )\n",
    "        \n",
    "    for dvAndfeatures in list_dvAndfeatures:\n",
    "        for i in range(0,numFeatures):\n",
    "            feature_val= dvAndfeatures[i+1]\n",
    "            feature_valueSet[i].add(feature_val)\n",
    "    return zip( feature_valueSet.keys(), feature_valueSet.values() )  \n",
    "\n",
    "def merge_featureValues(  valueSet1, valueSet2):\n",
    "    return valueSet1.union(valueSet2)\n",
    "    \n",
    "def data_featureValues_collect( data_rdd ):\n",
    "    feature_valueSet_part = data_rdd.mapPartitions( partition_featureValues )\n",
    "    feature_valueSet_local = \\\n",
    "    feature_valueSet_part.reduceByKey(lambda set1, set2: merge_featureValues(set1,set2 )).collect()\n",
    "    feature_valueSet_local.sort()\n",
    "    #test\n",
    "#     print feature_valueSet_local\n",
    "    return feature_valueSet_local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split decision [on the local side]\n",
    "\n",
    "#implementation notes:\n",
    "# bit-operation to record the set of selected feature values in the left child\n",
    "\n",
    "#asecending order, upper: larger, lower: smaller\n",
    "def upper_tail_boundary( percentage, dat_rdd, tt_cnt):\n",
    "    tmpcnt= int(tt_cnt*percentage) \n",
    "    tmplist = dat_rdd.top( int(tt_cnt*percentage)  )\n",
    "    \n",
    "    \n",
    "    return tmplist[tmpcnt-1]\n",
    "\n",
    "def lower_tail_boundary( percentage, dat_rdd, tt_cnt):\n",
    "#     print tt_cnt,percentage, type(dat_rdd)\n",
    "    sorted_rdd = dat_rdd.sortBy(lambda line:line, ascending= True).cache()\n",
    "    tmpcnt= int(tt_cnt*percentage) \n",
    "    tmplist= dat_rdd.take( tmpcnt )\n",
    "    return tmplist[ tmpcnt-1 ]\n",
    "    \n",
    "# def trimmed_MSE_cal(dat_rdd, dat_rdd_cnt, trim_percentage ):\n",
    "    \n",
    "#     upper_bound = upper_tail_boundary( trim_percentage, dat_rdd, dat_rdd_cnt)\n",
    "# #     lower_bound = lower_tail_boundary( trim_percentage, dat_rdd, dat_rdd_cnt)\n",
    "    \n",
    "#     trimmed_rdd= dat_rdd.filter( lambda line: line < upper_bound ).cache()\n",
    "# #     trimmed_rdd= trimmed_rdd.filter(lambda line: line > lower_bound).cache()\n",
    "#     trimmed_rdd_cnt = trimmed_rdd.count()\n",
    "    \n",
    "#     if trimmed_rdd_cnt ==0:\n",
    "        \n",
    "#         print 'trimmed zeor happens!'\n",
    "#         tmp_mean=0\n",
    "#         tmp_mse=0\n",
    "        \n",
    "#     else:\n",
    "#         tmp_mean = trimmed_rdd.reduce(lambda a,b:a+b) / trimmed_rdd_cnt\n",
    "#         tmp_mse= trimmed_rdd.map(lambda line:(line-tmp_mean)*(line-tmp_mean)).reduce(lambda a,b:a+b)/trimmed_rdd_cnt\n",
    "    \n",
    "#     return (trimmed_rdd_cnt, tmp_mean, tmp_mse, trimmed_rdd)\n",
    "\n",
    "def trimmed_MSE( dataToSplit_rdd, feature_id, value_set ):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.filter(lambda l:l[1][feature_id+1] in value_set).map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "    if tmp_rdd_count ==0:\n",
    "        \n",
    "        print 'trimmed zeor happens!'\n",
    "        tmp_mean=0\n",
    "        tmp_mse=0\n",
    "    else:\n",
    "        tmp_mean = tmp_rdd.reduce(lambda a,b:a+b)*1.0 / tmp_rdd_count\n",
    "        tmp_mse= tmp_rdd.map(lambda line:(line-tmp_mean)*(line-tmp_mean)).reduce(lambda a,b:a+b)*1.0/tmp_rdd_count\n",
    "    \n",
    "    return (tmp_rdd_count, tmp_mean, tmp_mse )\n",
    "    \n",
    "def trimmed_MSE_node( dataToSplit_rdd, trim_percentage ):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "    upper_bound = upper_tail_boundary( trim_percentage, tmp_rdd, tmp_rdd_count)\n",
    "#     lower_bound = lower_tail_boundary( trim_percentage, dat_rdd, dat_rdd_cnt)\n",
    "    tmp_trim_rdd= tmp_rdd.filter( lambda line: line < upper_bound ).cache()\n",
    " \n",
    "    trimmed_rdd= dataToSplit_rdd.filter( lambda line: line[1][0] < upper_bound ).cache()\n",
    "#     trimmed_rdd= trimmed_rdd.filter(lambda line: line > lower_bound).cache()\n",
    "    trimmed_rdd_cnt = trimmed_rdd.count()\n",
    "    \n",
    "    if trimmed_rdd_cnt ==0:\n",
    "        \n",
    "        print 'trimmed zeor happens!'\n",
    "        tmp_mean=0\n",
    "        tmp_mse=0\n",
    "        \n",
    "    else:\n",
    "        tmp_mean = tmp_trim_rdd.reduce(lambda a,b:a+b) / trimmed_rdd_cnt\n",
    "        tmp_mse= tmp_trim_rdd.map(lambda line:(line-tmp_mean)*(line-tmp_mean)).reduce(lambda a,b:a+b)/trimmed_rdd_cnt\n",
    "    \n",
    "    return (trimmed_rdd_cnt, tmp_mean, tmp_mse, trimmed_rdd)\n",
    "\n",
    "def split_onOneFeature_exact_trimmedMSE(node_data, node_data_cnt, TMSE_node_ini, feature_valueList, feature_id):\n",
    "    \n",
    "    left_count=0\n",
    "    right_count= node_data_cnt\n",
    "    bestSplitMetric=  TMSE_node_ini \n",
    "    \n",
    "    #------check the number of feature values------------------\n",
    "    if len(feature_valueList) <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    #-----------------------\n",
    "\n",
    "    #this is a bit-record variable\n",
    "    leftSplit_valueSet =0 \n",
    "    leftSplit_count=0 \n",
    "    rightSplit_count=0\n",
    "    \n",
    "    bestLeft=(-1,-1)\n",
    "    bestRight=(-1,-1)\n",
    "    \n",
    "    #sort feature values according to median \n",
    "    sorted_value_map=[]\n",
    "    \n",
    "    for i in feature_valueList:\n",
    "        currentVal = i\n",
    "        #          adapt to different loss function \n",
    "#-----------------------------------        \n",
    "        tmp_rdd=node_data.filter(lambda l:l[1][feature_id+1] == i ).map(lambda l:l[1][0]).cache()  \n",
    "        tmp_rdd_count = tmp_rdd.count()\n",
    "        \n",
    "        if tmp_rdd_count==0:\n",
    "            continue\n",
    "        else:\n",
    "            tmp_mean = tmp_rdd.reduce(lambda a,b:a+b)*1.0 / tmp_rdd_count\n",
    "# ------------------------------------\n",
    "        sorted_value_map.append( (tmp_mean,i) )\n",
    "        \n",
    "        \n",
    "    sorted_value_map.sort()\n",
    "    len_values= len(sorted_value_map)\n",
    "\n",
    "    # scan the split positions    \n",
    "    left_value_set=[]\n",
    "    right_value_set= copy.deepcopy(feature_valueList)\n",
    "    \n",
    "    #--------- initialize the value set ----------------------\n",
    "    \n",
    "    # check the number of feature values\n",
    "    if len_values <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    \n",
    "    current_feature_value = sorted_value_map[0][1] \n",
    "\n",
    "    left_value_set.append(current_feature_value)\n",
    "    right_value_set.remove(current_feature_value)\n",
    "        \n",
    "    #          adapt to different loss function     \n",
    "    left = trimmed_MSE( node_data, feature_id, left_value_set )\n",
    "    right= trimmed_MSE( node_data, feature_id, right_value_set )\n",
    "        \n",
    "    leftSplit_count = left[0]\n",
    "    leftMedian= left[1]\n",
    "    leftMetric= left[2]\n",
    "        \n",
    "    rightSplit_count= right[0]\n",
    "    rightMedian= right[1]\n",
    "    rightMetric= right[2]  \n",
    "        \n",
    "    current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "    bestSplitMetric=current_splitMetric     \n",
    "    bestLeft=left\n",
    "    bestRight=right\n",
    "    #bit variable to record which value is chosen for the left child\n",
    "    leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "    \n",
    "    #--------------------------------------------------------------\n",
    "    \n",
    "    for k in range(1,len_values-1):\n",
    "        \n",
    "        current_feature_value = sorted_value_map[k][1] \n",
    "        \n",
    "        left_value_set.append(current_feature_value)\n",
    "        right_value_set.remove(current_feature_value)\n",
    "        \n",
    "#          adapt to different loss function \n",
    "        left = trimmed_MSE( node_data, feature_id, left_value_set )\n",
    "        right= trimmed_MSE( node_data, feature_id, right_value_set )\n",
    "        \n",
    "        leftSplit_count = left[0]\n",
    "        leftMedian= left[1]\n",
    "        leftMetric= left[2]\n",
    "        \n",
    "        rightSplit_count= right[0]\n",
    "        rightMedian= right[1]\n",
    "        rightMetric= right[2]  \n",
    "        \n",
    "        current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "        if current_splitMetric < bestSplitMetric:\n",
    "            bestSplitMetric=current_splitMetric     \n",
    "            bestLeft=left\n",
    "            bestRight=right\n",
    "            #bit variable to record which value is chosen for the left child\n",
    "            leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "\n",
    "    return [ leftSplit_valueSet, bestLeft[1], bestRight[1], bestSplitMetric  ] \n",
    "#          value_set, left median, right median, weighted mad\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def median_MSE( dataToSplit_rdd, feature_id, value_set ):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.filter(lambda l:l[1][feature_id+1] in value_set).map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "    if tmp_rdd_count==0:\n",
    "        return (10000, 100000, 10000000)\n",
    "    \n",
    "    tmp_mean = tmp_rdd.reduce( lambda a,b: a+b)*1.0/tmp_rdd_count\n",
    "    tmpMSE= tmp_rdd.map( lambda a: (a-tmp_mean)*(a-tmp_mean) ).reduce( lambda a,b: a+b)*1.0/tmp_rdd_count\n",
    "    \n",
    "    return (tmp_rdd_count, tmp_mean, tmpMSE)\n",
    "\n",
    "def median_MSE_node( dataToSplit_rdd):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = dataToSplit_rdd.count()\n",
    "\n",
    "    if tmp_rdd_count==0:\n",
    "        return (10000, 100000, 10000000)\n",
    "    \n",
    "    tmp_mean = tmp_rdd.reduce( lambda a,b: a+b)*1.0/tmp_rdd_count\n",
    "    tmpMSE= tmp_rdd.map( lambda a: (a-tmp_mean)*(a-tmp_mean) ).reduce( lambda a,b: a+b)*1.0/tmp_rdd_count\n",
    "    \n",
    "    return (tmp_rdd_count, tmp_mean, tmpMSE)\n",
    "\n",
    "def split_onOneFeature_exact_MSE(node_data, node_data_cnt, MSE_node_ini, feature_valueList, feature_id):\n",
    "    \n",
    "    left_count=0\n",
    "    right_count= node_data_cnt\n",
    "    bestSplitMetric=  MSE_node_ini \n",
    "    \n",
    "    #------check the number of feature values------------------\n",
    "    if len(feature_valueList) <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    #-----------------------\n",
    "\n",
    "    #this is a bit-record variable\n",
    "    leftSplit_valueSet =0 \n",
    "    leftSplit_count=0  \n",
    "    rightSplit_count=0\n",
    "    \n",
    "    bestLeft=(-1,-1)\n",
    "    bestRight=(-1,-1)\n",
    "    \n",
    "    #sort feature values according to mean \n",
    "    sorted_value_map=[]\n",
    "    \n",
    "    for i in feature_valueList:\n",
    "        currentVal = i\n",
    "#-----------------------------------        \n",
    "        tmp_rdd=node_data.filter(lambda l:l[1][feature_id+1] == i ).map(lambda l:l[1][0]).cache()  \n",
    "        tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "        if tmp_rdd_count==0:\n",
    "            continue\n",
    "        else:\n",
    "            tmp_mean = tmp_rdd.reduce( lambda a,b: a+b)*1.0/tmp_rdd_count\n",
    "       \n",
    "        tmp_mse=(tmp_rdd_count, tmp_mean, 0)\n",
    "# ------------------------------------\n",
    "        sorted_value_map.append( (tmp_mse[1],i) )\n",
    "        \n",
    "    sorted_value_map.sort()\n",
    "    len_values= len(sorted_value_map)\n",
    "\n",
    "    # scan the split positions    \n",
    "    left_value_set=[]\n",
    "    right_value_set= copy.deepcopy(feature_valueList)\n",
    "    \n",
    "    #--------- initialize the value set ----------------------\n",
    "    \n",
    "    # check the number of feature values\n",
    "    if len_values <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    \n",
    "    current_feature_value = sorted_value_map[0][1] \n",
    "\n",
    "    left_value_set.append(current_feature_value)\n",
    "    right_value_set.remove(current_feature_value)\n",
    "        \n",
    "    left = median_MSE( node_data, feature_id, left_value_set )\n",
    "    right= median_MSE( node_data, feature_id, right_value_set )\n",
    "        \n",
    "    leftSplit_count = left[0]\n",
    "    leftMedian= left[1]\n",
    "    leftMetric= left[2]\n",
    "        \n",
    "    rightSplit_count= right[0]\n",
    "    rightMedian= right[1]\n",
    "    rightMetric= right[2]  \n",
    "        \n",
    "    #debug\n",
    "    if leftSplit_count+rightSplit_count!=node_data_cnt:\n",
    "        print 'problem in left and right'\n",
    "        \n",
    "    current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "    bestSplitMetric=current_splitMetric     \n",
    "    bestLeft=left\n",
    "    bestRight=right\n",
    "    #bit variable to record which value is chosen for the left child\n",
    "    leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "    \n",
    "    #--------------------------------------------------------------\n",
    "    \n",
    "    for k in range(1,len_values-1):\n",
    "        \n",
    "        current_feature_value = sorted_value_map[k][1] \n",
    "        \n",
    "        left_value_set.append(current_feature_value)\n",
    "        right_value_set.remove(current_feature_value)\n",
    "        \n",
    "        left = median_MSE( node_data, feature_id, left_value_set )\n",
    "        right= median_MSE( node_data, feature_id, right_value_set )\n",
    "        \n",
    "        leftSplit_count = left[0]\n",
    "        leftMedian= left[1]\n",
    "        leftMetric= left[2]\n",
    "        \n",
    "        rightSplit_count= right[0]\n",
    "        rightMedian= right[1]\n",
    "        rightMetric= right[2]  \n",
    "        \n",
    "        #debug\n",
    "        if leftSplit_count+rightSplit_count!=node_data_cnt:\n",
    "            print 'problem in left and right'\n",
    "        \n",
    "        \n",
    "        current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "        if current_splitMetric < bestSplitMetric:\n",
    "            bestSplitMetric=current_splitMetric     \n",
    "            bestLeft=left\n",
    "            bestRight=right\n",
    "            #bit variable to record which value is chosen for the left child\n",
    "            leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "\n",
    "    return [ leftSplit_valueSet, bestLeft[1], bestRight[1], bestSplitMetric  ] \n",
    "#          value_set, left median, right median, weighted mad\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#calculate median and MAD for a set of data in dataToSplit_rdd\n",
    "\n",
    "def median_MAD_cal( dataToSplit_rdd ):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd\n",
    "    tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "    if tmp_rdd_count==0:\n",
    "        return (10000, 100000, 10000000)\n",
    "    \n",
    "#     optimization\n",
    "    sorted_rdd = tmp_rdd.sortBy().zipWithIndex().map(lambda line: (line[1], line[0]) )\n",
    "    \n",
    "    \n",
    "    \n",
    "    if(tmp_rdd_count%2 ==0):\n",
    "        tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "        tmphalf.sort()\n",
    "        tmpcnt= len(tmphalf)\n",
    "        tmp_median = (tmphalf[0] + tmphalf[1])*1.0/2.0\n",
    "    else:\n",
    "        tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "        tmphalf.sort()\n",
    "        tmpcnt= len(tmphalf)\n",
    "        tmp_median = tmphalf[0]\n",
    "    \n",
    "    tmpMAD_rdd = tmp_rdd.map(lambda line: abs(line-tmp_median) ).cache()\n",
    "    \n",
    "    if(tmp_rdd_count%2 ==0):\n",
    "        tmphalf = tmpMAD_rdd.top( tmp_rdd_count/2+1  )\n",
    "        tmpcnt= len(tmphalf)\n",
    "        tmp_mad = (tmphalf[tmpcnt-1] + tmphalf[tmpcnt-2])*1.0/2.0\n",
    "    else:\n",
    "        tmphalf = tmpMAD_rdd.top( tmp_rdd_count/2+1  )\n",
    "        tmpcnt= len(tmphalf)\n",
    "        tmp_mad = tmphalf[tmpcnt-1]\n",
    "    \n",
    "    return (tmp_rdd_count, tmp_median, tmp_mad)\n",
    "\n",
    "\n",
    "def median_MAD( dataToSplit_rdd, feature_id, value_set ):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.filter(lambda l:l[1][feature_id+1] in value_set).map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = tmp_rdd.count()\n",
    "    return  median_MAD_cal( tmp_rdd )\n",
    "\n",
    "def median_MAD_node( dataToSplit_rdd):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = dataToSplit_rdd.count()\n",
    "    return  median_MAD_cal( tmp_rdd )\n",
    "\n",
    "def split_onOneFeature_exact_MAD(node_data, node_data_cnt, node_ini, feature_valueList, feature_id):\n",
    "    \n",
    "    left_count=0\n",
    "    right_count= node_data_cnt\n",
    "    bestSplitMetric=  node_ini \n",
    "    \n",
    "    #------check the number of feature values------------------\n",
    "    if len(feature_valueList) <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    #-----------------------\n",
    "\n",
    "    #this is a bit-record variable\n",
    "    leftSplit_valueSet =0 \n",
    "    leftSplit_count=0 \n",
    "    rightSplit_count=0\n",
    "    \n",
    "    bestLeft=(-1,-1)\n",
    "    bestRight=(-1,-1)\n",
    "    \n",
    "    #sort feature values according to median \n",
    "    sorted_value_map=[]\n",
    "    \n",
    "    for i in feature_valueList:\n",
    "        currentVal = i\n",
    "#         dataToFeatureValue_rdd=node_data.filter(lambda l:l[1][feature_id+1]==currentVal).map(lambda l:l[1][0]).cache()\n",
    "#         dataToFeatureValue_rdd_count = dataToFeatureValue_rdd.count()\n",
    "\n",
    "#-----------------------------------        \n",
    "        tmp_rdd=node_data.filter(lambda l:l[1][feature_id+1] == i ).map(lambda l:l[1][0]).cache()  \n",
    "        tmp_rdd_count = tmp_rdd.count()   \n",
    "        tmp_mad=  median_MAD_cal( tmp_rdd )\n",
    "# ------------------------------------\n",
    "        sorted_value_map.append( (tmp_mad[1],i) )\n",
    "        \n",
    "    sorted_value_map.sort()\n",
    "    len_values= len(sorted_value_map)\n",
    "\n",
    "    # scan the split positions    \n",
    "    left_value_set=[]\n",
    "    right_value_set= copy.deepcopy(feature_valueList)\n",
    "    \n",
    "    #--------- initialize the value set ----------------------\n",
    "    \n",
    "    # check the number of feature values\n",
    "    if len_values <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    \n",
    "    current_feature_value = sorted_value_map[0][1] \n",
    "\n",
    "    left_value_set.append(current_feature_value)\n",
    "    right_value_set.remove(current_feature_value)\n",
    "        \n",
    "    left = median_MAD( node_data, feature_id, left_value_set )\n",
    "    right= median_MAD( node_data, feature_id, right_value_set )\n",
    "        \n",
    "    leftSplit_count = left[0]\n",
    "    leftMedian= left[1]\n",
    "    leftMetric= left[2]\n",
    "        \n",
    "    rightSplit_count= right[0]\n",
    "    rightMedian= right[1]\n",
    "    rightMetric= right[2]  \n",
    "        \n",
    "    current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "    bestSplitMetric=current_splitMetric     \n",
    "    bestLeft=left\n",
    "    bestRight=right\n",
    "    #bit variable to record which value is chosen for the left child\n",
    "    leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "    \n",
    "    #--------------------------------------------------------------\n",
    "    \n",
    "    for k in range(1,len_values-1):\n",
    "        \n",
    "        current_feature_value = sorted_value_map[k][1] \n",
    "        \n",
    "        left_value_set.append(current_feature_value)\n",
    "        right_value_set.remove(current_feature_value)\n",
    "        \n",
    "        left = median_MAD( node_data, feature_id, left_value_set )\n",
    "        right= median_MAD( node_data, feature_id, right_value_set )\n",
    "        \n",
    "        leftSplit_count = left[0]\n",
    "        leftMedian= left[1]\n",
    "        leftMetric= left[2]\n",
    "        \n",
    "        rightSplit_count= right[0]\n",
    "        rightMedian= right[1]\n",
    "        rightMetric= right[2]  \n",
    "        \n",
    "        current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "        if current_splitMetric < bestSplitMetric:\n",
    "            bestSplitMetric=current_splitMetric     \n",
    "            bestLeft=left\n",
    "            bestRight=right\n",
    "            #bit variable to record which value is chosen for the left child\n",
    "            leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "\n",
    "    return [ leftSplit_valueSet, bestLeft[1], bestRight[1], bestSplitMetric  ] \n",
    "#          value_set, left median, right median, weighted mad\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#calculate median and MAD for a set of data in dataToSplit_rdd\n",
    "def median_LAD( dataToSplit_rdd, feature_id, value_set ):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.filter(lambda l:l[1][feature_id+1] in value_set).map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "    if tmp_rdd_count==0:\n",
    "        return (10000, 100000, 10000000)\n",
    "    \n",
    "#----------median computation optimization---------------------------------\n",
    "    sorted_rdd = tmp_rdd.sortBy(lambda line: line).zipWithIndex().map(lambda line: (line[1], line[0]) )\n",
    "    if(tmp_rdd_count%2 ==0):\n",
    "        tmp1=sorted_rdd.lookup( tmp_rdd_count/2-1  )[0]\n",
    "        tmp2=sorted_rdd.lookup( tmp_rdd_count/2  )[0]\n",
    "        tmp_median = (tmp1 + tmp1)*1.0/2.0\n",
    "    else:\n",
    "        tmp_median = sorted_rdd.lookup( tmp_rdd_count/2  )[0]\n",
    "#-------------------------------------------------------------\n",
    "    \n",
    "#     if(tmp_rdd_count%2 ==0):\n",
    "#         tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "#         tmphalf.sort()\n",
    "#         tmpcnt= len(tmphalf)\n",
    "#         tmp_median = (tmphalf[0] + tmphalf[1])*1.0/2.0\n",
    "#     else:\n",
    "#         tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "#         tmphalf.sort()\n",
    "#         tmpcnt= len(tmphalf)\n",
    "#         tmp_median = tmphalf[0]\n",
    "    \n",
    "    tmpMAD_rdd = tmp_rdd.map(lambda line: abs(line-tmp_median) ).cache()\n",
    "    tmpMAD = tmpMAD_rdd.reduce( lambda a,b: a+b)*1.0 / tmp_rdd_count\n",
    "    \n",
    "    return (tmp_rdd_count, tmp_median, tmpMAD)\n",
    "\n",
    "def median_LAD_node( dataToSplit_rdd):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = dataToSplit_rdd.count()\n",
    " \n",
    "    if tmp_rdd_count==0:\n",
    "        return (10000, 100000, 10000000)\n",
    "    \n",
    "    #----------median computation optimization---------------------------------\n",
    "    sorted_rdd = tmp_rdd.sortBy(lambda line: line).zipWithIndex().map(lambda line: (line[1], line[0]) )\n",
    "    if(tmp_rdd_count%2 ==0):\n",
    "        tmp1=sorted_rdd.lookup( tmp_rdd_count/2-1  )[0]\n",
    "        tmp2=sorted_rdd.lookup( tmp_rdd_count/2  )[0]\n",
    "        tmp_median = (tmp1 + tmp1)*1.0/2.0\n",
    "    else:\n",
    "        tmp_median = sorted_rdd.lookup( tmp_rdd_count/2  )[0]\n",
    "    #-------------------------------------------------------------\n",
    "    \n",
    "#     if(tmp_rdd_count%2 ==0):\n",
    "#         tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "#         tmphalf.sort()\n",
    "#         tmpcnt= len(tmphalf)\n",
    "#         tmp_median = (tmphalf[0] + tmphalf[1])*1.0/2.0\n",
    "#     else:\n",
    "#         tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "#         tmphalf.sort()\n",
    "#         tmpcnt= len(tmphalf)\n",
    "#         tmp_median = tmphalf[0]\n",
    "    \n",
    "    tmpMAD_rdd = tmp_rdd.map(lambda line: abs(line-tmp_median) ).cache()\n",
    "    tmpMAD = tmpMAD_rdd.reduce( lambda a,b: a+b)*1.0 / tmp_rdd_count\n",
    "    \n",
    "    return (tmp_rdd_count, tmp_median, tmpMAD)\n",
    "\n",
    "def split_onOneFeature_exact_LAD(node_data, node_data_cnt, LAD_node_ini, feature_valueList, feature_id):\n",
    "    \n",
    "    left_count=0\n",
    "    right_count= node_data_cnt\n",
    "    bestSplitMetric=  LAD_node_ini \n",
    "    \n",
    "    #------check the number of feature values------------------\n",
    "    if len(feature_valueList) <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    #-----------------------\n",
    "\n",
    "    #this is a bit-record variable\n",
    "    leftSplit_valueSet =0 \n",
    "    leftSplit_count=0 \n",
    "    rightSplit_count=0\n",
    "    \n",
    "    bestLeft=(-1,-1)\n",
    "    bestRight=(-1,-1)\n",
    "    \n",
    "    #sort feature values according to median \n",
    "    sorted_value_map=[]\n",
    "    tmpvalues=[]\n",
    "    \n",
    "    for i in feature_valueList:\n",
    "#         dataToFeatureValue_rdd=node_data.filter(lambda l:l[1][feature_id+1]==currentVal).map(lambda l:l[1][0]).cache()\n",
    "#         dataToFeatureValue_rdd_count = dataToFeatureValue_rdd.count()\n",
    "\n",
    "#-----------------------------------        \n",
    "        tmp_rdd=node_data.filter(lambda l:l[1][feature_id+1] == i ).map(lambda l:l[1][0]).cache()  \n",
    "        tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "        if tmp_rdd_count==0:\n",
    "            continue\n",
    "        else:\n",
    "            sorted_rdd = tmp_rdd.sortBy(lambda line: line).zipWithIndex().map(lambda line: (line[1], line[0]) )\n",
    "            if(tmp_rdd_count%2 ==0):\n",
    "                tmp1=sorted_rdd.lookup( tmp_rdd_count/2-1  )[0]\n",
    "                tmp2=sorted_rdd.lookup( tmp_rdd_count/2  )[0]\n",
    "                tmp_median = (tmp1 + tmp1)*1.0/2.0\n",
    "            else:\n",
    "                tmp_median = sorted_rdd.lookup( tmp_rdd_count/2  )[0]\n",
    "    \n",
    "#             tmpMAD_rdd = tmp_rdd.map(lambda line: abs(line-tmp_median) ).cache()\n",
    "#             tmpMAD = tmpMAD_rdd.reduce( lambda a,b: a+b)\n",
    "       \n",
    "        tmp_mad=(tmp_rdd_count, tmp_median, 0)\n",
    "        \n",
    "#         del tmpvalues[:]\n",
    "#         tmpvalues.append(i)\n",
    "#         tmp_mad = median_LAD( node_data, i, tmpvalues )\n",
    "        \n",
    "# ------------------------------------\n",
    "        sorted_value_map.append( (tmp_mad[1],i) )\n",
    "        \n",
    "    sorted_value_map.sort()\n",
    "    len_values= len(sorted_value_map)\n",
    "\n",
    "    # scan the split positions    \n",
    "    left_value_set=[]\n",
    "    right_value_set= copy.deepcopy(feature_valueList)\n",
    "    \n",
    "    #--------- initialize the value set ----------------------\n",
    "    \n",
    "    # check the number of feature values\n",
    "    if len_values <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    \n",
    "    current_feature_value = sorted_value_map[0][1] \n",
    "\n",
    "    left_value_set.append(current_feature_value)\n",
    "    right_value_set.remove(current_feature_value)\n",
    "        \n",
    "    left = median_LAD( node_data, feature_id, left_value_set )\n",
    "    right= median_LAD( node_data, feature_id, right_value_set )\n",
    "        \n",
    "    leftSplit_count = left[0]\n",
    "    leftMedian= left[1]\n",
    "    leftMetric= left[2]\n",
    "        \n",
    "    rightSplit_count= right[0]\n",
    "    rightMedian= right[1]\n",
    "    rightMetric= right[2]  \n",
    "        \n",
    "    current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "    bestSplitMetric=current_splitMetric     \n",
    "    bestLeft=left\n",
    "    bestRight=right\n",
    "    #bit variable to record which value is chosen for the left child\n",
    "    leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "    \n",
    "    #--------------------------------------------------------------\n",
    "    \n",
    "    for k in range(1,len_values-1):\n",
    "        \n",
    "        current_feature_value = sorted_value_map[k][1] \n",
    "        \n",
    "        left_value_set.append(current_feature_value)\n",
    "        right_value_set.remove(current_feature_value)\n",
    "        \n",
    "        left = median_LAD( node_data, feature_id, left_value_set )\n",
    "        right= median_LAD( node_data, feature_id, right_value_set )\n",
    "        \n",
    "        leftSplit_count = left[0]\n",
    "        leftMedian= left[1]\n",
    "        leftMetric= left[2]\n",
    "        \n",
    "        rightSplit_count= right[0]\n",
    "        rightMedian= right[1]\n",
    "        rightMetric= right[2]  \n",
    "        \n",
    "        current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "        if current_splitMetric < bestSplitMetric:\n",
    "            bestSplitMetric=current_splitMetric     \n",
    "            bestLeft=left\n",
    "            bestRight=right\n",
    "            #bit variable to record which value is chosen for the left child\n",
    "            leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "\n",
    "    return [ leftSplit_valueSet, bestLeft[1], bestRight[1], bestSplitMetric  ] \n",
    "#          value_set, left median, right median, weighted mad\n",
    "\n",
    "def find_bestSplit_exact( dataToNode, current_NumNodes, nodes_tree, nodes_tree_test, feature_valueList, loss_func):\n",
    "    \n",
    "    tmpnode_cnt=[]\n",
    "    presplit=[]\n",
    "        \n",
    "#------------- grow the unbalanced tree------------------------    \n",
    "    cur_node_num= len(nodes_tree)\n",
    "# -------------------------------------------------------------\n",
    "        \n",
    "    for i in range(0, current_NumNodes):\n",
    "    \n",
    "    #------------- grow the unbalanced tree ---------------------------------- \n",
    "        if cur_node_num >= 3:\n",
    "            parent_node = int(math.ceil(  (i + cur_node_num -2.0)*1.0/ 2 ))\n",
    "            parent_node_esti =nodes_tree_test[2* parent_node +1 ] \n",
    "            tmp_split_feature= nodes_tree[parent_node][0]\n",
    "            \n",
    "            if tmp_split_feature == -1:\n",
    "                nodes_tree.append(  (-1,  -1  )  )\n",
    "                nodes_tree_test.append( parent_node_esti  )\n",
    "                nodes_tree_test.append( parent_node_esti )\n",
    "                continue\n",
    "    #-------------------------------------------------------------------------\n",
    "        # prepare data for this node\n",
    "        current_node=i\n",
    "        current_node_data = dataToNode.filter( lambda line: line[0] == current_node ).cache()\n",
    "        current_node_data_cnt = current_node_data.count()\n",
    "        \n",
    "        # split initialization  \n",
    "        if loss_func == 'tmse':\n",
    "            \n",
    "            if abs(trimm_ratio-0.0)<1e-9:\n",
    "                \n",
    "                trimmed_rdd= current_node_data\n",
    "                trimmed_rdd_cnt = trimmed_rdd.count()\n",
    "                    \n",
    "                tmp_rdd=current_node_data.map(lambda l:l[1][0]).cache()\n",
    "            \n",
    "                tmp_mean = tmp_rdd.reduce(lambda a,b:a+b) / trimmed_rdd_cnt\n",
    "                tmp_mse= tmp_rdd.map(lambda line:(line-tmp_mean)*(line-tmp_mean)).reduce(lambda a,b:a+b)/trimmed_rdd_cnt\n",
    "                \n",
    "                \n",
    "                tmp_metric=(trimmed_rdd_cnt, tmp_mean, tmp_mse, trimmed_rdd )\n",
    "                \n",
    "            else:\n",
    "                tmp_metric =  trimmed_MSE_node( current_node_data, trimm_ratio )\n",
    "                \n",
    "                \n",
    "            best_splitMetric_sofar= tmp_metric[2]\n",
    "                \n",
    "#                  (trimmed_rdd_cnt, tmp_mean, tmp_mse, trimmed_rdd)\n",
    "                \n",
    "#                 tmp_rdd=current_node_data.map(lambda l:l[1][0]).cache()\n",
    "#                 tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "#                 upper_bound = upper_tail_boundary( trimm_ratio, tmp_rdd, tmp_rdd_count)\n",
    "#         #     lower_bound = lower_tail_boundary( trim_percentage, dat_rdd, dat_rdd_cnt)\n",
    "#                 tmp_trim_rdd= tmp_rdd.filter( lambda line: line < upper_bound ).cache()\n",
    " \n",
    "#                 trimmed_rdd= current_node_data.filter( lambda line: line[1][0] < upper_bound ).cache()\n",
    "#         #     trimmed_rdd= trimmed_rdd.filter(lambda line: line > lower_bound).cache()\n",
    "#                 trimmed_rdd_cnt = trimmed_rdd.count()\n",
    "            \n",
    "#                 if trimmed_rdd_cnt ==0:\n",
    "#                     print 'trimmed zeor happens!'\n",
    "#                     tmp_mean=0\n",
    "#                     tmp_mse=0\n",
    "#                 else:\n",
    "#                     tmp_mean = tmp_trim_rdd.reduce(lambda a,b:a+b) / trimmed_rdd_cnt\n",
    "#                     tmp_mse= tmp_trim_rdd.map(lambda line:(line-tmp_mean)*(line-tmp_mean)).reduce(lambda a,b:a+b)/trimmed_rdd_cnt\n",
    "                \n",
    "#             tmp_metric=(trimmed_rdd_cnt,tmp_mean,tmp_mse)\n",
    "            \n",
    "            \n",
    "    \n",
    "        elif loss_func == 'lad':\n",
    "                tmp_metric = median_LAD_node( current_node_data)\n",
    "                best_splitMetric_sofar= tmp_metric[2]\n",
    "        elif loss_func == 'mse':\n",
    "                tmp_metric = median_MSE_node( current_node_data)\n",
    "                best_splitMetric_sofar= tmp_metric[2]\n",
    "        elif loss_func == 'mad':\n",
    "                tmp_metric = median_MAD_node( current_node_data)\n",
    "                best_splitMetric_sofar= tmp_metric[2]\n",
    "        \n",
    "        \n",
    "        best_split=(-1,-1,-1,-1)\n",
    "        best_split_feature=-1\n",
    "        best_split_featureValueSet=-1\n",
    "                \n",
    "        #debug\n",
    "        tmpnode_cnt.append( current_node_data.count()  )\n",
    "        presplit.append( best_splitMetric_sofar/100000 )\n",
    "        \n",
    "        for j in range(0,numFeatures):    \n",
    "                                    \n",
    "            if loss_func == 'tmse':\n",
    "                \n",
    "                current_node_data= tmp_metric[3]\n",
    "                current_node_data_cnt= tmp_metric[0]\n",
    "                \n",
    "                cur_split=split_onOneFeature_exact_trimmedMSE(current_node_data,current_node_data_cnt,\\\n",
    "                                           best_splitMetric_sofar,feature_valueList[j][1],j)\n",
    "            elif loss_func == 'lad':\n",
    "                 cur_split=split_onOneFeature_exact_LAD(current_node_data,current_node_data_cnt,\\\n",
    "                                           best_splitMetric_sofar,feature_valueList[j][1],j)\n",
    "            elif loss_func == 'mad':\n",
    "                 cur_split=split_onOneFeature_exact_MAD(current_node_data,current_node_data_cnt,\\\n",
    "                                           best_splitMetric_sofar,feature_valueList[j][1],j)\n",
    "            elif loss_func == 'mse':\n",
    "                 cur_split=split_onOneFeature_exact_MSE(current_node_data,current_node_data_cnt,\\\n",
    "                                           best_splitMetric_sofar,feature_valueList[j][1],j)\n",
    "                \n",
    "            if  cur_split[0]!=-1  and cur_split[3] < best_splitMetric_sofar:\n",
    "                best_split=cur_split\n",
    "                best_splitMetric_sofar=cur_split[3]\n",
    "                best_split_feature = j\n",
    "                best_split_featureValueSet= cur_split[0]                  \n",
    "                \n",
    "#------------- grow the unbalanced tree ---------------------------------- \n",
    "        if best_split_feature == -1:\n",
    "            nodes_tree.append(  (-1,  -1  )  )\n",
    "            nodes_tree_test.append( tmp_metric[1]  )\n",
    "            nodes_tree_test.append( tmp_metric[1] )\n",
    "            continue\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "        # tree for spliting nodes        \n",
    "        nodes_tree.append(  (best_split_feature,  best_split_featureValueSet  )  )\n",
    "        \n",
    "        # tree for predicting   \n",
    "        nodes_tree_test.append(best_split[1])\n",
    "        nodes_tree_test.append(best_split[2])\n",
    "        \n",
    "#-----debug---------\n",
    "#     print tmpnode_cnt\n",
    "#     print presplit\n",
    "    \n",
    "#     return nodes_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994511 332190\n"
     ]
    }
   ],
   "source": [
    "# training and testing process: data preparation\n",
    "\n",
    "# with outliers\n",
    "# tmpdta = dta.takeSample(False, 1000000, 12243)\n",
    "# dta_train_all = sc.parallelize(tmpdta).cache().sample(False, .1, 12345)\n",
    "# dta_test_all = sc.parallelize(tmpdta).cache().sample(False, .1, 43243)\n",
    "\n",
    "\n",
    "dta_train_all = dta.cache().sample(False, .3, 12345)\n",
    "dta_test_all = dta.cache().sample(False, .1, 43243)\n",
    "\n",
    "print dta_train_all.count(), dta_test_all.count() \n",
    "# configurate extraction\n",
    "# print 'number of feature-value combinations:',len(dta_train_all.map(lambda line:(line[1],line[2],line[3])).distinct().collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994511\n",
      "312746\n"
     ]
    }
   ],
   "source": [
    "# training and testing process: clearned or noisy data ?\n",
    "\n",
    "# 98974\n",
    "# 99987\n",
    "\n",
    "dta_train = dta_train_all\n",
    "# .filter(lambda line: line[0]<1000 )\n",
    "dta_test = dta_test_all.filter(lambda line: line[0]<1000 )\n",
    "\n",
    "print dta_train.count()\n",
    "print dta_test.count()\n",
    "# print 'number of feature-value combinations:',len(dta_train.map(lambda line:(line[1],line[2],line[3])).distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training and testing process: main process\n",
    "\n",
    "#parameters\n",
    "maxdepth=8\n",
    "numFeatures=7\n",
    "trimm_ratio = 0.00\n",
    "# tmse (trimmed mse), lad, ma d, mse\n",
    "loss_func= 'lad'\n",
    "\n",
    "def tree_test_mapFunc_median(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], node_split)\n",
    "    return (leaf_nodes[ tmpnode ]  - line[0])*(leaf_nodes[ tmpnode ]  - line[0])\n",
    "\n",
    "def tree_test_mapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], node_split)\n",
    "    return (leaf_nodes[ tmpnode ]  - line[0])*(leaf_nodes[ tmpnode ]  - line[0])\n",
    "    \n",
    "def tree_test( testData_rdd ):\n",
    "    err_rdd=testData_rdd.map( lambda line: tree_test_mapFunc(line))\n",
    "    err_sum = err_rdd.reduce( lambda a,b: a+b)\n",
    "    return err_sum / testData_rdd.count()\n",
    "\n",
    "# variables for the tree structure \n",
    "node_split=[]\n",
    "node_test=[]\n",
    "\n",
    "test_err = []\n",
    "train_err = []\n",
    "run_time=[]\n",
    "\n",
    "tree_history_split=[]\n",
    "tree_history_esti=[]\n",
    "tree_history_leaf=[]\n",
    "tree_history_runtime=[]\n",
    "\n",
    "\n",
    "#ini parameter on the cluster\n",
    "currentNode_split_fromMaster = sc.broadcast([])\n",
    "\n",
    "\n",
    "#prepare feature value set\n",
    "feature_valueSet = data_featureValues_collect( dta_train )\n",
    "feature_valueList=[]\n",
    "for i in range(0, len(feature_valueSet)):\n",
    "    feature_valueList.append( ( i, list(feature_valueSet[i][1]) ) )\n",
    "    \n",
    "#     tune the starting depth\n",
    "for i in range(maxdepth,maxdepth+1):\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    del node_split[:]\n",
    "    del node_test[:]\n",
    "    currentNode_split_fromMaster = sc.broadcast([])\n",
    "    \n",
    "    for cur_depth in range(0,i):\n",
    "        \n",
    "        current_nodeNum= int( math.pow(2,  cur_depth)) \n",
    "        dataToNode = dataToNode_assignment( dta_train )\n",
    "        \n",
    "        #debug\n",
    "        print  dataToNode.map(lambda line:line[0]).distinct().count() , current_nodeNum\n",
    "        \n",
    "        find_bestSplit_exact(dataToNode,current_nodeNum, node_split, node_test, feature_valueList,loss_func)   \n",
    "#         ( dataToNode, current_NumNodes, nodes_tree, nodes_tree_test, feature_valueSet):\n",
    "        #for whole tree\n",
    "        currentNode_split_fromMaster = sc.broadcast(node_split)\n",
    "    \n",
    "    end = time.time()\n",
    "    leaf_nodes = node_test[ len(node_test)-  (int)( math.pow(2,i)): len(node_test)  ]\n",
    "    \n",
    "    tmp_test_err= 0 \n",
    "#     tree_test( dta_test )\n",
    "    tmp_train_err= 0\n",
    "#     tree_test( dta_train )\n",
    "#     test_err.append( tmp_test_err)\n",
    "#     train_err.append( tmp_train_err )\n",
    "\n",
    "    elapsed = end-start\n",
    "    run_time.append(elapsed)\n",
    "\n",
    "    tree_history_split.append(copy.deepcopy(node_split) )\n",
    "    tree_history_esti.append( copy.deepcopy(node_test)  )\n",
    "    tree_history_leaf.append( copy.deepcopy(leaf_nodes)  )\n",
    "    tree_history_runtime.append( copy.deepcopy(run_time)  )\n",
    "        \n",
    "    print \"error at tree height\", i,\":\",  tmp_test_err, tmp_train_err\n",
    "    print \"running time at tree height\", i,\":\",  elapsed\n",
    "    print \"number of leaf nodes at tree height\", i,\":\",  len(leaf_nodes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp_split=tree_history_split\n",
    "tmp_esti= tree_history_esti\n",
    "tmp_leaf= tree_history_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: java.lang.IllegalStateException: SparkContext has been shutdown\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1726)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1767)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:366)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-07d3821f7dfd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mdta_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0merr_rdd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdta_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msearch_nodeToData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnumFeatures\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_split\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleaf_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1293\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m         \"\"\"\n\u001b[1;32m-> 1295\u001b[1;33m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1296\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1297\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1277\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m    895\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m         port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions,\n\u001b[1;32m--> 897\u001b[1;33m                                           allowLocal)\n\u001b[0m\u001b[0;32m    898\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: java.lang.IllegalStateException: SparkContext has been shutdown\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1726)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1752)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1767)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:366)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "print dta_test.first()\n",
    "err_rdd=dta_test.map(lambda line:search_nodeToData(line[1:numFeatures+1], node_split) ) \n",
    "print len(node_split)\n",
    "print len(node_test)\n",
    "print len(leaf_nodes)\n",
    "\n",
    "# print node_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.995640799666\n"
     ]
    }
   ],
   "source": [
    "err_rdd=dta_test.map(lambda line:(line[0],leaf_nodes[search_nodeToData(line[1:numFeatures+1], node_split) ], search_nodeToData(line[1:numFeatures+1], node_split) ) )\n",
    "\n",
    "print err_rdd.map( lambda line: (line[0]-line[1])*(line[0]-line[1])  ).reduce(lambda a, b: a+b)/err_rdd.count()\n",
    "# err_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# training and testing process: extract all the feature-value combinations \n",
    "# in the training data set\n",
    "\n",
    "total_featureVal_set=[]\n",
    "for i in range(0, numFeatures):\n",
    "    featureValues = dta_train.map(lambda line: line[1+i]).distinct().collect()\n",
    "    total_featureVal_set.append( featureValues)\n",
    "\n",
    "print 'done'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "15\n",
      "1\n",
      "30\n",
      "1\n",
      "16\n",
      "[[4014.9761939048767]]\n"
     ]
    }
   ],
   "source": [
    "#---------------- LAD -------------------  data backup\n",
    "lad_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "lad_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "lad_tree_history_leaf =  copy.deepcopy( tree_history_leaf)\n",
    "lad_tree_history_runtime =  copy.deepcopy( tree_history_runtime)\n",
    "\n",
    "print len(lad_tree_history_split)\n",
    "print len(lad_tree_history_split[0])\n",
    "print len(lad_tree_history_esti)\n",
    "print len(lad_tree_history_esti[0])\n",
    "\n",
    "print len(lad_tree_history_leaf)\n",
    "print len(lad_tree_history_leaf[0])\n",
    "\n",
    "print lad_tree_history_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[312.0,\n",
       "  426.0,\n",
       "  294.0,\n",
       "  333.0,\n",
       "  389.0,\n",
       "  505.0,\n",
       "  263.0,\n",
       "  304.0,\n",
       "  323.0,\n",
       "  345.0,\n",
       "  376.0,\n",
       "  420.0,\n",
       "  500.0,\n",
       "  1150.0,\n",
       "  244.0,\n",
       "  295.0,\n",
       "  266.0,\n",
       "  308.0,\n",
       "  316.0,\n",
       "  346.0,\n",
       "  344.0,\n",
       "  505.0,\n",
       "  359.0,\n",
       "  393.0,\n",
       "  405.0,\n",
       "  455.0,\n",
       "  404.0,\n",
       "  508.0,\n",
       "  504.0,\n",
       "  1150.0]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lad_tree_history_esti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LAD result statistic: node infor.\n",
    "\n",
    "# lad_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "# lad_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "\n",
    "print 'test error:',test_err\n",
    "print 'training error:',train_err\n",
    "print len(lad_tree_history_split)\n",
    "\n",
    "recog_conf=bfs_tree(lad_tree_history_split[ 0 ], total_featureVal_set, maxdepth-1,  lad_tree_history_esti[0], dta_train )  \n",
    "\n",
    "print len(recog_conf[0])\n",
    "print recog_conf[1]\n",
    "print recog_conf[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error at depth 1 : 19766.1169192\n",
      "test error at depth 2 : 18967.6044969\n",
      "test error at depth 3 : 48333.2289398\n",
      "test error at depth 4 : 19889.3832151\n"
     ]
    }
   ],
   "source": [
    "# LAD result statistic : test error\n",
    "\n",
    "pre=0\n",
    "tree_esti = lad_tree_history_esti[0]\n",
    "tree_split =  lad_tree_history_split[ 0 ]\n",
    "\n",
    "for i in range(1,maxdepth+1):\n",
    "            \n",
    "    end_bound= pre+ (int)( math.pow(2,i)) \n",
    "    test_leaf_nodes = tree_esti[ pre: end_bound  ]\n",
    "    pre= end_bound        \n",
    "    test_tree_split= tree_split[ 0:(int)( math.pow(2,i)-1)  ]\n",
    "    \n",
    "#     print len(test_tree_split)\n",
    "#     print len(test_leaf_nodes)\n",
    "\n",
    "    test_error_tree( i, dta_test  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n"
     ]
    }
   ],
   "source": [
    "#--------------- MAD --------------- result backup\n",
    "\n",
    "mad_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "mad_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "mad_tree_history_leaf =  copy.deepcopy( tree_history_leaf)\n",
    "mad_tree_history_runtime =  copy.deepcopy( tree_history_runtime)\n",
    "\n",
    "\n",
    "print len(mad_tree_history_split)\n",
    "print len(mad_tree_history_split[0])\n",
    "print len(mad_tree_history_esti)\n",
    "print len(mad_tree_history_esti[0])\n",
    "\n",
    "print len(mad_tree_history_leaf)\n",
    "print len(mad_tree_history_leaf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MAD result statistic\n",
    "# mad_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "# mad_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "\n",
    "# for i in range(1, maxdepth):\n",
    "#     print '-------tree with depth: ',i,'-----------'\n",
    "#     bfs_tree(mse_tree_history_esti[i], total_featureVal_set, i)\n",
    "\n",
    "print 'test error:',test_err\n",
    "print 'training error:',train_err\n",
    "print len(mad_tree_history_split)\n",
    "\n",
    "bfs_tree(mad_tree_history_split[ maxdepth-1 ], total_featureVal_set, maxdepth-1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MAD result statistic : test error\n",
    "\n",
    "pre=0\n",
    "tree_esti = mad_tree_history_esti[0]\n",
    "tree_split =  mad_tree_history_split[ 0 ]\n",
    "\n",
    "for i in range(1,maxdepth+1):\n",
    "            \n",
    "    end_bound= pre+ (int)( math.pow(2,i)) \n",
    "    test_leaf_nodes = tree_esti[ pre: end_bound  ]\n",
    "    pre= end_bound        \n",
    "    test_tree_split= tree_split[ 0:(int)( math.pow(2,i)-1)  ]\n",
    "    \n",
    "#     print len(test_tree_split)\n",
    "#     print len(test_leaf_nodes)\n",
    "\n",
    "    test_error_tree( i, dta_test  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1023\n",
      "1\n",
      "2046\n",
      "1\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "#---------------MSE-------------- result backup\n",
    "\n",
    "mse_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "mse_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "mse_tree_history_leaf =  copy.deepcopy( tree_history_leaf)\n",
    "mse_tree_history_runtime =  copy.deepcopy( tree_history_runtime)\n",
    "\n",
    "\n",
    "print len(mse_tree_history_split)\n",
    "print len(mse_tree_history_split[0])\n",
    "print len(mse_tree_history_esti)\n",
    "print len(mse_tree_history_esti[0])\n",
    "\n",
    "print len(mse_tree_history_leaf)\n",
    "print len(mse_tree_history_leaf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error: []\n",
      "training error: []\n",
      "1\n",
      "non_leaf_node count at previous depth 0\n",
      "depth 0 : 1\n",
      "[[3, 6, 4, 2, 5], [12, 15, 13, 11, 14], [105, 102, 103, 104, 101]] non-leaf node\n",
      "non_leaf_node count at previous depth 1\n",
      "depth 1 : 2\n",
      "[[2], [12, 15, 13, 11, 14], [105, 102, 103, 104, 101]] non-leaf node\n",
      "[[3, 4, 5, 6], [12, 15, 13, 11, 14], [105, 102, 103, 104, 101]] non-leaf node\n",
      "non_leaf_node count at previous depth 2\n",
      "depth 2 : 4\n",
      "[[2], [12, 15, 13, 11, 14], [104, 105]] non-leaf node\n",
      "[[2], [12, 15, 13, 11, 14], [101, 102, 103]] non-leaf node\n",
      "[[3, 4, 5, 6], [15], [105, 102, 103, 104, 101]] non-leaf node\n",
      "[[3, 4, 5, 6], [11, 12, 13, 14], [105, 102, 103, 104, 101]] non-leaf node\n",
      "non_leaf_node count at previous depth 4\n",
      "depth 3 : 8\n",
      "[[2], [11, 13, 14], [104, 105]] non-leaf node\n",
      "[[2], [12, 15], [104, 105]] non-leaf node\n",
      "[[2], [12, 15, 13, 11, 14], [102, 103]] non-leaf node\n",
      "[[2], [12, 15, 13, 11, 14], [101]] non-leaf node\n",
      "[[6], [15], [105, 102, 103, 104, 101]] non-leaf node\n",
      "[[3, 4, 5], [15], [105, 102, 103, 104, 101]] non-leaf node\n",
      "[[3, 4, 5, 6], [11, 12, 13, 14], [105]] non-leaf node\n",
      "[[3, 4, 5, 6], [11, 12, 13, 14], [104, 101, 102, 103]] non-leaf node\n",
      "non_leaf_node count at previous depth 8\n",
      "depth 4 : 16\n",
      "[[2], [11, 13, 14], [104]] non-leaf node\n",
      "[[2], [11, 13, 14], [105]] non-leaf node\n",
      "[[2], [12, 15], [104]] non-leaf node\n",
      "[[2], [12, 15], [105]] non-leaf node\n",
      "[[2], [13, 14, 15], [102, 103]] non-leaf node\n",
      "[[2], [11, 12], [102, 103]] non-leaf node\n",
      "[[2], [11, 13], [101]] non-leaf node\n",
      "[[2], [12, 14, 15], [101]] non-leaf node\n",
      "[[6], [15], [102]] has the estimate: 75.0241216344 true: 12.0472007918\n",
      "[[6], [15], [104, 105, 101, 103]] non-leaf node\n",
      "[[3, 4, 5], [15], [101, 103, 104, 105]] non-leaf node\n",
      "[[3, 4, 5], [15], [102]] non-leaf node\n",
      "[[3, 4, 5, 6], [11, 13], [105]] non-leaf node\n",
      "[[3, 4, 5, 6], [12, 14], [105]] non-leaf node\n",
      "[[3, 4, 5, 6], [12, 13, 14], [104, 101, 102, 103]] non-leaf node\n",
      "[[3, 4, 5, 6], [11], [104, 101, 102, 103]] non-leaf node\n",
      "non_leaf_node count at previous depth 15\n",
      "depth 5 : 30\n",
      "[[2], [11], [104]] has the estimate: 53.1358615297 true: 41.0353254845\n",
      "[[2], [13, 14], [104]] non-leaf node\n",
      "[[2], [13, 14], [105]] non-leaf node\n",
      "[[2], [11], [105]] has the estimate: 119.912162957 true: 72.0114719371\n",
      "[[2], [15], [104]] has the estimate: 120.089908397 true: 68.9629558988\n",
      "[[2], [12], [104]] has the estimate: 121.2463388 true: 44.0633750192\n",
      "[[2], [15], [105]] has the estimate: 128.179049088 true: 4.00219652747\n",
      "[[2], [12], [105]] has the estimate: 143.644169497 true: 40.0313041475\n",
      "[[2], [13, 14, 15], [102]] non-leaf node\n",
      "[[2], [13, 14, 15], [103]] non-leaf node\n",
      "[[2], [11, 12], [103]] non-leaf node\n",
      "[[2], [11, 12], [102]] non-leaf node\n",
      "[[2], [11], [101]] has the estimate: 136.123806941 true: 85.9944274041\n",
      "[[2], [13], [101]] has the estimate: 175.504081032 true: 83.0641514957\n",
      "[[2], [14], [101]] has the estimate: 175.426516092 true: 33.0401838621\n",
      "[[2], [12, 15], [101]] non-leaf node\n",
      "[[6], [15], [101]] has the estimate: 114.697218127 true: 38.0707082793\n",
      "[[6], [15], [104, 105, 103]] non-leaf node\n",
      "[[4, 5], [15], [101, 103, 104, 105]] non-leaf node\n",
      "[[3], [15], [101, 103, 104, 105]] non-leaf node\n",
      "[[3], [15], [102]] has the estimate: 150.405900943 true: 23.9625677351\n",
      "[[4, 5], [15], [102]] non-leaf node\n",
      "[[3, 5, 6], [11, 13], [105]] non-leaf node\n",
      "[[4], [11, 13], [105]] non-leaf node\n",
      "[[4, 6], [12, 14], [105]] non-leaf node\n",
      "[[3, 5], [12, 14], [105]] non-leaf node\n",
      "[[4, 5, 6], [12, 13, 14], [104, 101, 102, 103]] non-leaf node\n",
      "[[3], [12, 13, 14], [104, 101, 102, 103]] non-leaf node\n",
      "[[3], [11], [104, 101, 102, 103]] non-leaf node\n",
      "[[4, 5, 6], [11], [104, 101, 102, 103]] non-leaf node\n",
      "non_leaf_node count at previous depth 19\n",
      "depth 6 : 38\n",
      "[[2], [13], [104]] has the estimate: 80.0637384696 true: 7.03395506407\n",
      "[[2], [14], [104]] has the estimate: 91.0316761691 true: 3.0027508578\n",
      "[[2], [14], [105]] has the estimate: 90.1831350424 true: 14.9163855783\n",
      "[[2], [13], [105]] has the estimate: 91.8911272528 true: 18.0361054367\n",
      "[[2], [14, 15], [102]] non-leaf node\n",
      "[[2], [13], [102]] has the estimate: 138.522548725 true: 48.9884006393\n",
      "[[2], [13], [103]] has the estimate: 121.765635397 true: 7.04921513468\n",
      "[[2], [14, 15], [103]] non-leaf node\n",
      "[[2], [12], [103]] has the estimate: 128.298631882 true: 77.9958115171\n",
      "[[2], [11], [103]] has the estimate: 197.031940218 true: 66.0087560856\n",
      "[[2], [11], [102]] has the estimate: 156.019783026 true: 91.9335367433\n",
      "[[2], [12], [102]] has the estimate: 227.01856057 true: 65.0402774817\n",
      "[[2], [15], [101]] has the estimate: 208.073919279 true: 74.9369559498\n",
      "[[2], [12], [101]] has the estimate: 220.959882836 true: 79.9788623623\n",
      "[[6], [15], [103, 105]] non-leaf node\n",
      "[[6], [15], [104]] has the estimate: 145.228976354 true: 20.0117960529\n",
      "[[4, 5], [15], [101, 103, 104]] non-leaf node\n",
      "[[4, 5], [15], [105]] non-leaf node\n",
      "[[3], [15], [103]] has the estimate: 108.444744498 true: 22.0028669441\n",
      "[[3], [15], [104, 105, 101]] non-leaf node\n",
      "[[4], [15], [102]] has the estimate: 208.751615469 true: 92.0311198812\n",
      "[[5], [15], [102]] has the estimate: 223.62880373 true: 72.0525420135\n",
      "[[3, 5, 6], [13], [105]] non-leaf node\n",
      "[[3, 5, 6], [11], [105]] non-leaf node\n",
      "[[4], [11], [105]] has the estimate: 157.874245489 true: 58.9558428993\n",
      "[[4], [13], [105]] has the estimate: 160.955670332 true: 46.9871285229\n",
      "[[4, 6], [14], [105]] non-leaf node\n",
      "[[4, 6], [12], [105]] non-leaf node\n",
      "[[3, 5], [12], [105]] non-leaf node\n",
      "[[3, 5], [14], [105]] non-leaf node\n",
      "[[4, 5, 6], [12, 13, 14], [104]] non-leaf node\n",
      "[[4, 5, 6], [12, 13, 14], [101, 102, 103]] non-leaf node\n",
      "[[3], [12, 13, 14], [101, 103]] non-leaf node\n",
      "[[3], [12, 13, 14], [104, 102]] non-leaf node\n",
      "[[3], [11], [102, 103, 104]] non-leaf node\n",
      "[[3], [11], [101]] has the estimate: 191.660227518 true: 95.9642227771\n",
      "[[4, 5, 6], [11], [101, 102, 104]] non-leaf node\n",
      "[[4, 5, 6], [11], [103]] non-leaf node\n",
      "non_leaf_node count at previous depth 19\n",
      "depth 7 : 38\n",
      "[[2], [14], [102]] has the estimate: 79.7375501987 true: 26.9738916025\n",
      "[[2], [15], [102]] has the estimate: 105.570071378 true: 69.0159606318\n",
      "[[2], [14], [103]] has the estimate: 144.491958569 true: 93.9537311672\n",
      "[[2], [15], [103]] has the estimate: 152.406621565 true: 50.096554595\n",
      "[[6], [15], [105]] has the estimate: 135.259938179 true: 55.9782638144\n",
      "[[6], [15], [103]] has the estimate: 138.819559957 true: 21.0143744806\n",
      "[[4], [15], [101, 103, 104]] non-leaf node\n",
      "[[5], [15], [101, 103, 104]] non-leaf node\n",
      "[[5], [15], [105]] has the estimate: 117.681510036 true: 37.9844489418\n",
      "[[4], [15], [105]] has the estimate: 158.4908344 true: 46.0838238983\n",
      "[[3], [15], [101]] has the estimate: 157.986181801 true: 24.9945160202\n",
      "[[3], [15], [104, 105]] non-leaf node\n",
      "[[5], [13], [105]] has the estimate: 119.840517147 true: 70.9897816187\n",
      "[[3, 6], [13], [105]] non-leaf node\n",
      "[[3, 6], [11], [105]] non-leaf node\n",
      "[[5], [11], [105]] has the estimate: 137.741415434 true: 23.0617723397\n",
      "[[6], [14], [105]] has the estimate: 75.8356596868 true: 0.932495420567\n",
      "[[4], [14], [105]] has the estimate: 162.32872118 true: 50.9585987436\n",
      "[[4], [12], [105]] has the estimate: 134.469274451 true: 37.0283167459\n",
      "[[6], [12], [105]] has the estimate: 192.091857825 true: 59.016648045\n",
      "[[3], [12], [105]] has the estimate: 97.4121182934 true: 36.9572677153\n",
      "[[5], [12], [105]] has the estimate: 200.207513576 true: 87.9905451245\n",
      "[[5], [14], [105]] has the estimate: 126.799447717 true: 18.9605451895\n",
      "[[3], [14], [105]] has the estimate: 253.779964768 true: 95.0063896209\n",
      "[[5], [12, 13, 14], [104]] non-leaf node\n",
      "[[4, 6], [12, 13, 14], [104]] non-leaf node\n",
      "[[4], [12, 13, 14], [101, 102, 103]] non-leaf node\n",
      "[[5, 6], [12, 13, 14], [101, 102, 103]] non-leaf node\n",
      "[[3], [12, 13], [101, 103]] non-leaf node\n",
      "[[3], [14], [101, 103]] non-leaf node\n",
      "[[3], [12], [104, 102]] non-leaf node\n",
      "[[3], [13, 14], [104, 102]] non-leaf node\n",
      "[[3], [11], [103]] has the estimate: 106.094660832 true: 44.0271996417\n",
      "[[3], [11], [104, 102]] non-leaf node\n",
      "[[4, 6], [11], [101, 102, 104]] non-leaf node\n",
      "[[5], [11], [101, 102, 104]] non-leaf node\n",
      "[[4, 5], [11], [103]] non-leaf node\n",
      "[[6], [11], [103]] has the estimate: 227.044288048 true: 78.9669522955\n",
      "non_leaf_node count at previous depth 17\n",
      "depth 8 : 34\n",
      "[[4], [15], [101, 104]] non-leaf node\n",
      "[[4], [15], [103]] has the estimate: 163.630479933 true: 28.0414101739\n",
      "[[5], [15], [103]] has the estimate: 93.4443363234 true: 18.9588365801\n",
      "[[5], [15], [104, 101]] non-leaf node\n",
      "[[3], [15], [104]] has the estimate: 182.377301394 true: 36.0281796575\n",
      "[[3], [15], [105]] has the estimate: 197.374778281 true: 32.9357737994\n",
      "[[3], [13], [105]] has the estimate: 130.141631601 true: 45.9884461035\n",
      "[[6], [13], [105]] has the estimate: 131.260233907 true: 56.0145726411\n",
      "[[3], [11], [105]] has the estimate: 131.809129797 true: 5.94799312481\n",
      "[[6], [11], [105]] has the estimate: 133.201192829 true: 60.0222207182\n",
      "[[5], [12, 14], [104]] non-leaf node\n",
      "[[5], [13], [104]] has the estimate: 205.050727922 true: 85.0166748893\n",
      "[[4, 6], [13], [104]] non-leaf node\n",
      "[[4, 6], [12, 14], [104]] non-leaf node\n",
      "[[4], [12, 13, 14], [102, 103]] non-leaf node\n",
      "[[4], [12, 13, 14], [101]] non-leaf node\n",
      "[[5, 6], [12, 13, 14], [101, 102]] non-leaf node\n",
      "[[5, 6], [12, 13, 14], [103]] non-leaf node\n",
      "[[3], [12, 13], [101]] non-leaf node\n",
      "[[3], [12, 13], [103]] non-leaf node\n",
      "[[3], [14], [101]] has the estimate: 173.190668884 true: 63.9998848045\n",
      "[[3], [14], [103]] has the estimate: 173.733629681 true: 95.9595642024\n",
      "[[3], [12], [104]] has the estimate: 155.674834392 true: 88.01639687\n",
      "[[3], [12], [102]] has the estimate: 202.438829083 true: 87.0175606518\n",
      "[[3], [13, 14], [102]] non-leaf node\n",
      "[[3], [13, 14], [104]] non-leaf node\n",
      "[[3], [11], [102]] has the estimate: 120.717148096 true: 9.00938849647\n",
      "[[3], [11], [104]] has the estimate: 129.379263005 true: 43.0189743283\n",
      "[[4, 6], [11], [104]] non-leaf node\n",
      "[[4, 6], [11], [101, 102]] non-leaf node\n",
      "[[5], [11], [101, 102]] non-leaf node\n",
      "[[5], [11], [104]] has the estimate: 258.186220515 true: 97.0549977642\n",
      "[[4], [11], [103]] has the estimate: 214.123183383 true: 23.9830225896\n",
      "[[5], [11], [103]] has the estimate: 214.986797173 true: 65.9755831538\n",
      "non_leaf_node count at previous depth 16\n",
      "depth 9 : 32\n",
      "[[4], [15], [104]] has the estimate: 97.5862085722 true: 40.0153444063\n",
      "[[4], [15], [101]] has the estimate: 108.821408055 true: 10.0284561992\n",
      "[[5], [15], [101]] has the estimate: 144.100356981 true: 59.0264689632\n",
      "[[5], [15], [104]] has the estimate: 163.065467911 true: 31.0231697459\n",
      "[[5], [14], [104]] has the estimate: 47.9554238491 true: 9.96780065981\n",
      "[[5], [12], [104]] has the estimate: 69.5208428322 true: 18.98030566\n",
      "[[4], [13], [104]] has the estimate: 80.3100627505 true: 41.0516592466\n",
      "[[6], [13], [104]] has the estimate: 93.8632358434 true: 47.0243781273\n",
      "[[4, 6], [14], [104]] non-leaf node\n",
      "[[4, 6], [12], [104]] non-leaf node\n",
      "[[4], [12, 14], [102, 103]] non-leaf node\n",
      "[[4], [13], [102, 103]] non-leaf node\n",
      "[[4], [13, 14], [101]] non-leaf node\n",
      "[[4], [12], [101]] has the estimate: 228.213847645 true: 66.9984492051\n",
      "[[5, 6], [14], [101, 102]] non-leaf node\n",
      "[[5, 6], [12, 13], [101, 102]] non-leaf node\n",
      "[[5, 6], [12, 13], [103]] non-leaf node\n",
      "[[5, 6], [14], [103]] non-leaf node\n",
      "[[3], [13], [101]] has the estimate: 102.619318445 true: 42.9267778604\n",
      "[[3], [12], [101]] has the estimate: 110.73774076 true: 0.995016382114\n",
      "[[3], [13], [103]] has the estimate: 141.668467052 true: 92.0067499012\n",
      "[[3], [12], [103]] has the estimate: 156.525536437 true: 82.9730159638\n",
      "[[3], [13], [102]] has the estimate: 170.005961621 true: 43.9771010288\n",
      "[[3], [14], [102]] has the estimate: 237.377164818 true: 98.0827886987\n",
      "[[3], [14], [104]] has the estimate: 253.154789967 true: 64.0198044223\n",
      "[[3], [13], [104]] has the estimate: 281.418189337 true: 94.9959237263\n",
      "[[6], [11], [104]] has the estimate: 116.503408897 true: 42.1019409905\n",
      "[[4], [11], [104]] has the estimate: 174.826043871 true: 73.9282383826\n",
      "[[4], [11], [101, 102]] non-leaf node\n",
      "[[6], [11], [101, 102]] non-leaf node\n",
      "[[5], [11], [101]] has the estimate: 128.136434735 true: 52.9720505533\n",
      "[[5], [11], [102]] has the estimate: 172.107963134 true: 30.9791007428\n",
      "bottom depth 10 : 22\n",
      "[[6], [14], [104]] has the estimate: 164.797832702 true: 60.0257131569\n",
      "[[4], [14], [104]] has the estimate: 177.760609821 true: 51.0202238742\n",
      "[[4], [12], [104]] has the estimate: 179.878357628 true: 83.9568700641\n",
      "[[6], [12], [104]] has the estimate: 202.801101245 true: 55.1041205432\n",
      "[[4], [12, 14], [103]] has the estimate: 106.949545831\n",
      "[[4], [12, 14], [102]] has the estimate: 143.255452216\n",
      "[[4], [13], [102]] has the estimate: 156.362685776 true: 79.0491984266\n",
      "[[4], [13], [103]] has the estimate: 190.044933248 true: 30.0339704142\n",
      "[[4], [13], [101]] has the estimate: 125.533196262 true: 23.995414092\n",
      "[[4], [14], [101]] has the estimate: 145.211765025 true: 67.0040520374\n",
      "[[6], [14], [101, 102]] has the estimate: 112.020864031\n",
      "[[5], [14], [101, 102]] has the estimate: 165.382360797\n",
      "[[5], [12, 13], [101, 102]] has the estimate: 157.228733525\n",
      "[[6], [12, 13], [101, 102]] has the estimate: 194.132006287\n",
      "[[6], [12, 13], [103]] has the estimate: 138.076900504\n",
      "[[5], [12, 13], [103]] has the estimate: 185.659648586\n",
      "[[6], [14], [103]] has the estimate: 210.230845887 true: 42.9899536304\n",
      "[[5], [14], [103]] has the estimate: 236.242382315 true: 98.9040786072\n",
      "[[4], [11], [101]] has the estimate: 167.308579033 true: 65.0169132598\n",
      "[[4], [11], [102]] has the estimate: 176.967360608 true: 90.0372206241\n",
      "[[6], [11], [102]] has the estimate: 169.895955147 true: 38.9968112979\n",
      "[[6], [11], [101]] has the estimate: 194.412400361 true: 94.9873515467\n",
      "inter-node number: 91\n",
      " *******  number of identified configurations: 105\n",
      "105\n",
      "[0, 0, 0, 0, 1, 12, 31, 52, 70, 91, 105]\n",
      "11747.4007048\n"
     ]
    }
   ],
   "source": [
    "# MSE result statistic\n",
    "\n",
    "# mse_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "# mse_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "\n",
    "# for i in range(1, maxdepth):\n",
    "#     print '-------tree with depth: ',i,'-----------'\n",
    "#     bfs_tree(mse_tree_history_esti[i], total_featureVal_set, i)\n",
    "\n",
    "print 'test error:',test_err\n",
    "print 'training error:',train_err\n",
    "print len(mse_tree_history_split)\n",
    "\n",
    "recog_conf = bfs_tree(mse_tree_history_split[ 0 ], total_featureVal_set, maxdepth-1,  mse_tree_history_esti[0], dta_train )\n",
    "\n",
    "print len(recog_conf[0])\n",
    "print recog_conf[1]\n",
    "print recog_conf[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error at depth 1 : 10951.9519154\n",
      "test error at depth 2 : 10874.4913409\n",
      "test error at depth 3 : 10898.5890795\n",
      "test error at depth 4 : 11074.6896286\n",
      "test error at depth 5 : 11027.8917796\n",
      "test error at depth 6 : 11258.8055739\n",
      "test error at depth 7 : 11308.4776686\n",
      "test error at depth 8 : 11397.2901348\n",
      "test error at depth 9 : 11471.4427182\n",
      "test error at depth 10 : 11477.0047595\n",
      "[10951.951915391994, 10874.491340893754, 10898.589079538771, 11074.689628642509, 11027.89177964135, 11258.80557388515, 11308.477668631882, 11397.29013480827, 11471.442718201137, 11477.004759472811]\n"
     ]
    }
   ],
   "source": [
    "# MSE result statistic : test error\n",
    "\n",
    "pre=0\n",
    "tree_esti = mse_tree_history_esti[0]\n",
    "tree_split =  mse_tree_history_split[ 0 ]\n",
    "test_error_depth = []\n",
    "\n",
    "for i in range(1,maxdepth+1):\n",
    "            \n",
    "    end_bound= pre+ (int)( math.pow(2,i)) \n",
    "    test_leaf_nodes = tree_esti[ pre: end_bound  ]\n",
    "    pre= end_bound        \n",
    "    test_tree_split= tree_split[ 0:(int)( math.pow(2,i)-1)  ]\n",
    "#     print len(test_tree_split)\n",
    "#     print len(test_leaf_nodes)\n",
    "#     test_error_tree( i, dta_test  )\n",
    "    \n",
    "    test_error_depth.append( test_error_tree( i, dta_test  ) )\n",
    "\n",
    "print test_error_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1023\n",
      "1\n",
      "2046\n",
      "1\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "#--------------TMSE-------: result backup\n",
    "\n",
    "tmse_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "tmse_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "tmse_tree_history_leaf =  copy.deepcopy( tree_history_leaf)\n",
    "tmse_tree_history_runtime =  copy.deepcopy( tree_history_runtime)\n",
    "\n",
    "\n",
    "print len(tmse_tree_history_split)\n",
    "print len(tmse_tree_history_split[0])\n",
    "print len(tmse_tree_history_esti)\n",
    "print len(tmse_tree_history_esti[0])\n",
    "\n",
    "print len(tmse_tree_history_leaf)\n",
    "print len(tmse_tree_history_leaf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error: []\n",
      "training error: []\n",
      "1\n",
      "non_leaf_node count at previous depth 0\n",
      "depth 0 : 1\n",
      "[[3, 6, 4, 2, 5], [12, 15, 13, 11, 14], [105, 102, 103, 104, 101]] non-leaf node\n",
      "non_leaf_node count at previous depth 1\n",
      "depth 1 : 2\n",
      "[[3, 6, 4, 2, 5], [15], [105, 102, 103, 104, 101]] non-leaf node\n",
      "[[3, 6, 4, 2, 5], [11, 12, 13, 14], [105, 102, 103, 104, 101]] non-leaf node\n",
      "non_leaf_node count at previous depth 2\n",
      "depth 2 : 4\n",
      "[[3, 6], [15], [105, 102, 103, 104, 101]] non-leaf node\n",
      "[[2, 4, 5], [15], [105, 102, 103, 104, 101]] non-leaf node\n",
      "[[3, 6, 4, 2, 5], [11, 12, 13, 14], [104, 105]] non-leaf node\n",
      "[[3, 6, 4, 2, 5], [11, 12, 13, 14], [101, 102, 103]] non-leaf node\n",
      "non_leaf_node count at previous depth 4\n",
      "depth 3 : 8\n",
      "[[3, 6], [15], [101, 102, 103, 104]] non-leaf node\n",
      "[[3, 6], [15], [105]] non-leaf node\n",
      "[[2, 4, 5], [15], [101, 103, 104, 105]] non-leaf node\n",
      "[[2, 4, 5], [15], [102]] non-leaf node\n",
      "[[2], [11, 12, 13, 14], [104, 105]] non-leaf node\n",
      "[[3, 4, 5, 6], [11, 12, 13, 14], [104, 105]] non-leaf node\n",
      "[[4], [11, 12, 13, 14], [101, 102, 103]] non-leaf node\n",
      "[[2, 3, 5, 6], [11, 12, 13, 14], [101, 102, 103]] non-leaf node\n",
      "non_leaf_node count at previous depth 8\n",
      "depth 4 : 16\n",
      "[[3, 6], [15], [102, 103]] non-leaf node\n",
      "[[3, 6], [15], [104, 101]] non-leaf node\n",
      "[[3], [15], [105]] has the estimate: 32.9357737994 true: 32.9357737994\n",
      "[[6], [15], [105]] has the estimate: 55.9782638144 true: 55.9782638144\n",
      "[[2, 4, 5], [15], [103, 105]] non-leaf node\n",
      "[[2, 4, 5], [15], [104, 101]] non-leaf node\n",
      "[[2, 5], [15], [102]] non-leaf node\n",
      "[[4], [15], [102]] has the estimate: 92.0311198812 true: 92.0311198812\n",
      "[[2], [13, 14], [104, 105]] non-leaf node\n",
      "[[2], [11, 12], [104, 105]] non-leaf node\n",
      "[[3, 4, 5, 6], [11, 12, 13, 14], [105]] non-leaf node\n",
      "[[3, 4, 5, 6], [11, 12, 13, 14], [104]] non-leaf node\n",
      "[[4], [11, 12, 13, 14], [103]] non-leaf node\n",
      "[[4], [11, 12, 13, 14], [101, 102]] non-leaf node\n",
      "[[2, 3, 5, 6], [11, 12, 13, 14], [102]] non-leaf node\n",
      "[[2, 3, 5, 6], [11, 12, 13, 14], [101, 103]] non-leaf node\n",
      "non_leaf_node count at previous depth 13\n",
      "depth 5 : 26\n",
      "[[6], [15], [102, 103]] non-leaf node\n",
      "[[3], [15], [102, 103]] non-leaf node\n",
      "[[3, 6], [15], [104]] non-leaf node\n",
      "[[3, 6], [15], [101]] non-leaf node\n",
      "[[2, 5], [15], [103, 105]] non-leaf node\n",
      "[[4], [15], [103, 105]] non-leaf node\n",
      "[[4, 5], [15], [104, 101]] non-leaf node\n",
      "[[2], [15], [104, 101]] non-leaf node\n",
      "[[2], [15], [102]] has the estimate: 69.0159606318 true: 69.0159606318\n",
      "[[5], [15], [102]] has the estimate: 72.0525420135 true: 72.0525420135\n",
      "[[2], [13, 14], [104]] non-leaf node\n",
      "[[2], [13, 14], [105]] non-leaf node\n",
      "[[2], [12], [104, 105]] non-leaf node\n",
      "[[2], [11], [104, 105]] non-leaf node\n",
      "[[3, 4, 5, 6], [11, 14], [105]] non-leaf node\n",
      "[[3, 4, 5, 6], [12, 13], [105]] non-leaf node\n",
      "[[5, 6], [11, 12, 13, 14], [104]] non-leaf node\n",
      "[[3, 4], [11, 12, 13, 14], [104]] non-leaf node\n",
      "[[4], [11, 13, 14], [103]] non-leaf node\n",
      "[[4], [12], [103]] has the estimate: 43.9519402509 true: 43.9519402509\n",
      "[[4], [12, 13], [101, 102]] non-leaf node\n",
      "[[4], [11, 14], [101, 102]] non-leaf node\n",
      "[[2, 3, 5, 6], [11, 13], [102]] non-leaf node\n",
      "[[2, 3, 5, 6], [12, 14], [102]] non-leaf node\n",
      "[[2, 3, 5, 6], [12, 13], [101, 103]] non-leaf node\n",
      "[[2, 3, 5, 6], [11, 14], [101, 103]] non-leaf node\n",
      "non_leaf_node count at previous depth 23\n",
      "depth 6 : 46\n",
      "[[6], [15], [102]] has the estimate: 12.0472007918 true: 12.0472007918\n",
      "[[6], [15], [103]] has the estimate: 21.0143744806 true: 21.0143744806\n",
      "[[3], [15], [103]] has the estimate: 22.0028669441 true: 22.0028669441\n",
      "[[3], [15], [102]] has the estimate: 23.9625677351 true: 23.9625677351\n",
      "[[6], [15], [104]] has the estimate: 20.0117960529 true: 20.0117960529\n",
      "[[3], [15], [104]] has the estimate: 36.0281796575 true: 36.0281796575\n",
      "[[3], [15], [101]] has the estimate: 24.9945160202 true: 24.9945160202\n",
      "[[6], [15], [101]] has the estimate: 38.0707082793 true: 38.0707082793\n",
      "[[2, 5], [15], [105]] non-leaf node\n",
      "[[2, 5], [15], [103]] non-leaf node\n",
      "[[4], [15], [103]] has the estimate: 28.0414101739 true: 28.0414101739\n",
      "[[4], [15], [105]] has the estimate: 46.0838238983 true: 46.0838238983\n",
      "[[4], [15], [104, 101]] non-leaf node\n",
      "[[5], [15], [104, 101]] non-leaf node\n",
      "[[2], [15], [104]] has the estimate: 68.9629558988 true: 68.9629558988\n",
      "[[2], [15], [101]] has the estimate: 74.9369559498 true: 74.9369559498\n",
      "[[2], [14], [104]] has the estimate: 3.0027508578 true: 3.0027508578\n",
      "[[2], [13], [104]] has the estimate: 7.03395506407 true: 7.03395506407\n",
      "[[2], [14], [105]] has the estimate: 14.9163855783 true: 14.9163855783\n",
      "[[2], [13], [105]] has the estimate: 18.0361054367 true: 18.0361054367\n",
      "[[2], [12], [105]] has the estimate: 40.0313041475 true: 40.0313041475\n",
      "[[2], [12], [104]] has the estimate: 44.0633750192 true: 44.0633750192\n",
      "[[2], [11], [104]] has the estimate: 41.0353254845 true: 41.0353254845\n",
      "[[2], [11], [105]] has the estimate: 72.0114719371 true: 72.0114719371\n",
      "[[5, 6], [11, 14], [105]] non-leaf node\n",
      "[[3, 4], [11, 14], [105]] non-leaf node\n",
      "[[3, 4, 6], [12, 13], [105]] non-leaf node\n",
      "[[5], [12, 13], [105]] non-leaf node\n",
      "[[5, 6], [12, 14], [104]] non-leaf node\n",
      "[[5, 6], [11, 13], [104]] non-leaf node\n",
      "[[3, 4], [11, 13, 14], [104]] non-leaf node\n",
      "[[3, 4], [12], [104]] non-leaf node\n",
      "[[4], [11, 14], [103]] non-leaf node\n",
      "[[4], [13], [103]] has the estimate: 30.0339704142 true: 30.0339704142\n",
      "[[4], [12, 13], [101]] non-leaf node\n",
      "[[4], [12, 13], [102]] non-leaf node\n",
      "[[4], [11, 14], [101]] non-leaf node\n",
      "[[4], [11, 14], [102]] non-leaf node\n",
      "[[3, 5], [11, 13], [102]] non-leaf node\n",
      "[[2, 6], [11, 13], [102]] non-leaf node\n",
      "[[2, 5, 6], [12, 14], [102]] non-leaf node\n",
      "[[3], [12, 14], [102]] non-leaf node\n",
      "[[2, 3, 6], [12, 13], [101, 103]] non-leaf node\n",
      "[[5], [12, 13], [101, 103]] non-leaf node\n",
      "[[5], [11, 14], [101, 103]] non-leaf node\n",
      "[[2, 3, 6], [11, 14], [101, 103]] non-leaf node\n",
      "non_leaf_node count at previous depth 25\n",
      "depth 7 : 50\n",
      "[[2], [15], [105]] has the estimate: 4.00219652747 true: 4.00219652747\n",
      "[[5], [15], [105]] has the estimate: 37.9844489418 true: 37.9844489418\n",
      "[[5], [15], [103]] has the estimate: 18.9588365801 true: 18.9588365801\n",
      "[[2], [15], [103]] has the estimate: 50.096554595 true: 50.096554595\n",
      "[[4], [15], [101]] has the estimate: 10.0284561992 true: 10.0284561992\n",
      "[[4], [15], [104]] has the estimate: 40.0153444063 true: 40.0153444063\n",
      "[[5], [15], [104]] has the estimate: 31.0231697459 true: 31.0231697459\n",
      "[[5], [15], [101]] has the estimate: 59.0264689632 true: 59.0264689632\n",
      "[[5, 6], [14], [105]] non-leaf node\n",
      "[[5, 6], [11], [105]] non-leaf node\n",
      "[[3, 4], [11], [105]] non-leaf node\n",
      "[[3, 4], [14], [105]] non-leaf node\n",
      "[[3, 4], [12, 13], [105]] non-leaf node\n",
      "[[6], [12, 13], [105]] non-leaf node\n",
      "[[5], [13], [105]] has the estimate: 70.9897816187 true: 70.9897816187\n",
      "[[5], [12], [105]] has the estimate: 87.9905451245 true: 87.9905451245\n",
      "[[5], [12, 14], [104]] non-leaf node\n",
      "[[6], [12, 14], [104]] non-leaf node\n",
      "[[6], [11, 13], [104]] non-leaf node\n",
      "[[5], [11, 13], [104]] non-leaf node\n",
      "[[4], [11, 13, 14], [104]] non-leaf node\n",
      "[[3], [11, 13, 14], [104]] non-leaf node\n",
      "[[4], [12], [104]] has the estimate: 83.9568700641 true: 83.9568700641\n",
      "[[3], [12], [104]] has the estimate: 88.01639687 true: 88.01639687\n",
      "[[4], [14], [103]] has the estimate: 18.9846489999 true: 18.9846489999\n",
      "[[4], [11], [103]] has the estimate: 23.9830225896 true: 23.9830225896\n",
      "[[4], [13], [101]] has the estimate: 23.995414092 true: 23.995414092\n",
      "[[4], [12], [101]] has the estimate: 66.9984492051 true: 66.9984492051\n",
      "[[4], [12], [102]] has the estimate: 31.0322361829 true: 31.0322361829\n",
      "[[4], [13], [102]] has the estimate: 79.0491984266 true: 79.0491984266\n",
      "[[4], [11], [101]] has the estimate: 65.0169132598 true: 65.0169132598\n",
      "[[4], [14], [101]] has the estimate: 67.0040520374 true: 67.0040520374\n",
      "[[4], [14], [102]] has the estimate: 74.9689172281 true: 74.9689172281\n",
      "[[4], [11], [102]] has the estimate: 90.0372206241 true: 90.0372206241\n",
      "[[3, 5], [11], [102]] non-leaf node\n",
      "[[3, 5], [13], [102]] non-leaf node\n",
      "[[6], [11, 13], [102]] non-leaf node\n",
      "[[2], [11, 13], [102]] non-leaf node\n",
      "[[2, 6], [12, 14], [102]] non-leaf node\n",
      "[[5], [12, 14], [102]] non-leaf node\n",
      "[[3], [12], [102]] has the estimate: 87.0175606518 true: 87.0175606518\n",
      "[[3], [14], [102]] has the estimate: 98.0827886987 true: 98.0827886987\n",
      "[[2, 3, 6], [13], [101, 103]] non-leaf node\n",
      "[[2, 3, 6], [12], [101, 103]] non-leaf node\n",
      "[[5], [12, 13], [103]] non-leaf node\n",
      "[[5], [12, 13], [101]] non-leaf node\n",
      "[[5], [11, 14], [101]] non-leaf node\n",
      "[[5], [11, 14], [103]] non-leaf node\n",
      "[[2, 3, 6], [14], [101, 103]] non-leaf node\n",
      "[[2, 3, 6], [11], [101, 103]] non-leaf node\n",
      "non_leaf_node count at previous depth 26\n",
      "depth 8 : 52\n",
      "[[6], [14], [105]] has the estimate: 0.932495420567 true: 0.932495420567\n",
      "[[5], [14], [105]] has the estimate: 18.9605451895 true: 18.9605451895\n",
      "[[5], [11], [105]] has the estimate: 23.0617723397 true: 23.0617723397\n",
      "[[6], [11], [105]] has the estimate: 60.0222207182 true: 60.0222207182\n",
      "[[3], [11], [105]] has the estimate: 5.94799312481 true: 5.94799312481\n",
      "[[4], [11], [105]] has the estimate: 58.9558428993 true: 58.9558428993\n",
      "[[4], [14], [105]] has the estimate: 50.9585987436 true: 50.9585987436\n",
      "[[3], [14], [105]] has the estimate: 95.0063896209 true: 95.0063896209\n",
      "[[3, 4], [12], [105]] non-leaf node\n",
      "[[3, 4], [13], [105]] non-leaf node\n",
      "[[6], [13], [105]] has the estimate: 56.0145726411 true: 56.0145726411\n",
      "[[6], [12], [105]] has the estimate: 59.016648045 true: 59.016648045\n",
      "[[5], [14], [104]] has the estimate: 9.96780065981 true: 9.96780065981\n",
      "[[5], [12], [104]] has the estimate: 18.98030566 true: 18.98030566\n",
      "[[6], [12], [104]] has the estimate: 55.1041205432 true: 55.1041205432\n",
      "[[6], [14], [104]] has the estimate: 60.0257131569 true: 60.0257131569\n",
      "[[6], [11], [104]] has the estimate: 42.1019409905 true: 42.1019409905\n",
      "[[6], [13], [104]] has the estimate: 47.0243781273 true: 47.0243781273\n",
      "[[5], [13], [104]] has the estimate: 85.0166748893 true: 85.0166748893\n",
      "[[5], [11], [104]] has the estimate: 97.0549977642 true: 97.0549977642\n",
      "[[4], [13, 14], [104]] non-leaf node\n",
      "[[4], [11], [104]] has the estimate: 73.9282383826 true: 73.9282383826\n",
      "[[3], [11, 14], [104]] non-leaf node\n",
      "[[3], [13], [104]] has the estimate: 94.9959237263 true: 94.9959237263\n",
      "[[3], [11], [102]] has the estimate: 9.00938849647 true: 9.00938849647\n",
      "[[5], [11], [102]] has the estimate: 30.9791007428 true: 30.9791007428\n",
      "[[5], [13], [102]] has the estimate: 14.9587456538 true: 14.9587456538\n",
      "[[3], [13], [102]] has the estimate: 43.9771010288 true: 43.9771010288\n",
      "[[6], [11], [102]] has the estimate: 38.9968112979 true: 38.9968112979\n",
      "[[6], [13], [102]] has the estimate: 76.0566138823 true: 76.0566138823\n",
      "[[2], [13], [102]] has the estimate: 48.9884006393 true: 48.9884006393\n",
      "[[2], [11], [102]] has the estimate: 91.9335367433 true: 91.9335367433\n",
      "[[2, 6], [14], [102]] non-leaf node\n",
      "[[2, 6], [12], [102]] non-leaf node\n",
      "[[5], [12], [102]] has the estimate: 51.9356296313 true: 51.9356296313\n",
      "[[5], [14], [102]] has the estimate: 85.9819077959 true: 85.9819077959\n",
      "[[2, 3, 6], [13], [103]] non-leaf node\n",
      "[[2, 3, 6], [13], [101]] non-leaf node\n",
      "[[3], [12], [101, 103]] non-leaf node\n",
      "[[2, 6], [12], [101, 103]] non-leaf node\n",
      "[[5], [12], [103]] has the estimate: 46.9440763592 true: 46.9440763592\n",
      "[[5], [13], [103]] has the estimate: 86.9948139793 true: 86.9948139793\n",
      "[[5], [13], [101]] has the estimate: 78.9781796176 true: 78.9781796176\n",
      "[[5], [12], [101]] has the estimate: 98.0778430528 true: 98.0778430528\n",
      "[[5], [14], [101]] has the estimate: 33.0241456091 true: 33.0241456091\n",
      "[[5], [11], [101]] has the estimate: 52.9720505533 true: 52.9720505533\n",
      "[[5], [11], [103]] has the estimate: 65.9755831538 true: 65.9755831538\n",
      "[[5], [14], [103]] has the estimate: 98.9040786072 true: 98.9040786072\n",
      "[[2, 3, 6], [14], [101]] non-leaf node\n",
      "[[2, 3, 6], [14], [103]] non-leaf node\n",
      "[[2, 3, 6], [11], [103]] non-leaf node\n",
      "[[2, 3, 6], [11], [101]] non-leaf node\n",
      "non_leaf_node count at previous depth 14\n",
      "depth 9 : 28\n",
      "[[3], [12], [105]] has the estimate: 36.9572677153 true: 36.9572677153\n",
      "[[4], [12], [105]] has the estimate: 37.0283167459 true: 37.0283167459\n",
      "[[3], [13], [105]] has the estimate: 45.9884461035 true: 45.9884461035\n",
      "[[4], [13], [105]] has the estimate: 46.9871285229 true: 46.9871285229\n",
      "[[4], [13], [104]] has the estimate: 41.0516592466 true: 41.0516592466\n",
      "[[4], [14], [104]] has the estimate: 51.0202238742 true: 51.0202238742\n",
      "[[3], [11], [104]] has the estimate: 43.0189743283 true: 43.0189743283\n",
      "[[3], [14], [104]] has the estimate: 64.0198044223 true: 64.0198044223\n",
      "[[2], [14], [102]] has the estimate: 26.9738916025 true: 26.9738916025\n",
      "[[6], [14], [102]] has the estimate: 44.0005878945 true: 44.0005878945\n",
      "[[2], [12], [102]] has the estimate: 65.0402774817 true: 65.0402774817\n",
      "[[6], [12], [102]] has the estimate: 70.0071138717 true: 70.0071138717\n",
      "[[2, 6], [13], [103]] non-leaf node\n",
      "[[3], [13], [103]] has the estimate: 92.0067499012 true: 92.0067499012\n",
      "[[3], [13], [101]] has the estimate: 42.9267778604 true: 42.9267778604\n",
      "[[2, 6], [13], [101]] non-leaf node\n",
      "[[3], [12], [101]] has the estimate: 0.995016382114 true: 0.995016382114\n",
      "[[3], [12], [103]] has the estimate: 82.9730159638 true: 82.9730159638\n",
      "[[2, 6], [12], [103]] non-leaf node\n",
      "[[2, 6], [12], [101]] non-leaf node\n",
      "[[2], [14], [101]] has the estimate: 33.0401838621 true: 33.0401838621\n",
      "[[3, 6], [14], [101]] non-leaf node\n",
      "[[6], [14], [103]] has the estimate: 42.9899536304 true: 42.9899536304\n",
      "[[2, 3], [14], [103]] non-leaf node\n",
      "[[3], [11], [103]] has the estimate: 44.0271996417 true: 44.0271996417\n",
      "[[2, 6], [11], [103]] non-leaf node\n",
      "[[2], [11], [101]] has the estimate: 85.9944274041 true: 85.9944274041\n",
      "[[3, 6], [11], [101]] non-leaf node\n",
      "bottom depth 10 : 16\n",
      "[[2], [13], [103]] has the estimate: 7.04921513468 true: 7.04921513468\n",
      "[[6], [13], [103]] has the estimate: 24.9568590066 true: 24.9568590066\n",
      "[[6], [13], [101]] has the estimate: 69.0242027564 true: 69.0242027564\n",
      "[[2], [13], [101]] has the estimate: 83.0641514957 true: 83.0641514957\n",
      "[[6], [12], [103]] has the estimate: 42.9990841506 true: 42.9990841506\n",
      "[[2], [12], [103]] has the estimate: 77.9958115171 true: 77.9958115171\n",
      "[[2], [12], [101]] has the estimate: 79.9788623623 true: 79.9788623623\n",
      "[[6], [12], [101]] has the estimate: 99.0429218156 true: 99.0429218156\n",
      "[[3], [14], [101]] has the estimate: 63.9998848045 true: 63.9998848045\n",
      "[[6], [14], [101]] has the estimate: 66.0243315817 true: 66.0243315817\n",
      "[[2], [14], [103]] has the estimate: 93.9537311672 true: 93.9537311672\n",
      "[[3], [14], [103]] has the estimate: 95.9595642024 true: 95.9595642024\n",
      "[[2], [11], [103]] has the estimate: 66.0087560856 true: 66.0087560856\n",
      "[[6], [11], [103]] has the estimate: 78.9669522955 true: 78.9669522955\n",
      "[[6], [11], [101]] has the estimate: 94.9873515467 true: 94.9873515467\n",
      "[[3], [11], [101]] has the estimate: 95.9642227771 true: 95.9642227771\n",
      "inter-node number: 109\n",
      " *******  number of identified configurations: 125\n",
      "125\n",
      "[0, 0, 0, 0, 3, 6, 27, 51, 89, 109, 125]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# TMSE result statistic\n",
    "\n",
    "# tmse_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "# tmse_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "\n",
    "# for i in range(1, maxdepth):\n",
    "#     print '-------tree with depth: ',i,'-----------'\n",
    "#     bfs_tree(mse_tree_history_esti[i], total_featureVal_set, i)\n",
    "\n",
    "print 'test error:',test_err\n",
    "print 'training error:',train_err\n",
    "print len(tmse_tree_history_split)   \n",
    "\n",
    "recog_conf=bfs_tree(tmse_tree_history_split[ 0 ],total_featureVal_set,maxdepth-1,tmse_tree_history_esti[0],dta_train ) \n",
    "\n",
    "print len(recog_conf[0])\n",
    "print recog_conf[1]\n",
    "print recog_conf[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error at depth 1 : 722.061178985\n",
      "test error at depth 2 : 677.890121689\n",
      "test error at depth 3 : 604.811011388\n",
      "test error at depth 4 : 526.53048095\n",
      "test error at depth 5 : 463.571501548\n",
      "test error at depth 6 : 348.338174181\n",
      "test error at depth 7 : 228.428619434\n",
      "test error at depth 8 : 107.664019556\n",
      "test error at depth 9 : 10.1351299399\n",
      "test error at depth 10 : 0.994190527863\n",
      "[722.0611789853202, 677.8901216887728, 604.8110113875669, 526.5304809500908, 463.57150154818726, 348.33817418080304, 228.42861943390577, 107.66401955584888, 10.135129939896686, 0.9941905278626019]\n"
     ]
    }
   ],
   "source": [
    "# TMSE result statistic : test error\n",
    "\n",
    "pre=0\n",
    "tree_esti = tmse_tree_history_esti[0]\n",
    "tree_split =  tmse_tree_history_split[ 0 ]\n",
    "\n",
    "test_error_depth=[]\n",
    "\n",
    "for i in range(1,maxdepth+1):\n",
    "            \n",
    "    end_bound= pre+ (int)( math.pow(2,i)) \n",
    "    test_leaf_nodes = tree_esti[ pre: end_bound  ]\n",
    "    pre= end_bound        \n",
    "    test_tree_split= tree_split[ 0:(int)( math.pow(2,i)-1)  ]\n",
    "    \n",
    "#     print len(test_tree_split)\n",
    "#     print len(test_leaf_nodes)\n",
    "\n",
    "    test_error_depth.append( test_error_tree( i, dta_test  ) )\n",
    "\n",
    "print test_error_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data inspection for the nodes of the trained tree\n",
    "\n",
    "def feature_value_extract( valueSet):\n",
    "    \n",
    "    tmpval=1\n",
    "    tmp_valSet= valueSet\n",
    "    valSet_list=[]\n",
    "    cnt=0\n",
    "    \n",
    "    while tmpval <= tmp_valSet:\n",
    "        if tmpval & tmp_valSet !=0:\n",
    "            valSet_list.append(cnt)\n",
    "        tmpval=tmpval<<1\n",
    "        cnt=cnt+1\n",
    "        \n",
    "    return valSet_list\n",
    "\n",
    "def dfs_tree(tree, current_nodeIdx, current_featureVal_set, current_depth):\n",
    "    \n",
    "    nodeNum=len(tree)   \n",
    "    if nodeNum == 0:\n",
    "        return\n",
    "    \n",
    "    split_feature= tree[current_nodeIdx][0]\n",
    "    split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "    if split_feature==-1:\n",
    "        print current_featureVal_set\n",
    "        return;\n",
    "    \n",
    "    featureValue_list = feature_value_extract( split_valueSet)\n",
    "    \n",
    "    left_featureValue_set=[]\n",
    "    right_featureValue_set=[]\n",
    "    featureNum= len(current_featureVal_set)\n",
    "    \n",
    "    for i in range(0, featureNum):\n",
    "        if i != split_feature:\n",
    "            left_featureValue_set.append(  copy.deepcopy(current_featureVal_set[i])   )\n",
    "            right_featureValue_set.append(  copy.deepcopy(current_featureVal_set[i])   )\n",
    "        else:\n",
    "            left_featureValue_set.append(  copy.deepcopy(featureValue_list )   )\n",
    "            right_featureValue_set.append(copy.deepcopy(list(set(current_featureVal_set[i] )-set(featureValue_list)))   )\n",
    "            \n",
    "        \n",
    "    traverse_tree(tree, current_nodeIdx*2+1,left_featureValue_set,current_depth+1)\n",
    "    traverse_tree(tree, current_nodeIdx*2+2,right_featureValue_set, current_depth+1)\n",
    "    \n",
    "    return \n",
    "\n",
    "def training_data_summary( train_rdd ):\n",
    "\n",
    "#     always remove outliers before\n",
    "    train_rdd_clean= train_rdd.filter(lambda line: line[0]<1000)\n",
    "    distinct_featureVals = train_rdd_clean.map(lambda line: (line[1], line[2],line[3],line[4]) ).distinct().collect()\n",
    "   \n",
    "    featureConfig_val={}\n",
    "    for i in distinct_featureVals:\n",
    "        \n",
    "        tmprdd=train_rdd_clean.filter(lambda line:(line[1], line[2],line[3],line[4]) ==i ).map(lambda line: line[0])\n",
    "        tmpval=tmprdd.reduce(lambda a,b:a+b)\n",
    "        tmpval= tmpval/tmprdd.count()*1.0\n",
    "        \n",
    "        featureConfig_val.update( {i:tmpval} )\n",
    "    \n",
    "#     print featureConfig_val\n",
    "    return featureConfig_val\n",
    "        \n",
    "\n",
    "def config_check( featureVal_set, feature_num):\n",
    "    \n",
    "    for i in range(0, feature_num):\n",
    "        if len(featureVal_set[i]) >1 :\n",
    "            return -1\n",
    "    return 1\n",
    "\n",
    "def lookup_featureConfig_val( configToVal_dict, value_set, feature_num):\n",
    "    \n",
    "    tmptuple=()\n",
    "    for i in range(0, feature_num):\n",
    "        tmptuple = tmptuple + ( value_set[i][0]  ,)\n",
    "#     print '++++++++++++', tmptuple\n",
    "    return configToVal_dict[tmptuple]\n",
    "    \n",
    "def test_error_mapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], test_tree_split)\n",
    "    return (test_leaf_nodes[ tmpnode ]  - line[0])*(test_leaf_nodes[ tmpnode ]  - line[0])\n",
    "    \n",
    "def test_error_tree( current_depth, testRDD  ):\n",
    "    \n",
    "    test_cnt= testRDD.count()\n",
    "    err_rdd=testRDD.map( lambda line: test_error_mapFunc(line))\n",
    "    err_sum = err_rdd.reduce( lambda a,b: a+b)\n",
    "    \n",
    "    print 'test error at depth',current_depth, ':', err_sum*1.0/test_cnt\n",
    "    return err_sum*1.0/test_cnt\n",
    "    \n",
    "def bfs_tree(tree, total_featureVal_set, depth, tree_esti, trainRDD ):\n",
    "    \n",
    "    nodeNum=len(tree)   \n",
    "    if nodeNum == 0:\n",
    "        return\n",
    "    \n",
    "    feature_ConfigToVal = training_data_summary( trainRDD )\n",
    "    \n",
    "    interNode_cnt=0\n",
    "    non_leaf_cnt=0\n",
    "    recog_config_cnt=0\n",
    "    \n",
    "    qu=[]\n",
    "    current_depth=0\n",
    "    qu.append((0,total_featureVal_set,0))\n",
    "    \n",
    "    \n",
    "    recog_config_cnt_depth=[]\n",
    "    recog_conf=[]\n",
    "    \n",
    "    while current_depth<= depth:\n",
    "        \n",
    "        print 'non_leaf_node count at previous depth', non_leaf_cnt\n",
    "        \n",
    "        print 'depth', current_depth,':', len(qu)\n",
    "        tmpqu=[]\n",
    "        \n",
    "        non_leaf_cnt=0\n",
    "        \n",
    "        for i in qu:\n",
    "            current_nodeIdx= i[0] \n",
    "            current_featureVal_set= i[1]\n",
    "            \n",
    "            split_feature= tree[current_nodeIdx][0]\n",
    "            split_valueSet= tree[current_nodeIdx][1]\n",
    "                 \n",
    "            if split_feature==-1: \n",
    "                \n",
    "                if config_check( current_featureVal_set, featureNum) ==1:\n",
    "                    recog_config_cnt= recog_config_cnt+1\n",
    "                    tmpval=lookup_featureConfig_val( feature_ConfigToVal, current_featureVal_set, featureNum)\n",
    "                    \n",
    "                    print current_featureVal_set, 'has the estimate:', tree_esti[ current_nodeIdx*2+1],\\\n",
    "                    'true:',tmpval\n",
    "                    #result\n",
    "                    recog_conf.append( (tree_esti[ current_nodeIdx*2+1], tmpval) )\n",
    "                    \n",
    "                else:\n",
    "                    print current_featureVal_set, 'has the estimate:', tree_esti[ current_nodeIdx*2+1],'!!! not seperated !!!'\n",
    "                \n",
    "                interNode_cnt=interNode_cnt+1\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                non_leaf_cnt=non_leaf_cnt+1\n",
    "                print current_featureVal_set,'non-leaf node'\n",
    "                \n",
    "                \n",
    "            featureValue_list = feature_value_extract( split_valueSet)\n",
    "    \n",
    "            left_featureValue_set=[]\n",
    "            right_featureValue_set=[]\n",
    "            featureNum= len(current_featureVal_set)\n",
    "    \n",
    "            for i in range(0, featureNum):\n",
    "                if i != split_feature:\n",
    "                    left_featureValue_set.append(  copy.deepcopy(current_featureVal_set[i])   )\n",
    "                    right_featureValue_set.append(  copy.deepcopy(current_featureVal_set[i])   )\n",
    "                else:\n",
    "                    left_featureValue_set.append(  copy.deepcopy(featureValue_list )   )\n",
    "                    right_featureValue_set.append(copy.deepcopy(list(set(current_featureVal_set[i] )-set(featureValue_list)))   )\n",
    "            \n",
    "            tmpqu.append(  (current_nodeIdx*2+1,left_featureValue_set, current_nodeIdx)  )\n",
    "            tmpqu.append(  (current_nodeIdx*2+2,right_featureValue_set,current_nodeIdx) )\n",
    "        \n",
    "        qu=copy.deepcopy(tmpqu)\n",
    "        current_depth= current_depth+1\n",
    "        \n",
    "        #result\n",
    "        recog_config_cnt_depth.append( recog_config_cnt )\n",
    "    \n",
    "    \n",
    "# print out leaf nodes\n",
    "    print 'bottom depth', current_depth,':', len(qu)\n",
    "    \n",
    "    for i in qu:\n",
    "        current_nodeIdx= i[0] \n",
    "        current_featureVal_set= i[1]\n",
    "        parent_nodeIdx= i[2]\n",
    "        \n",
    "        if 2*parent_nodeIdx+1 == current_nodeIdx: \n",
    "#         parent_nodeIdx == (current_nodeIdx*2+1):\n",
    "            tree_esti_val= tree_esti[ parent_nodeIdx*2]\n",
    "        else:\n",
    "            tree_esti_val= tree_esti[ parent_nodeIdx*2+1] \n",
    "        \n",
    "        if config_check( current_featureVal_set, featureNum) ==1:\n",
    "            recog_config_cnt= recog_config_cnt+1\n",
    "            tmpval=lookup_featureConfig_val( feature_ConfigToVal, current_featureVal_set, featureNum)\n",
    "            \n",
    "            #result\n",
    "            recog_conf.append( (tree_esti_val, tmpval) )\n",
    "            \n",
    "            print current_featureVal_set, 'has the estimate:', tree_esti_val,\\\n",
    "                    'true:',tmpval\n",
    "        else:\n",
    "            print current_featureVal_set, 'has the estimate:', tree_esti_val\n",
    "    \n",
    "    #result\n",
    "    recog_config_cnt_depth.append( recog_config_cnt )\n",
    "        \n",
    "    print 'inter-node number:', interNode_cnt\n",
    "    print ' *******  number of identified configurations:', recog_config_cnt\n",
    "    \n",
    "    tmpval=0\n",
    "    for i in range(0,recog_config_cnt):\n",
    "        tmpval=tmpval+ (recog_conf[i][0]-recog_conf[i][1])*(recog_conf[i][0]-recog_conf[i][1])\n",
    "    \n",
    "    return (recog_conf, recog_config_cnt_depth, tmpval*1.0/recog_config_cnt)\n",
    "\n",
    "# len( tree_history_split)\n",
    "# len(tree_history_esti)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#do not run this part\n",
    "# training and testing process\n",
    "\n",
    "# without outliers\n",
    "tmpdta = dta.filter(lambda line: line[0]<1000).takeSample(False, 100000, 12243)\n",
    "dta_train = sc.parallelize(tmpdta).cache().sample(False, .7, 12345)\n",
    "dta_test = sc.parallelize(tmpdta).cache().sample(False, .3, 43243)\n",
    "\n",
    "maxdepth=10\n",
    "numFeatures=3\n",
    "\n",
    "node_split=[]\n",
    "node_test=[]\n",
    "run_time=[]\n",
    "\n",
    "#------------ estimate phase ---------------------\n",
    "# def search_nodeToData(features, tree):\n",
    "#     nodeNum=len(tree)   \n",
    "#     if nodeNum == 0:\n",
    "#         return 0;\n",
    "    \n",
    "#     current_nodeIdx=0\n",
    "#     while current_nodeIdx< nodeNum:\n",
    "#         split_feature= tree[current_nodeIdx][0]\n",
    "#         split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "#         if split_feature==-1:\n",
    "#             return -1\n",
    "#         if (( 1<<features[split_feature]) &  split_valueSet) != 0 :\n",
    "#             current_nodeIdx=current_nodeIdx*2+1\n",
    "#         else:\n",
    "#             current_nodeIdx=current_nodeIdx*2+2        \n",
    "#     return current_nodeIdx - nodeNum\n",
    "\n",
    "def tree_test_mapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], node_split)\n",
    "    return (leaf_nodes[ tmpnode ]  - line[0])*(leaf_nodes[ tmpnode ]  - line[0])\n",
    "    \n",
    "def tree_test( testData_rdd ):\n",
    "    err_rdd=testData_rdd.map( lambda line: tree_test_mapFunc(line))\n",
    "    err_sum = err_rdd.reduce( lambda a,b: a+b)\n",
    "    return err_sum / testData_rdd.count()\n",
    "#------------------------------------------------\n",
    "    \n",
    "#ini parameter on the cluster\n",
    "currentNode_split_fromMaster = sc.broadcast([])\n",
    "test_err = []\n",
    "train_err = []\n",
    "#prepare feature value set\n",
    "feature_valueSet = data_featureValues_collect( dta_train )\n",
    "feature_valueList=[]\n",
    "for i in range(0, len(feature_valueSet)):\n",
    "    feature_valueList.append( ( i, list(feature_valueSet[i][1]) ) )\n",
    "    \n",
    "    \n",
    "for i in range(6,maxdepth):\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    del node_split[:]\n",
    "    del node_test[:]\n",
    "    currentNode_split_fromMaster = sc.broadcast([])\n",
    "    \n",
    "    for cur_depth in range(0,i):\n",
    "        \n",
    "        current_nodeNum= int( math.pow(2,  cur_depth)) \n",
    "        \n",
    "        dataToNode = dataToNode_assignment( dta_train )\n",
    "        find_bestSplit_exact(dataToNode,current_nodeNum, node_split, node_test, feature_valueList)   \n",
    "#         ( dataToNode, current_NumNodes, nodes_tree, nodes_tree_test, feature_valueSet):\n",
    "        #for whole tree\n",
    "        currentNode_split_fromMaster = sc.broadcast(node_split)\n",
    "    \n",
    "    end = time.time()\n",
    "    elapsed = end - start    \n",
    "    leaf_nodes = node_test[ len(node_test)-  (int)( math.pow(2,i)): len(node_test)  ]\n",
    "\n",
    "    \n",
    "    tmp_test_err= tree_test( dta_test )\n",
    "    tmp_train_err= tree_test( dta_train )\n",
    "    \n",
    "    test_err.append( tmp_test_err)\n",
    "    train_err.append( tmp_train_err )\n",
    "    run_time.append(elapsed)\n",
    "    \n",
    "    print \"error at tree height\", i,\":\",  tmp_test_err, tmp_train_err\n",
    "    print \"running time at tree height\", i,\":\",  elapsed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "error at tree height 1 : 836.120673266 993197.408799\n",
    "running time at tree height 1 : 71.6192398071\n",
    "error at tree height 2 : 805.794778995 994017.544055\n",
    "running time at tree height 2 : 138.060220957\n",
    "error at tree height 3 : 765.500220461 994394.511005\n",
    "running time at tree height 3 : 208.832412004\n",
    "    \n",
    "error at tree height 4 : 734.829314704 993707.63729\n",
    "running time at tree height 4 : 281.407808065\n",
    "    \n",
    "error at tree height 5 : 644.236932264 994344.82116\n",
    "running time at tree height 5 : 4879.72735\n",
    "\n",
    "error at tree height 6 : 529.4375226396904 994434.7416043472\n",
    "\n",
    "error at tree height 7 : 770.5749206468843 992877.3588177576\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# singal-run training  process  \n",
    "dta_train = dta.sample(False, .7, 12345)\n",
    "dta_test = dta.sample(False, .3, 43243)\n",
    "dta_test = dta_test.filter(lambda line: line[0] <1000 )\n",
    "\n",
    "#parameters\n",
    "maxdepth=\n",
    "numFeatures=3\n",
    "\n",
    "node_split=[]\n",
    "node_test=[]\n",
    "#ini parameter\n",
    "currentNode_split_fromMaster = sc.broadcast([])\n",
    "\n",
    "#prepare feature value set\n",
    "feature_valueSet = data_featureValues_collect( dta_train )\n",
    "feature_valueList=[]\n",
    "for i in range(0, len(feature_valueSet)):\n",
    "    feature_valueList.append( ( i, list(feature_valueSet[i][1]) ) )\n",
    "    \n",
    "\n",
    "for i in range(0,maxdepth):\n",
    "    \n",
    "    start = time.time()\n",
    "    current_nodeNum= int( math.pow(2,  i))\n",
    "    \n",
    "    print 'current split decision:',currentNode_split_fromMaster.value\n",
    "\n",
    "    dataToNode = dataToNode_assignment( dta_train )\n",
    "    cluster_end = time.time() \n",
    "    find_bestSplit_exact(dataToNode,current_nodeNum,node_split,node_test, feature_valueList)   \n",
    "#   ( dataToNode, current_NumNodes, nodes_tree, nodes_tree_test, feature_valueSet):\n",
    "    currentNode_split_fromMaster = sc.broadcast(node_split)\n",
    "  \n",
    "    end = time.time() \n",
    "\n",
    "    print i,'-th level running time: ', cluster_end - start,'sec', end- cluster_end, 'sec'\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69015"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#select one feature-value\n",
    "feature_id=1\n",
    "feature_val=11\n",
    "\n",
    "dataToFeatureValue_rdd=node_data.filter(lambda line:line[1][feature_id+1]==feature_val).map(lambda line:line[1][0])\n",
    "\n",
    "dataToFeatureValue_rdd_count = dataToFeatureValue_rdd.count()\n",
    "        \n",
    "# med_tmpLarge= dataToFeatureValue_rdd.top( dataToFeatureValue_rdd_count/2+1  )\n",
    "med_tmp = dataToFeatureValue_rdd.top( dataToFeatureValue_rdd_count/2  )\n",
    "med_tmp.sort()\n",
    "tmpcnt= len(med_tmp)\n",
    "print med_tmp[ tmpcnt-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437547\n"
     ]
    }
   ],
   "source": [
    "#select several feature-values\n",
    "feature_values=[1,2,3]\n",
    "\n",
    "dataToFeatureValue_rdd=node_data.filter(lambda line:line[1][feature_id+1] in feature_values).map(lambda line:line[1][0])\n",
    "\n",
    "dataToFeatureValue_rdd_count = dataToFeatureValue_rdd.count()\n",
    "\n",
    "print dataToFeatureValue_rdd_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
