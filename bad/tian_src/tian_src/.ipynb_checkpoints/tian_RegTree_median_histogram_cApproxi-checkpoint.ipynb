{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os \n",
    "import sys \n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "from datetime  import datetime \n",
    "\n",
    "import numpy as np  # learn \n",
    "import pandas as pd # learn\n",
    "from pandas import *\n",
    "from numpy import *\n",
    "\n",
    "from scipy import stats # look at scipy\n",
    "from scipy import linalg\n",
    "from scipy import *\n",
    "\n",
    " \n",
    "import matplotlib as mplt # learn matplolib \n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "import seaborn as sns \n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc={\"figure.figsize\": (14, 6)})\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import sklearn as sk\n",
    "import itertools\n",
    "\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import random\n",
    "import time\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#BT data loading\n",
    "\n",
    "# dta_RDD = sc.textFile(\"file:///home/tguo/data/tian-test/udpjitter-H1april2014_regTree.csv\")\n",
    "\n",
    "dta_RDD = sc.textFile(\"hdfs://computer61.ant-net/user/tguo/udpjitter-H1april2014_regTree_mllib.csv\")\n",
    "\n",
    "\n",
    "dta_splited = dta_RDD.map(lambda line: line.split(\",\")).map(lambda r:\n",
    "                                                                (float(r[0]),int( float(r[1])), \n",
    "                                                                 int(float(r[2])),int(float(r[3])),\n",
    "                                                                 int(float(r[4])),int(float(r[5])),\n",
    "                                                                 int(float(r[6])),int(float(r[7])) ))\n",
    "# dta_splited.first()\n",
    "\n",
    "dta= dta_splited\n",
    "dta.cache()\n",
    "print dta.first()\n",
    "print dta.count()\n",
    "\n",
    "\n",
    "# dta_filtered = dta.filter(lambda line: line[0] <2000 )\n",
    "# dta_filtered.cache()\n",
    "# dta_filtered.count()\n",
    "# print dta.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51.7446171548, 4, 13, 105)\n",
      "1000000\n"
     ]
    }
   ],
   "source": [
    "# synthetic data\n",
    "dta_RDD = sc.textFile(\"file:///home/tguo/tian_src/synthetic_data.txt\")\n",
    "dta_RDD.cache()\n",
    "\n",
    "dta_splited = dta_RDD.map(lambda line: line.split(\",\")).map(lambda r: ( float(r[3]),int(r[0]),int(r[1]),int(r[2])) )\n",
    "# ( float(r[2]),float(r[0]),float(r[1]) ) \n",
    "\n",
    "dta= dta_splited\n",
    "dta.cache()\n",
    "\n",
    "dta = dta.map(lambda line: ( line[0]+5, line[1],line[2],line[3] )  )\n",
    "\n",
    "print dta.first()\n",
    "print dta.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157.833010916 55.3303962639\n",
      "989021.330554 123.770749356\n"
     ]
    }
   ],
   "source": [
    "# initial MSE MAD \n",
    "\n",
    "ycnt= dta.count()\n",
    "ysum = dta.map( lambda line: line[0]).reduce(lambda a,b: a+b)\n",
    "\n",
    "ymean= 1.0*ysum/ycnt\n",
    "\n",
    "ymedian_arr= dta.map(lambda line: line[0]).top( ycnt/2 )\n",
    "ymedian_arr.sort()\n",
    "ymedian= ymedian_arr[0]\n",
    "\n",
    "print ymean, ymedian\n",
    "\n",
    "mse= dta.map( lambda line: (line[0]- ymean)* (line[0]- ymean) ).reduce(lambda a,b: a+b)/ycnt\n",
    "mad= dta.map( lambda line: abs(line[0]- ymedian) ).reduce(lambda a,b: a+b)/ycnt\n",
    "\n",
    "print mse, mad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "print type(ymedian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test c-approxmate hist\n",
    "\n",
    "# for i in range(16,32):\n",
    "print dta.filter(lambda line: line[0]<0).collect()\n",
    "\n",
    "print cal_bin_cAppHist(-10.2, 1.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# basic functions in one-pass method [on the cluster side]\n",
    "\n",
    "# assign each data instance to the node\n",
    "def search_nodeToData(features, tree):\n",
    "        \n",
    "    nodeNum=len(tree)\n",
    "    if nodeNum == 0:\n",
    "        return 0;\n",
    "    \n",
    "    current_nodeIdx=0\n",
    "    while current_nodeIdx< nodeNum:\n",
    "        split_feature= tree[current_nodeIdx][0]\n",
    "        split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "        if split_feature==-1:\n",
    "            return -1\n",
    "        \n",
    "        if (( 1<<features[split_feature]) &  split_valueSet) != 0 :\n",
    "            current_nodeIdx=current_nodeIdx*2+1\n",
    "        else:\n",
    "            current_nodeIdx=current_nodeIdx*2+2\n",
    "      \n",
    "    return current_nodeIdx - nodeNum\n",
    "\n",
    "def ini_newBin_cAppHist(hist,binId):\n",
    "    hist.update( {binId:[1]} )\n",
    "    \n",
    "def cal_bin_cAppHist(Y, cratio):\n",
    "#     lower-bound as the bin index\n",
    "#     print Y\n",
    "    return int(math.floor( math.log10(Y)*1.0 / math.log10(cratio)  ))\n",
    "    \n",
    "def update_cAppHist(hist, binId):\n",
    "    hist_local= hist \n",
    "    \n",
    "    if binId in hist.keys():\n",
    "        hist[binId][0]=hist[binId][0]+1\n",
    "    else:\n",
    "        ini_newBin_cAppHist(hist,binId)\n",
    "    return hist_local\n",
    "\n",
    "def merge_cAppHist(hist1, hist2):\n",
    "    hist_local= hist1\n",
    "    \n",
    "    for k in hist_local.keys():\n",
    "        if k in hist2.keys():\n",
    "            hist_local[k][0]= hist_local[k][0]+ hist2[k][0]\n",
    "    \n",
    "    for k in hist2.keys():\n",
    "        if k not in hist_local.keys():\n",
    "            hist_local.update({k:hist2[k]})\n",
    "    return hist_local\n",
    "\n",
    "\n",
    "def partition_combiner_cAppHist(list_dvAndfeatures):\n",
    "    \n",
    "    nodes_dict={}\n",
    "    tmpcnt=0\n",
    "    \n",
    "    for dvAndfeatures in list_dvAndfeatures:\n",
    "        \n",
    "        Y= dvAndfeatures[0]\n",
    "        binId= cal_bin_cAppHist(Y, cApp_ratio)\n",
    "        node= search_nodeToData(dvAndfeatures[1:numFeatures+1], \\\n",
    "                                currentNode_split_fromMaster.value)\n",
    "        \n",
    "        if node in nodes_dict.keys():\n",
    "            \n",
    "            # new added: sum of y in a node\n",
    "            nodes_dict[node]['sumY'] = nodes_dict[node]['sumY']+Y \n",
    "            nodes_dict[node]['count'] = nodes_dict[node]['count']+1 \n",
    "            update_cAppHist(nodes_dict[node]['hist'], binId)\n",
    "            \n",
    "            for i in range(0,numFeatures):    \n",
    "                feature_val= dvAndfeatures[i+1]\n",
    "                \n",
    "                if feature_val not in nodes_dict[node][i]:\n",
    "              \n",
    "                    nodes_dict[node][i].update( { feature_val: {}   }  )\n",
    "                    nodes_dict[node][i][feature_val].update( {'count_inFeatureValue':1} )\n",
    "                    nodes_dict[node][i][feature_val].update( {'hist_inFeatureValue': {} } )  \n",
    "                    ini_newBin_cAppHist(nodes_dict[node][i][feature_val]['hist_inFeatureValue'], binId)\n",
    "                    \n",
    "                else:\n",
    "                    nodes_dict[node][i][feature_val]['count_inFeatureValue'] = \\\n",
    "                    nodes_dict[node][i][feature_val]['count_inFeatureValue']+1     \n",
    "                    update_cAppHist(nodes_dict[node][i][feature_val]['hist_inFeatureValue'], binId)\n",
    "        else:\n",
    "            nodes_dict.update( {node: {}})  \n",
    "            # new added: sum of y in a node\n",
    "            nodes_dict[node].update( { 'sumY': Y} )\n",
    "            nodes_dict[node].update( { 'count': 1} )\n",
    "            nodes_dict[node].update( { 'hist': {}} )\n",
    "            ini_newBin_cAppHist(nodes_dict[node]['hist'],binId)\n",
    "        \n",
    "            for i in range(0,numFeatures):\n",
    "                feature_val= dvAndfeatures[i+1]\n",
    "                \n",
    "                nodes_dict[node].update( { i: {}} )    \n",
    "                nodes_dict[node][i].update( { feature_val: {}   }  )\n",
    "                nodes_dict[node][i][feature_val].update( {'count_inFeatureValue':1} )\n",
    "                nodes_dict[node][i][feature_val].update( {'hist_inFeatureValue': {} } )\n",
    "                \n",
    "                ini_newBin_cAppHist(nodes_dict[node][i][feature_val]['hist_inFeatureValue'], binId)      \n",
    "                        \n",
    "    return zip(nodes_dict.keys(), nodes_dict.values())\n",
    "\n",
    "def merge_parttion_combiner_cAppHist(nodeToFeatureToValue_1,nodeToFeatureToValue_2 ):\n",
    "# optimization: calculate median and amd for feature-value    \n",
    "    \n",
    "    nodeToFeatureToValue_1['sumY']= nodeToFeatureToValue_1['sumY']+\\\n",
    "    nodeToFeatureToValue_2['sumY']\n",
    "    nodeToFeatureToValue_1['count']= nodeToFeatureToValue_1['count']+\\\n",
    "    nodeToFeatureToValue_2['count'] \n",
    "    nodeToFeatureToValue_1['hist']=merge_cAppHist(nodeToFeatureToValue_1['hist'],nodeToFeatureToValue_2['hist'])\n",
    "    \n",
    "    for i in range(0, numFeatures): #feature\n",
    "        for j in nodeToFeatureToValue_1[i].keys(): #feature value\n",
    "            \n",
    "            feature_val=j\n",
    "            \n",
    "            if feature_val in nodeToFeatureToValue_2[i].keys():\n",
    "                nodeToFeatureToValue_1[i][j]['hist_inFeatureValue'] = \\\n",
    "                merge_cAppHist(nodeToFeatureToValue_1[i][j]['hist_inFeatureValue'],\\\n",
    "                               nodeToFeatureToValue_2[i][j]['hist_inFeatureValue'])\n",
    "                nodeToFeatureToValue_1[i][j]['count_inFeatureValue'] = \\\n",
    "                    nodeToFeatureToValue_1[i][j]['count_inFeatureValue']+ \\\n",
    "                    nodeToFeatureToValue_2[i][j]['count_inFeatureValue']\n",
    "                \n",
    "    for i in range(0, numFeatures):\n",
    "        for j in nodeToFeatureToValue_2[i].keys():\n",
    "            feature_val=j\n",
    "            if feature_val not in nodeToFeatureToValue_1[i].keys():\n",
    "                nodeToFeatureToValue_1[i].update({feature_val: {} })\n",
    "                nodeToFeatureToValue_1[i][feature_val]=nodeToFeatureToValue_2[i][feature_val].copy()\n",
    "    return  nodeToFeatureToValue_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0895049504950496, 7.6651430462562296)\n"
     ]
    }
   ],
   "source": [
    "#debug for median and mean absolute deviation \n",
    "\n",
    "tmphist= { 1:[100], 2:[30], 5:[22]   }\n",
    "\n",
    "print MAD_cAppHist( tmphist, 152 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.05\n",
      "1.1025\n",
      "1.2762815625\n"
     ]
    }
   ],
   "source": [
    "#debug for median and mean absolute deviation \n",
    "print math.pow(1.05,1)\n",
    "print math.pow(1.05,2)\n",
    "print math.pow(1.05,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split decision [on the local side]\n",
    "\n",
    "#implementation notes:\n",
    "# bit-operation to record the set of selected feature values in the left child\n",
    "\n",
    "#remove hist2 from hist1\n",
    "def minus_local_cAppHist(hist1, hist2):\n",
    "    \n",
    "    hist = copy.deepcopy(hist1)\n",
    "    for i in hist2.keys():\n",
    "        hist[i][0]= hist[i][0]-hist2[i][0]\n",
    "    return hist\n",
    "    \n",
    "def merge_local_cAppHist(hist1, hist2):\n",
    "    hist_local= copy.deepcopy(hist1)  \n",
    "    \n",
    "    for k in hist_local.keys():\n",
    "        if k in hist2.keys():\n",
    "            hist_local[k][0]= hist_local[k][0]+ hist2[k][0]\n",
    "    \n",
    "    for k in hist2.keys():\n",
    "        if k not in hist_local.keys():\n",
    "            hist_local.update({k:hist2[k]})\n",
    "    return hist_local\n",
    "\n",
    "#MAD: median absolute median in a histogram   \n",
    "def MAD_cAppHist(hist, cnt):\n",
    "    \n",
    "    sorted_hist= sorted(hist.items(), key= lambda line: line[0])\n",
    "    num_bins= len(hist)\n",
    "    \n",
    "    cntByBounds=0\n",
    "    curCnt=0\n",
    "    \n",
    "    mid_bin=0\n",
    "    midCnt= cnt/2.0\n",
    "    flag=0\n",
    "    cnt_beforeMedian=0\n",
    "    \n",
    "    meanAbsDevi=0\n",
    "    median=0\n",
    "    \n",
    "    for i in range(0, num_bins): \n",
    "        \n",
    "        if flag==0:\n",
    "            \n",
    "            tmp_curCnt=curCnt+ sorted_hist[i][1][0]\n",
    "            tmp_cntByBounds =cntByBounds - ( math.pow(cApp_ratio, sorted_hist[i][0])*(cApp_ratio+1))/2.0\\\n",
    "            *sorted_hist[i][1][0]\n",
    "        \n",
    "            if tmp_curCnt >= midCnt:\n",
    "                flag=1\n",
    "                mid_bin= i\n",
    "                cnt_beforeMedian = curCnt\n",
    "                \n",
    "            else:    \n",
    "                curCnt=tmp_curCnt\n",
    "                cntByBounds=tmp_cntByBounds\n",
    "        else:\n",
    "            curCnt= curCnt - sorted_hist[i][1][0]\n",
    "            cntByBounds= cntByBounds + \\\n",
    "            ( math.pow(cApp_ratio, sorted_hist[i][0])*(cApp_ratio+1) )/2.0*sorted_hist[i][1][0]\n",
    "\n",
    "    if cnt == 1:\n",
    "        return ( ( math.pow(cApp_ratio, sorted_hist[i][0])*(cApp_ratio+1))/2.0  ,0) \n",
    "    elif cnt ==0:\n",
    "        return (0,0)\n",
    "    else:\n",
    "        sample_inMedBin = midCnt - cnt_beforeMedian\n",
    "        median= math.pow(cApp_ratio, sorted_hist[mid_bin][0])+\\\n",
    "        ( math.pow(cApp_ratio, sorted_hist[mid_bin][0]+1)-math.pow(cApp_ratio, sorted_hist[mid_bin][0]) )/(sorted_hist[ mid_bin][1][0]+1)*sample_inMedBin \n",
    "    \n",
    "        meanAbsDevi =  cntByBounds + curCnt*median\n",
    "        \n",
    "        #debug\n",
    "#         print len(beforeMed)\n",
    "#         print 'MAD before median bin', beforeMed[0]*med + beforeMed[1]*bin_width\n",
    "        \n",
    "        sample_cnt = sorted_hist[mid_bin][1][0]\n",
    "        sample_interval=(math.pow(cApp_ratio, sorted_hist[mid_bin][0]+1)-math.pow(cApp_ratio, sorted_hist[mid_bin][0]) )/\\\n",
    "        (sorted_hist[ mid_bin][1][0]+1)\n",
    "    \n",
    "        sample_val= math.pow(cApp_ratio, sorted_hist[mid_bin][0])\n",
    "       \n",
    "        for i in range(0, sample_cnt):\n",
    "            sample_val= sample_val + sample_interval\n",
    "            meanAbsDevi= meanAbsDevi + abs( sample_val-median)\n",
    "        \n",
    "        return (median,meanAbsDevi)\n",
    "\n",
    "\n",
    "def split_onOneFeature_cAppHist(values,count_node,hist_node,meanAbsDevi_node):\n",
    "        \n",
    "    bestSplitMetric =  meanAbsDevi_node \n",
    "\n",
    "    #this is a bit-record variable\n",
    "    leftSplit_valueSet =0 \n",
    "    leftSplit_count=0\n",
    "    \n",
    "    leftSplit_hist= {}\n",
    "    rightSplit_hist={}\n",
    "        \n",
    "    bestLeft=(-1,-1)\n",
    "    bestRight=(-1,-1)\n",
    "\n",
    "    #sort feature values according to median \n",
    "    sorted_value_map=[]\n",
    "     \n",
    "    for i in values.keys():\n",
    "        currentVal_hist_count= values[i]['count_inFeatureValue']\n",
    "        currentVal_hist= values[i]['hist_inFeatureValue']\n",
    "        tmpMedian= MAD_cAppHist(currentVal_hist,currentVal_hist_count)\n",
    "        sorted_value_map.append(( tmpMedian[0],i ))\n",
    "            \n",
    "    sorted_value_map.sort()\n",
    "    values_cnt= len(sorted_value_map)\n",
    "    \n",
    "    node_hist_backup= copy.deepcopy(hist_node)\n",
    "    \n",
    "    # scan the sorted feature values\n",
    "    for k in range(0,values_cnt-1):\n",
    "        \n",
    "        current_feature_value=sorted_value_map[k][1]\n",
    "        currentVal_hist_count= values[current_feature_value]['count_inFeatureValue']\n",
    "        currentVal_hist= values[current_feature_value]['hist_inFeatureValue']\n",
    "        \n",
    "       \n",
    "        \n",
    "        #prepare the histograms for left and right splits\n",
    "        leftSplit_count = leftSplit_count+currentVal_hist_count\n",
    "        # histograms for the left values\n",
    "        leftSplit_hist=merge_local_cAppHist( leftSplit_hist, currentVal_hist, )\n",
    "        # histograms for the right values\n",
    "        rightSplit_hist= minus_local_cAppHist(node_hist_backup, leftSplit_hist )\n",
    "    \n",
    "        left = MAD_cAppHist( leftSplit_hist, leftSplit_count)\n",
    "        right= MAD_cAppHist( rightSplit_hist, (count_node - leftSplit_count))\n",
    "        \n",
    "        leftMedian= left[0]\n",
    "        leftMetric= left[1]    \n",
    "        rightMedian= right[0]\n",
    "        rightMetric= right[1]\n",
    "        \n",
    "        #debug\n",
    "        tmpcnt=0\n",
    "        for j in leftSplit_hist.keys():\n",
    "            tmpcnt = tmpcnt + leftSplit_hist[j][0]\n",
    "        if tmpcnt != leftSplit_count:\n",
    "            print '!!!! problem in left split'\n",
    "            \n",
    "        tmpcnt=0\n",
    "        for j in rightSplit_hist.keys():\n",
    "            tmpcnt = tmpcnt + rightSplit_hist[j][0]\n",
    "        if tmpcnt != (count_node - leftSplit_count):\n",
    "            print '!!!! problem in right split'\n",
    "        \n",
    "        \n",
    "        #debug\n",
    "        if leftMedian<0 or rightMedian<0 or leftMetric <=0 or rightMetric <=0:\n",
    "            print '$$$$ problem in MAD calculation'\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "        current_splitMetric=1.0*leftSplit_count/count_node*leftMetric + \\\n",
    "                               1.0*(count_node - leftSplit_count)/count_node*rightMetric\n",
    "        \n",
    "        if current_splitMetric < bestSplitMetric:\n",
    "            \n",
    "            bestSplitMetric=current_splitMetric  \n",
    "            \n",
    "            bestLeft=left\n",
    "            bestRight=right\n",
    "            \n",
    "            #bit variable to record which value is chosen for the left child\n",
    "            leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)        \n",
    "    \n",
    "    return (leftSplit_valueSet, [bestLeft[0], bestRight[0], bestSplitMetric  ] )\n",
    "\n",
    "\n",
    "def find_bestSplit_cAppHist(local_aggre_nodes, current_NumNodes, nodes_tree, \\\n",
    "                        nodes_tree_test):\n",
    "    \n",
    "#     nodes_layer=[]\n",
    "    current_idx_nodes =0\n",
    "    \n",
    "    if local_aggre_nodes[ current_idx_nodes ][0] == -1:\n",
    "        current_idx_nodes=1\n",
    "    \n",
    "    #debug \n",
    "    tmpnode_cnt=[]\n",
    "    presplit=[]\n",
    "\n",
    "    \n",
    "    for i in range(0, current_NumNodes):\n",
    "        if local_aggre_nodes[ current_idx_nodes ][0] != i:\n",
    "#             nodes_layer.append( (-1,-1) )\n",
    "            nodes_tree.append( (-1,-1) )\n",
    "            continue\n",
    "\n",
    "        node_count= local_aggre_nodes[i][1]['count'] \n",
    "        node_hist=local_aggre_nodes[i][1]['hist'] \n",
    "        \n",
    "        \n",
    "        best_splitMetric_ini= MAD_cAppHist( node_hist, node_count)\n",
    "        best_split_sofar= best_splitMetric_ini[1]\n",
    "        preSplitMetric_node= best_splitMetric_ini[1]\n",
    "        best_split=(-1,[-1,-1,-1])\n",
    "        \n",
    "        #debug\n",
    "        tmpnode_cnt.append(node_count)\n",
    "        presplit.append( best_splitMetric_ini[1]/100000 )\n",
    "        \n",
    "        best_split_feature=-1\n",
    "        best_split_featureValueSet=-1\n",
    "        \n",
    "        for j in range(0,numFeatures):\n",
    "            \n",
    "            statisticToValues = local_aggre_nodes[i][1][j] \n",
    "            split=split_onOneFeature_cAppHist(statisticToValues,node_count, node_hist,preSplitMetric_node)\n",
    "            \n",
    "            if split[1][2] < best_split_sofar:\n",
    "                best_split=split\n",
    "                best_split_sofar=split[1][2]\n",
    "                best_split_feature = j\n",
    "                best_split_featureValueSet= split[0]\n",
    "   \n",
    "        \n",
    "        # for one-layer point assignment\n",
    "#         nodes_layer.append( (best_split_feature,  best_split_featureValueSet  )  )\n",
    "        \n",
    "        # for whole-tree point assignment\n",
    "        nodes_tree.append(  (best_split_feature,  best_split_featureValueSet  )      )\n",
    "        \n",
    "        # tree for predicting\n",
    "        nodes_tree_test.append(best_split[1][0])\n",
    "        nodes_tree_test.append(best_split[1][1])\n",
    "        \n",
    "        current_idx_nodes= current_idx_nodes+1\n",
    "    \n",
    "    #debug\n",
    "#     print tmpnode_cnt\n",
    "#     print presplit\n",
    "    \n",
    "#     return nodes_layer\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check function for distributed information collection\n",
    "\n",
    "def statistic_infor_check(local_aggre_nodes ):\n",
    "    \n",
    "    numNode = len(local_aggre_nodes)\n",
    "\n",
    "#     print numNode\n",
    "    tmpsum=0\n",
    "    for i in range(0, numNode):\n",
    "        tmpsum=tmpsum+local_aggre_nodes[i][1]['count']\n",
    "    if tmpsum != dta_train_cnt:\n",
    "        print '!!!!!! problem in node count'\n",
    "\n",
    "\n",
    "    tmpsum=0\n",
    "    for i in range(0, numNode):\n",
    "        #check node hist\n",
    "        node_count = local_aggre_nodes[i][1]['count']\n",
    "        tmpsum=0\n",
    "        for j in local_aggre_nodes[i][1]['hist']:\n",
    "            tmpsum= tmpsum+local_aggre_nodes[i][1]['hist'][j][0]\n",
    "        if tmpsum!= node_count:\n",
    "            print '$$$$$ problem in histogram of node'\n",
    "            \n",
    "        #check feature values\n",
    "        for j in range(0, numFeatures):\n",
    "            tmpsum=0\n",
    "            for k in local_aggre_nodes[i][1][j].keys():\n",
    "                value_hist= local_aggre_nodes[i][1][j][k]['hist_inFeatureValue']\n",
    "                value_cnt = local_aggre_nodes[i][1][j][k]['count_inFeatureValue']\n",
    "                \n",
    "                tmpsum= tmpsum+value_cnt\n",
    "            \n",
    "                #check feature-value hist\n",
    "                tmpsum1=0\n",
    "                for m in value_hist.keys():\n",
    "                    tmpsum1= tmpsum1 + value_hist[m][0]\n",
    "            \n",
    "                if tmpsum1 != value_cnt:\n",
    "                    print '+++++++++++ problem in feature-value histogram', tmpsum1, value_cnt\n",
    "            \n",
    "            \n",
    "            if tmpsum != node_count:\n",
    "                print '------problem !!!! in feature:', tmpsum, node_count    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current split decision: []\n",
      "0 -th level ----------------------- \n",
      "0 -th level running time:  5.71322894096 sec 0.133812904358 sec\n",
      "current split decision: [(2, 70988433612780846483815379501056L)]\n",
      "1 -th level ----------------------- \n",
      "1 -th level running time:  6.06703186035 sec 0.174139976501 sec\n",
      "current split decision: [(2, 70988433612780846483815379501056L), (1, 45056), (0, 40)]\n",
      "2 -th level ----------------------- \n",
      "2 -th level running time:  6.02945184708 sec 0.187134981155 sec\n",
      "current split decision: [(2, 70988433612780846483815379501056L), (1, 45056), (0, 40), (0, 28), (1, 16384), (0, 8), (1, 49152)]\n",
      "3 -th level ----------------------- \n",
      "3 -th level running time:  5.99491810799 sec 0.196682929993 sec\n",
      "current split decision: [(2, 70988433612780846483815379501056L), (1, 45056), (0, 40), (0, 28), (1, 16384), (0, 8), (1, 49152), (0, 20), (1, 32768), (0, 36), (0, 24), (1, 38912), (1, 24576), (0, 68), (2, 5070602400912917605986812821504L)]\n",
      "4 -th level ----------------------- \n",
      "4 -th level running time:  5.99157094955 sec 0.222578048706 sec\n",
      "current split decision: [(2, 70988433612780846483815379501056L), (1, 45056), (0, 40), (0, 28), (1, 16384), (0, 8), (1, 49152), (0, 20), (1, 32768), (0, 36), (0, 24), (1, 38912), (1, 24576), (0, 68), (2, 5070602400912917605986812821504L), (0, 4), (1, 32768), (2, 30423614405477505635920876929024L), (0, 64), (2, 60847228810955011271841753858048L), (0, 80), (0, 8), (2, 20282409603651670423947251286016L), (2, 5070602400912917605986812821504L), (1, 8192), (2, 5070602400912917605986812821504L), (2, 5070602400912917605986812821504L), (2, 5070602400912917605986812821504L), (2, 2535301200456458802993406410752L), (1, 12288), (0, 16)]\n",
      "5 -th level ----------------------- \n",
      "5 -th level running time:  6.03502106667 sec 0.289712905884 sec\n",
      "current split decision: [(2, 70988433612780846483815379501056L), (1, 45056), (0, 40), (0, 28), (1, 16384), (0, 8), (1, 49152), (0, 20), (1, 32768), (0, 36), (0, 24), (1, 38912), (1, 24576), (0, 68), (2, 5070602400912917605986812821504L), (0, 4), (1, 32768), (2, 30423614405477505635920876929024L), (0, 64), (2, 60847228810955011271841753858048L), (0, 80), (0, 8), (2, 20282409603651670423947251286016L), (2, 5070602400912917605986812821504L), (1, 8192), (2, 5070602400912917605986812821504L), (2, 5070602400912917605986812821504L), (2, 5070602400912917605986812821504L), (2, 2535301200456458802993406410752L), (1, 12288), (0, 16), (1, 8192), (1, 40960), (2, 10141204801825835211973625643008L), (2, 40564819207303340847894502572032L), (2, 10141204801825835211973625643008L), (0, 32), (2, 10141204801825835211973625643008L), (1, 4096), (2, 20282409603651670423947251286016L), (0, 4), (0, 64), (2, 20282409603651670423947251286016L), (2, 40564819207303340847894502572032L), (2, 10141204801825835211973625643008L), (0, 68), (0, 32), (1, 34816), (1, 36864), (2, 2535301200456458802993406410752L), (2, 2535301200456458802993406410752L), (1, 8192), (1, 16384), (1, 2048), (1, 34816), (1, 32768), (1, 32768), (1, 32768), (1, 16384), (1, 4096), (0, 64), (1, 8192), (0, 4)]\n",
      "6 -th level ----------------------- \n",
      "6 -th level running time:  6.10657382011 sec 0.373625040054 sec\n"
     ]
    }
   ],
   "source": [
    "# training  process\n",
    "# current_layer=1hist\n",
    "\n",
    "#without outliers\n",
    "dta= dta.filter(lambda line: line[0]<1000)\n",
    "dta_train = dta.sample(False, .7, 12345)\n",
    "dta_test = dta.sample(False, .3, 43243)\n",
    "dta_train_cnt = dta_train.count()\n",
    "\n",
    "maxdepth=7\n",
    "numFeatures=3\n",
    "cApp_ratio = 1.05\n",
    "\n",
    "# dta_train = dta.sample(False, .7, 12345)\n",
    "# dta_test = dta.sample(False, .3, 43243)\n",
    "# dta_test = dta_test.filter(lambda line: line[0] <1000 )\n",
    "# dta_train_cnt = dta_train.count()\n",
    "\n",
    "\n",
    "nodeSplits_tree=[]\n",
    "node_tree_test=[]\n",
    "\n",
    "#ini parameter\n",
    "currentNode_split_fromMaster = sc.broadcast([])\n",
    "\n",
    "\n",
    "for i in range(0,maxdepth):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    current_NumNodes= int( math.pow(2,  i)) \n",
    "    print 'current split decision:',currentNode_split_fromMaster.value\n",
    "    \n",
    "    \n",
    "      #----------------------- debug ---------------\n",
    "    \n",
    "#     error_partitions = dta_train.mapPartitions( error_partition_combiner_hist )\n",
    "\n",
    "#     error_nodes =error_partitions.reduceByKey(lambda statis_partition_1, \\\n",
    "#                                               statis_partition_2: \n",
    "#                               error_merge_parttion_combiner_hist(statis_partition_1,\\\n",
    "#                                                       statis_partition_2 ))\n",
    "    \n",
    "#     local_error_nodes= error_nodes.collect()\n",
    "    \n",
    "#     local_sorted_error_nodes = sorted(local_error_nodes,key= lambda val:val[0])\n",
    "    \n",
    "# #     print 'current number of nodes:', len(local_sorted_error_nodes)\n",
    "#     tmpres=[]\n",
    "#     tmpcnt=[]\n",
    "#     for k in range(0, len(local_sorted_error_nodes)):   \n",
    "            \n",
    "#         tmpvar=local_sorted_error_nodes[k][1]['sumYsquare']/local_sorted_error_nodes[k][1]['count']\\\n",
    "#         -(local_sorted_error_nodes[k][1]['sumY']/local_sorted_error_nodes[k][1]['count'])*\\\n",
    "#         (local_sorted_error_nodes[k][1]['sumY']/local_sorted_error_nodes[k][1]['count'])\n",
    "        \n",
    "#         tmpres.append( tmpvar/ 100000.0 )\n",
    "#         tmpcnt.append(local_sorted_error_nodes[k][1]['count'])\n",
    "        \n",
    "#     print 'before split:',tmpres\n",
    "#     print 'before split:', tmpcnt\n",
    "    \n",
    "    #---------------------------------------------\n",
    "    \n",
    "    \n",
    "    statis_partitions = dta_train.mapPartitions( partition_combiner_cAppHist )\n",
    "    aggre_nodes =statis_partitions.reduceByKey(lambda statis_partition_1, statis_partition_2: \n",
    "                              merge_parttion_combiner_cAppHist(statis_partition_1,statis_partition_2 ))\n",
    "    \n",
    "    local_aggre_nodes= aggre_nodes.collect()\n",
    "    \n",
    "    cluster_end=time.time()\n",
    "    \n",
    "    #debug\n",
    "#     statistic_infor_check(local_aggre_nodes)\n",
    "    \n",
    "    \n",
    "    nodeSplits_layer=find_bestSplit_cAppHist(sorted(local_aggre_nodes,key= lambda val:val[0])\\\n",
    "                                    ,current_NumNodes,nodeSplits_tree,node_tree_test)\n",
    "    \n",
    "    \n",
    "    print i,'-th level ----------------------- '\n",
    "    \n",
    "    \n",
    "    #debug\n",
    "#     statistic_infor_check(local_aggre_nodes)\n",
    "    \n",
    "    \n",
    "    #for whole tree\n",
    "    currentNode_split_fromMaster = sc.broadcast(nodeSplits_tree)\n",
    "    \n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "\n",
    "    print i,'-th level running time: ', cluster_end - start,'sec', end- cluster_end, 'sec'\n",
    "    \n",
    "    \n",
    "    #----------------------- debug ---------------\n",
    "    \n",
    "#     error_partitions = dta_train.mapPartitions( error_partition_combiner )\n",
    "\n",
    "#     error_nodes =error_partitions.reduceByKey(lambda statis_partition_1, \\\n",
    "#                                               statis_partition_2: \n",
    "#                               error_merge_parttion_combiner(statis_partition_1,\\\n",
    "#                                                       statis_partition_2 ))\n",
    "    \n",
    "#     local_error_nodes= error_nodes.collect()\n",
    "   \n",
    "    \n",
    "#     local_sorted_error_nodes = sorted(local_error_nodes,key= lambda val:val[0])\n",
    "    \n",
    "# #     print 'current number of nodes:', len(local_sorted_error_nodes)\n",
    "#     tmpres=[]\n",
    "#     tmpcnt=[]\n",
    "    \n",
    "#     for k in range(0, len(local_sorted_error_nodes)):\n",
    "        \n",
    "#         tmpweight=0.0\n",
    "        \n",
    "#         if len(local_sorted_error_nodes) > 1:\n",
    "#             if (k%2) == 0 :\n",
    "#                 tmpweight= local_sorted_error_nodes[k][1]['count']+ \\\n",
    "#                 local_sorted_error_nodes[k+1][1]['count']\n",
    "#             else:\n",
    "#                 tmpweight= local_sorted_error_nodes[k][1]['count']+ \\\n",
    "#                 local_sorted_error_nodes[k-1][1]['count']\n",
    "#             weight=1.0*local_sorted_error_nodes[k][1]['count']/tmpweight\n",
    "            \n",
    "#         else:\n",
    "#             weight=1.0    \n",
    "            \n",
    "# #       weight=1.0     \n",
    "            \n",
    "#         tmpvar=local_sorted_error_nodes[k][1]['sumYsquare']/local_sorted_error_nodes[k][1]['count']\\\n",
    "#         -(local_sorted_error_nodes[k][1]['sumY']/local_sorted_error_nodes[k][1]['count'])*\\\n",
    "#         (local_sorted_error_nodes[k][1]['sumY']/local_sorted_error_nodes[k][1]['count'])\n",
    "        \n",
    "#         tmpvar= weight*tmpvar/ 100000.0\n",
    "        \n",
    "#         tmpres.append( tmpvar )\n",
    "#         tmpcnt.append( local_sorted_error_nodes[k][1]['count'] )\n",
    "    \n",
    "#     reduced_error=[]\n",
    "#     for k in range(0, len(local_sorted_error_nodes),2):\n",
    "#         reduced_error.append( tmpres[k]+tmpres[k+1]    )\n",
    "        \n",
    "#     print 'after split:',reduced_error\n",
    "#     print 'after split:',tmpcnt\n",
    "    \n",
    "    #---------------------------------------------\n",
    "    \n",
    "    #for layer of nodes\n",
    "    #currentNode_split_fromMaster = sc.broadcast(nodeSplits_layer)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11.994305061338633, 23.009706257677312, 11.655536813488723, 69.84793457774158, 33.970533614922516, 48.492409983298316, 46.046167205380634, 89.03330229508093, -1, -1, 37.9673170956686, 41.078971578491355, 41.92198902333693, 50.91803323388012, 89.87058265734697, 98.61105368984497, 23.984570496636056, 25.993116258902663, 24.99546743026356, 36.02168009938599, -1, -1, -1, -1, 29.941250991151218, 48.13159447816529, 54.63663977198971, 62.56559834309815, 49.572230304274434, 92.32288789196225, 75.4566953990882, 91.10198522909808, 7.994789188044853, 14.99877063084032, 19.992255293070812, 24.047430752148493, -1, -1, -1, -1, 5.983774730128319, 60.607909301800106, 24.00897032674882, 56.00933093356765, -1, -1, 100.50802056518128, 100.62802027357161, -1, -1, 48.14416054764111, 48.81883602817873, -1, -1, 64.37665583255757, 78.85825987624672, 46.01924255221292, 46.89233395412004, -1, -1, 28.047808227035826, 71.2929409750476, 69.35418159504353, 80.77412009636357, 14.001900888024508, 29.028258838286746, -1, -1, 5.968191545750538, 29.964292506727237, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 56.7755982496124, 77.20758548334669, 58.33770618695728, 64.39669271613481, -1, -1, 16.992302461728013, 74.54395083909282, 32.017239082472436, 48.79888502993861, 43.11425103041064, 79.37642247537737, 37.9717454230718, 71.28865507636534, -1, -1, -1, -1, -1, -1, -1, -1, 36.024894781458755, 73.01936569671606, 53.78398191728785, 82.495642756973, -1, -1, 95.63348470881509, 96.11700900148895, -1, -1, 70.32974681178663, 71.6867404552115, 86.49664406743268, 91.19956913485215, 74.51894700074955, 102.28742647397038]\n"
     ]
    }
   ],
   "source": [
    "leaf_nodes = node_tree_test[ len(node_tree_test)-  (int)( math.pow(2,maxdepth)): len(node_tree_test)  ]\n",
    "print leaf_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with outliers:\n",
      "160.723202855 55.2350485478\n",
      "1017655.01623 1028782.76693\n",
      "165.227786328 55.0262689108\n",
      "1062693.64813 1074838.02257\n",
      "\n",
      "\n",
      "without outliers:\n",
      "57.3384721362 54.3359499795\n",
      "762.073426148 771.08856545\n",
      "57.3911037204 54.7313674314\n",
      "762.666948 769.741145127\n"
     ]
    }
   ],
   "source": [
    "# initial mse and mad before training\n",
    "\n",
    "#------small dataset:\n",
    "\n",
    "# with outliers\n",
    "tmpdta = dta.takeSample(False, 100000, 12243)\n",
    "dta_train = sc.parallelize(tmpdta).cache().sample(False, .7, 12345)\n",
    "dta_test = sc.parallelize(tmpdta).cache().sample(False, .3, 43243)\n",
    "\n",
    "# without outliers\n",
    "tmpdta = dta.filter(lambda line: line[0]<1000).takeSample(False, 100000, 12243)\n",
    "dta_train_clean = sc.parallelize(tmpdta).cache().sample(False, .7, 12345)\n",
    "dta_test_clean =  sc.parallelize(tmpdta).cache().sample(False, .3, 43243)\n",
    "\n",
    "\n",
    "def ini_mse_mad( dataset):\n",
    "    ycnt= dataset.count()\n",
    "    ysum = dataset.map( lambda line: line[0]).reduce(lambda a,b: a+b)\n",
    " \n",
    "    ymean= 1.0*ysum/ycnt\n",
    "\n",
    "    ymedian_arr= dataset.map(lambda line: line[0]).top( ycnt/2 )\n",
    "    ymedian_arr.sort()\n",
    "    ymedian= ymedian_arr[0]\n",
    "\n",
    "    print ymean, ymedian\n",
    "\n",
    "    mse_mean= dataset.map( lambda line: (line[0]- ymean)* (line[0]- ymean) ).reduce(lambda a,b: a+b)/ycnt\n",
    "    mse_median= dataset.map( lambda line: (line[0]- ymedian)*(line[0]- ymedian) ).reduce(lambda a,b: a+b)/ycnt\n",
    "    \n",
    "    print mse_mean, mse_median\n",
    "    \n",
    "\n",
    "print 'with outliers:'\n",
    "ini_mse_mad( dta_train )    \n",
    "ini_mse_mad( dta_test )   \n",
    "\n",
    "print '\\n'\n",
    "\n",
    "print 'without outliers:'\n",
    "ini_mse_mad( dta_train_clean )    \n",
    "ini_mse_mad( dta_test_clean ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "error at tree height 1 : 761.951848178 764.967209818\n",
      "1 1\n",
      "2 2\n",
      "error at tree height 2 : 738.278565973 740.393911689\n",
      "1 1\n",
      "2 2\n",
      "4 4\n",
      "error at tree height 3 : 702.962032729 700.997574111\n",
      "1 1\n",
      "2 2\n",
      "4 4\n",
      "8 8\n",
      "error at tree height 4 : 651.532592237 650.900407512\n",
      "1 1\n",
      "2 2\n",
      "4 4\n",
      "8 8\n",
      "16 16\n",
      "error at tree height 5 : 486.690084516 483.046465661\n",
      "1 1\n",
      "2 2\n",
      "4 4\n",
      "8 8\n",
      "16 16\n",
      "32 32\n",
      "error at tree height 6 : 183.404081484 178.225891752\n",
      "1 1\n",
      "2 2\n",
      "4 4\n",
      "8 8\n",
      "16 16\n",
      "32 32\n",
      "64 64\n",
      "error at tree height 7 : 532.403199355 531.632788609\n",
      "1 1\n",
      "2 2\n",
      "4 4\n",
      "8 8\n",
      "16 16\n",
      "32 32\n",
      "64 64\n",
      "75 128\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-62f8a5af11a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_aggre_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_NumNodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mnodeSplits_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfind_bestSplit_cAppHist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_aggre_nodes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m                                    \u001b[1;33m,\u001b[0m\u001b[0mcurrent_NumNodes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnodeSplits_tree\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnode_tree_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[1;31m#for whole tree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mcurrentNode_split_fromMaster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnodeSplits_tree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-699afbcf6077>\u001b[0m in \u001b[0;36mfind_bestSplit_cAppHist\u001b[1;34m(local_aggre_nodes, current_NumNodes, nodes_tree, nodes_tree_test)\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mnode_count\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mlocal_aggre_nodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'count'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m         \u001b[0mnode_hist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal_aggre_nodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hist'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# training and testing process\n",
    "\n",
    "#------big dataset:\n",
    "\n",
    "#with outliers\n",
    "# dta_train = dta.sample(False, .7, 12345)\n",
    "# dta_test = dta.sample(False, .3, 43243)\n",
    "# dta_test = dta_test.filter(lambda line: line[0] <1000 )\n",
    "\n",
    "#without outliers\n",
    "# dta= dta.filter(lambda line: line[0]<1000)\n",
    "# dta_train = dta.sample(False, .7, 12345)\n",
    "# dta_test = dta.sample(False, .3, 43243)\n",
    "# dta_test = dta_test.filter(lambda line: line[0] <1000 )\n",
    "\n",
    "#------small dataset:\n",
    "\n",
    "# with outliers\n",
    "# tmpdta = dta.takeSample(False, 100000, 12243)\n",
    "# dta_train = sc.parallelize(tmpdta).cache().sample(False, .7, 12345)\n",
    "# dta_test = sc.parallelize(tmpdta).cache().sample(False, .3, 43243)\n",
    "\n",
    "# # without outliers\n",
    "tmpdta = dta.filter(lambda line: line[0]<1000).takeSample(False, 100000, 12243)\n",
    "dta_train = sc.parallelize(tmpdta).cache().sample(False, .7, 12345)\n",
    "dta_test =  sc.parallelize(tmpdta).cache().sample(False, .3, 43243)\n",
    "\n",
    "\n",
    "maxdepth=10\n",
    "numFeatures=3\n",
    "cApp_ratio = 1.05\n",
    "\n",
    "\n",
    "def search_nodeToData(features, tree):\n",
    "    nodeNum=len(tree)   \n",
    "    if nodeNum == 0:\n",
    "        return 0;\n",
    "    \n",
    "    current_nodeIdx=0\n",
    "    while current_nodeIdx< nodeNum:\n",
    "        split_feature= tree[current_nodeIdx][0]\n",
    "        split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "        if split_feature==-1:\n",
    "            return -1\n",
    "        if (( 1<<features[split_feature]) &  split_valueSet) != 0 :\n",
    "            current_nodeIdx=current_nodeIdx*2+1\n",
    "        else:\n",
    "            current_nodeIdx=current_nodeIdx*2+2        \n",
    "    return current_nodeIdx - nodeNum\n",
    "\n",
    "def tree_test_mapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], nodeSplits_tree)\n",
    "    return (leaf_nodes[ tmpnode ]  - line[0])*(leaf_nodes[ tmpnode ]  - line[0])\n",
    "    \n",
    "def tree_test( testData_rdd ):\n",
    "    err_rdd=testData_rdd.map( lambda line: tree_test_mapFunc(line))\n",
    "    \n",
    "    #debug\n",
    "#     print err_rdd.take(20)\n",
    "    \n",
    "    err_sum = err_rdd.reduce( lambda a,b: a+b)\n",
    "    return err_sum / testData_rdd.count()\n",
    "\n",
    "\n",
    "#debug\n",
    "def debug_tree_test_mapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], nodeSplits_tree)\n",
    "    return (leaf_nodes[ tmpnode ], line[0])\n",
    "    \n",
    "def debug_tree_test( testData_rdd ):\n",
    "    err_rdd=testData_rdd.map( lambda line: debug_tree_test_mapFunc(line))\n",
    "#     print err_rdd.take(20)\n",
    "    \n",
    "    \n",
    "#ini parameter on the cluster\n",
    "currentNode_split_fromMaster = sc.broadcast([])\n",
    "test_err = []\n",
    "train_err = []\n",
    "run_time=[]\n",
    "\n",
    "for i in range(1,maxdepth):\n",
    "    nodeSplits_tree=[]\n",
    "    node_tree_test=[]\n",
    "    currentNode_split_fromMaster = sc.broadcast([])\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for cur_depth in range(0,i):\n",
    "        \n",
    "        current_NumNodes= int( math.pow(2,  cur_depth)) \n",
    "        \n",
    "        statis_partitions = dta_train.mapPartitions( partition_combiner_cAppHist )\n",
    "        aggre_nodes =statis_partitions.reduceByKey(lambda statis_partition_1, statis_partition_2: \n",
    "                              merge_parttion_combiner_cAppHist(statis_partition_1,statis_partition_2 ))\n",
    "        local_aggre_nodes= aggre_nodes.collect()\n",
    "        cluster_end=time.time()\n",
    "        \n",
    "        #debug\n",
    "        print len(local_aggre_nodes), current_NumNodes\n",
    "    \n",
    "        nodeSplits_layer=find_bestSplit_cAppHist(sorted(local_aggre_nodes,key= lambda val:val[0])\\\n",
    "                                    ,current_NumNodes,nodeSplits_tree,node_tree_test)\n",
    "        #for whole tree\n",
    "        currentNode_split_fromMaster = sc.broadcast(nodeSplits_tree)\n",
    "        \n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "\n",
    "    leaf_nodes = node_tree_test[ len(node_tree_test)-  (int)( math.pow(2,i)): len(node_tree_test)  ]\n",
    "    \n",
    "    tmp_test_err= tree_test( dta_test )\n",
    "    tmp_train_err= tree_test( dta_train )\n",
    "    \n",
    "    #debug\n",
    "    debug_tree_test( dta_test )\n",
    "    \n",
    "    test_err.append( tmp_test_err)\n",
    "    train_err.append( tmp_train_err )\n",
    "    run_time.append( elapsed )\n",
    "    print \"error at tree height\", i,\":\",  tmp_test_err, tmp_train_err\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1074437.8287446771, 1074565.7027683556, 1075109.9780677597, 1074537.4393239901, 1074242.333610338, 1073387.8549230846, 1072270.5637668928]\n",
      "[1028415.244310336, 1028540.0697712521, 1028884.5523299589, 1028339.3640963054, 1027913.7543517951, 1027202.2369429773, 1026151.4573390299]\n",
      "[3.839794874191284, 7.201169013977051, 1.8741850852966309, 2.3509421348571777, 3.0638809204101562, 3.8096978664398193, 4.051553964614868]\n"
     ]
    }
   ],
   "source": [
    "#both with outliers \n",
    "print  test_err\n",
    "print  train_err\n",
    "print  run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[761.9518481781977, 738.2785659728175, 702.9620327293563, 651.5325922367125, 486.69008451630873, 183.40408148431095, 532.4031993554111]\n",
      "[764.9672098181824, 740.3939116892855, 700.9975741112505, 650.90040751234, 483.04646566080595, 178.22589175168397, 531.6327886088019]\n",
      "[0.7894189357757568, 1.2197539806365967, 1.9294650554656982, 2.376145124435425, 3.0789270401000977, 3.6338610649108887, 4.388199090957642]\n"
     ]
    }
   ],
   "source": [
    "#both without outliers \n",
    "print  test_err\n",
    "print  train_err\n",
    "print  run_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
