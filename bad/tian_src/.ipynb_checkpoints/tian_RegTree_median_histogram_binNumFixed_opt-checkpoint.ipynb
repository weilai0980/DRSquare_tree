{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os \n",
    "import sys \n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "from datetime  import datetime \n",
    "\n",
    "import numpy as np  # learn \n",
    "import pandas as pd # learn\n",
    "from pandas import *\n",
    "from numpy import *\n",
    "\n",
    "from scipy import stats # look at scipy\n",
    "from scipy import linalg\n",
    "from scipy import *\n",
    "\n",
    " \n",
    "import matplotlib as mplt # learn matplolib \n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "import seaborn as sns \n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc={\"figure.figsize\": (14, 6)})\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import sklearn as sk\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "from heapq import heappush, heappop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328.0, 10, 10, 2, 2, 8, 1, 3)\n",
      "3315067\n"
     ]
    }
   ],
   "source": [
    "#BT data loading\n",
    "\n",
    "# dta_RDD = sc.textFile(\"file:///home/tguo/data/tian-test/udpjitter-H1april2014_regTree.csv\")\n",
    "\n",
    "dta_RDD = sc.textFile(\"hdfs://computer61.ant-net/user/tguo/udpjitter-H1april2014_regTree_mllib.csv\")\n",
    "\n",
    "\n",
    "dta_splited = dta_RDD.map(lambda line: line.split(\",\")).map(lambda r:\n",
    "                                                                (float(r[0]),int( float(r[1])), \n",
    "                                                                 int(float(r[2])),int(float(r[3])),\n",
    "                                                                 int(float(r[4])),int(float(r[5])),\n",
    "                                                                 int(float(r[6])),int(float(r[7])) ))\n",
    "# dta_splited.first()\n",
    "\n",
    "dta= dta_splited\n",
    "dta.cache()\n",
    "print dta.first()\n",
    "print dta.count()\n",
    "\n",
    "\n",
    "# dta_filtered = dta.filter(lambda line: line[0] <2000 )\n",
    "# dta_filtered.cache()\n",
    "# dta_filtered.count()\n",
    "# print dta.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "a=1\n",
    "b=[1,2,3,4,4,2]\n",
    "print b[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate sythetic data\n",
    "\n",
    "from SyntheticDataGenerator import *\n",
    "\n",
    "filename = './synthetic_data_6zeros_0.01percen.txt'\n",
    "rows = 1000000\n",
    "cols = 3\n",
    "\n",
    "data = SyntheticDataGenerator(rows, cols) \n",
    "data.writeData(filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: [(72.4117614075, 4, 12, 103), (86.9859192762, 5, 14, 104), (87.3043928999, 5, 14, 104), (43.0594861041, 2, 12, 101), (85.4633714471, 6, 15, 101), (66.9857762839, 4, 15, 102), (76.8275284345, 2, 11, 104), (29.3306427401, 5, 11, 102), (66.2259821558, 3, 13, 101), (64.5693767784, 3, 12, 104)]\n",
      "1000000\n"
     ]
    }
   ],
   "source": [
    "# synthetic data\n",
    "dta_RDD = sc.textFile(\"file:///home/tguo/tian_src/synthetic_data_6zeros_0.01percen.txt\")\n",
    "dta_RDD.cache()\n",
    "\n",
    "dta_splited = dta_RDD.map(lambda line: line.split(\",\")).map(lambda r: ( float(r[3]),int(r[0]),int(r[1]),int(r[2])) )\n",
    "# ( float(r[2]),float(r[0]),float(r[1]) ) \n",
    "\n",
    "dta= dta_splited\n",
    "dta.cache()\n",
    "\n",
    "print 'original:',dta.take(10)\n",
    "print dta.count()\n",
    "\n",
    "# re-set index of categorical features\n",
    "\n",
    "# feature_dist=[]\n",
    "\n",
    "# tmp1 = dta.map(lambda line: line[1]).distinct().collect()\n",
    "# feature_dist.append(tmp1)\n",
    "\n",
    "# tmp1 = dta.map(lambda line: line[2]).distinct().collect()\n",
    "# feature_dist.append(tmp1)\n",
    "\n",
    "# tmp1 = dta.map(lambda line: line[3]).distinct().collect()\n",
    "# feature_dist.append(tmp1)\n",
    "\n",
    "# fea_cnt= len(feature_dist )\n",
    "# fea_map=[]\n",
    "\n",
    "# for i in range(0, fea_cnt):\n",
    "#     tmpcnt = len(feature_dist[i])\n",
    "#     val_map= dict( zip(feature_dist[i], range(0,tmpcnt)) )\n",
    "#     fea_map.append(val_map)\n",
    "\n",
    "# def reset_index( line ):\n",
    "#     tmp=[]\n",
    "#     tmp.append(line[0])\n",
    "#     for i in range(1,4):\n",
    "#         tmp.append(fea_map[i-1][ line[i] ] )\n",
    "#     return tmp\n",
    "\n",
    "# dta = dta.map( lambda line: reset_index(line)  )\n",
    "\n",
    "# print 'feature value re-indexed:',dta.first()\n",
    "# print dta.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [1, 10, 8], 12: [12, 25, 8], 100: [100, 100, 1]}\n"
     ]
    }
   ],
   "source": [
    "#debug: histogram operations\n",
    "bin_num =3\n",
    "\n",
    "tmphis1={  1:[1,10,8] , 20:[20,25,4],  12:[12,17,4]  }\n",
    "\n",
    "update_hist(tmphis1, 100)\n",
    "\n",
    "print tmphis1\n",
    "\n",
    "\n",
    "# update_hist(tmphis1, 0.5)\n",
    "\n",
    "# print tmphis1\n",
    "\n",
    "# update_hist(tmphis1, 600)\n",
    "\n",
    "# print tmphis1\n",
    "\n",
    "bin_num=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 2, 3], [4, 5, 6], [8, 6, 4]]\n",
      "[[4, 5, 6], [8, 6, 4], [10, 2, 3]]\n",
      "None\n",
      "[4, 5, 6]\n",
      "[8, 6, 4]\n",
      "[10, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "#debug\n",
    "\n",
    "a=[ [10,2,3],[4,5,6],[8,6,4]]\n",
    "print a\n",
    "b=a.sort()\n",
    "print a\n",
    "print b\n",
    "\n",
    "for i in a:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, ['c', 'd'])\n",
      "(5, [2])\n",
      "(100, ['a', 'b'])\n"
     ]
    }
   ],
   "source": [
    "#debug\n",
    "\n",
    "\n",
    "heap = []\n",
    "data = [(100, ['a','b'] ), (2,['c','d'] ), (5, [2]) ]\n",
    "for item in data:\n",
    "    heappush(heap, item)\n",
    "\n",
    "while heap:\n",
    "    print heappop(heap)\n",
    "\n",
    "# ordered = []\n",
    "# # while heap:\n",
    "# #     print heappop(heap)\n",
    "    \n",
    "# print heap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 5, 100]\n"
     ]
    }
   ],
   "source": [
    "#debug\n",
    "bin_num=3\n",
    "tmplist=[ [1,10,3],[ 34,213,12 ], [ 456,783,212 ] ]\n",
    "\n",
    "\n",
    "# for i in tmplist:\n",
    "# tmphist =update_hist_list(tmplist, 10000 )\n",
    "#     print tmp\n",
    "    \n",
    "# print tmplist\n",
    "\n",
    "a=[1,4,5]\n",
    "a.insert(3,100)\n",
    "print a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# basic functions in one-pass method  [on the cluster side]\n",
    "\n",
    "# assign each data instance to the node\n",
    "def search_nodeToData(features, tree):\n",
    "    \n",
    "    nodeNum=len(tree)   \n",
    "    if nodeNum == 0:\n",
    "        return 0;\n",
    "    \n",
    "    current_nodeIdx=0\n",
    "    while current_nodeIdx< nodeNum:\n",
    "        \n",
    "        split_feature= tree[current_nodeIdx][0]\n",
    "        split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "        # unbalanced tree grow\n",
    "        if split_feature==-1:\n",
    "            current_nodeIdx=current_nodeIdx*2+1+ random.randint(0, 1)\n",
    "            continue\n",
    "            \n",
    "        if (( 1<<features[split_feature]) &  split_valueSet) != 0 :\n",
    "            current_nodeIdx=current_nodeIdx*2+1\n",
    "        else:\n",
    "            current_nodeIdx=current_nodeIdx*2+2\n",
    "            \n",
    "    return current_nodeIdx - nodeNum\n",
    "\n",
    "def search_nodeToData_4test(features, tree):\n",
    "    \n",
    "    nodeNum=len(tree)   \n",
    "    if nodeNum == 0:\n",
    "        return 0;\n",
    "    \n",
    "    current_nodeIdx=0\n",
    "    while current_nodeIdx< nodeNum:\n",
    "        \n",
    "#         split_feature= tree[current_nodeIdx][0]\n",
    "#         split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "        current_nodeIdx=current_nodeIdx*2+1+ random.randint(0, 1)\n",
    "        \n",
    "        # unbalanced tree grow\n",
    "#         if split_feature==-1:\n",
    "#             current_nodeIdx=current_nodeIdx*2+1+ random.randint(0, 1)\n",
    "#             continue\n",
    "#         if (( 1<<features[split_feature]) &  split_valueSet) != 0 :\n",
    "#             current_nodeIdx=current_nodeIdx*2+1\n",
    "#         else:\n",
    "#             current_nodeIdx=current_nodeIdx*2+2\n",
    "            \n",
    "    return current_nodeIdx - nodeNum\n",
    "\n",
    "# ****\n",
    "def update_hist(hist, newY):\n",
    "\n",
    "    numbins=len(hist)\n",
    "    \n",
    "    if newY in hist.keys():\n",
    "        hist[newY][2]= hist[newY][2] +1 \n",
    "    else:\n",
    "        hist.update({newY: [newY,newY,1]})\n",
    "        if numbins+1 > bin_num:\n",
    "            hist.update({newY: [newY,newY,1]}) \n",
    "            #data in each bin: left bound, right bound, count\n",
    "        \n",
    "            sorted_binIds=hist.keys()\n",
    "            # binId is the lower bound of the value range of the bin\n",
    "            sorted_binIds.sort()\n",
    "        \n",
    "            mindis= sorted_binIds[1]- sorted_binIds[0]\n",
    "            mindis_bin_left=sorted_binIds[0]\n",
    "            mindis_bin_right=sorted_binIds[1]\n",
    "            \n",
    "            for i in range(1,numbins+1):\n",
    "                tmp= sorted_binIds[i]- sorted_binIds[i-1]\n",
    "                if tmp < mindis:\n",
    "                    mindis=tmp\n",
    "                    mindis_bin_left= sorted_binIds[i-1]\n",
    "                    mindis_bin_right= sorted_binIds[i]       \n",
    "            \n",
    "#           print hist_local[mindis_bin_right][1],hist_local[mindis_bin_left][1]\n",
    "            hist[mindis_bin_left][1] = max(hist[mindis_bin_right][1],hist[mindis_bin_left][1] )                        \n",
    "            hist[mindis_bin_left][2] = hist[mindis_bin_left][2] + \\\n",
    "            hist[mindis_bin_right][2]\n",
    "            del hist[mindis_bin_right]\n",
    "            \n",
    "    return hist\n",
    "\n",
    "# def update_hist_list_insert( hist_list, dis_heap, dis_set, newY):\n",
    "    \n",
    "#     pos= update_hist_list_lookup( hist_list, newY )\n",
    "#     cur_cnt= len(hist_list)\n",
    "    \n",
    "#     if pos ==0:\n",
    "#         tmpl=newY\n",
    "#         tmpr=hist_list[ pos ][1]\n",
    "#         heappush(dis_heap, ( abs( tmpr-tmpl ),  tmpl,tmpr, 0,1 ) )\n",
    "#         dis_set.add(  ( tmpl,tmpr )    )\n",
    "        \n",
    "#     elif pos > cur_cnt:\n",
    "#         tmpl=hist_list[cur_cnt-1][2]\n",
    "#         tmpr= newY\n",
    "#         heappush(dis_heap, ( abs( tmpr-tmpl ), tmpl,tmpr, cur_cnt-1, cur_cnt)     )\n",
    "#         dis_set.add(  ( tmpl,tmpr )    )                 \n",
    "#     else:\n",
    "#         tmpl= hist_list[pos-1 ]\n",
    "#         tmpr= hist_list[pos]               \n",
    "#         heappush(dis_heap, ( abs( tmpr-newY ),  tmpl,tmpr, pos-1,pos+1 ) )\n",
    "#         heappush(dis_heap, ( abs( newY-tmpl ),  tmpl,tmpr, pos-1,pos+1 ) )\n",
    "                 \n",
    "#         dis_set.discard(  ( tmpl,tmpr )    )\n",
    "#         dis_set.add( (tmpl, newY  ) )\n",
    "#         dis_set.add( (newY,tmpr  ) )\n",
    "#     return\n",
    "\n",
    "# # cnt, left, right\n",
    "# def update_hist_list_interface( hist_list, dis_heap, dis_set, newY ):\n",
    "    \n",
    "#     update_hist_list_insert( hist_list, dis_heap, dis_set, newY)\n",
    "    \n",
    "#     cnt= len(hist_list)+1\n",
    "#     if cnt< bin_num:\n",
    "#         return\n",
    "#     else:\n",
    "#         tmp_dis_tuple= heappop(dis_heap)\n",
    "#         while ( tmp_dis_tuple[1],tmp_dis_tuple[2] ) not in  dis_set:\n",
    "#                  tmp_dis_tuple= heappop(dis_heap)\n",
    "    \n",
    "#         dis_set.discard( ( tmp_dis_tuple[1],tmp_dis_tuple[2] )  )         \n",
    "#         lidx= tmp_dis_tuple[3]\n",
    "#         ridx= tmp_dis_tuple[4]\n",
    "                 \n",
    "#         tmpcnt= hist_list[lidx][0]+hist_list[ridx][0]\n",
    "#         tmpl= min( hist_list[lidx][1], hist_list[ridx][1]   )    \n",
    "#         tmpl= max( hist_list[lidx][2], hist_list[ridx][2]   )\n",
    "                 \n",
    "#         hist_list[lidx][0]=tmpcnt \n",
    "#         hist_list[lidx][1]=tmpl\n",
    "#         hist_list[lidx][2]=tmpr\n",
    "                 \n",
    "#         hist_list.pop( ridx  )\n",
    "    \n",
    "#     return\n",
    "\n",
    "def update_hist_list_lookup( hist_list, newY ):\n",
    "    \n",
    "    cnt= len(hist_list)\n",
    "    l=0\n",
    "    r= len(hist_list)-1\n",
    "    \n",
    "    while l<r-1:\n",
    "        mid= l+ (r-l)/2\n",
    "        \n",
    "        if hist_list[mid][0]> newY:\n",
    "            r=mid\n",
    "        elif newY > hist_list[mid][1]:\n",
    "            l=mid\n",
    "        elif  hist_list[mid][0]  <= newY and newY<= hist_list[mid][1]:\n",
    "            return [1,mid]\n",
    "    \n",
    "    if hist_list[l][0]  <= newY and newY<= hist_list[l][1]:\n",
    "            return [1,l]\n",
    "    \n",
    "    if l+1<cnt and hist_list[l+1][0]  <= newY and newY<= hist_list[l+1][1]:\n",
    "            return [1,l+1]\n",
    "\n",
    "    if newY < hist_list[l][0]:\n",
    "        return [0,l]\n",
    "    \n",
    "    if l+1<cnt and newY< hist_list[ l+1 ][0]:\n",
    "        return [0,l+1]\n",
    "    elif l+1<cnt and newY> hist_list[ l+1 ][1]:\n",
    "        return [0,l+2]\n",
    "    \n",
    "    return [0,l+1]\n",
    "\n",
    "\n",
    "def update_hist_list( hist_list, newY ):\n",
    "    \n",
    "    cnt= len(hist_list)\n",
    "    \n",
    "    if cnt==0:\n",
    "        hist_list.append([newY, newY,1])\n",
    "    else:\n",
    "        pos=update_hist_list_lookup( hist_list, newY )\n",
    "        tmpidx=pos[1]\n",
    "        \n",
    "        if pos[0]==1:\n",
    "            hist_list[tmpidx][2]= hist_list[tmpidx][2]+1\n",
    "        else:\n",
    "            hist_list.insert(tmpidx, [ newY,newY,1])\n",
    "            \n",
    "    tmpdis=-1\n",
    "    merge_l=0\n",
    "    merge_r=0\n",
    "    cnt=len(hist_list)\n",
    "    \n",
    "    minDis =  hist_list[cnt-1][1] - hist_list[0][0]\n",
    "    \n",
    "    if cnt> bin_num:\n",
    "        \n",
    "        for i in range(0,cnt-1):\n",
    "            tmpdis= hist_list[i+1][0]-hist_list[i][1]\n",
    "            if tmpdis<minDis:\n",
    "                merge_l=i\n",
    "                merge_r=i+1\n",
    "                minDis=tmpdis\n",
    "        \n",
    "        hist_list[merge_l][1]= max(hist_list[merge_l][1],  hist_list[merge_r][1])\n",
    "        hist_list[merge_l][2]= hist_list[merge_l][2] + hist_list[merge_r][2]\n",
    "        hist_list.pop(merge_r)\n",
    "        \n",
    "def partition_combiner_hist(list_dvAndfeatures):\n",
    "    \n",
    "    nodes_dict={}\n",
    "    \n",
    "    for dvAndfeatures in local_dvFeatures:\n",
    "        \n",
    "        Y= dvAndfeatures[0]\n",
    "        \n",
    "        node= search_nodeToData(dvAndfeatures[1:numFeatures+1], \\\n",
    "                                currentNode_split_fromMaster.value)\n",
    "        \n",
    "        if node in nodes_dict.keys():\n",
    "            \n",
    "            # new added: s um of y in a node\n",
    "            nodes_dict[node]['sumY'] = nodes_dict[node]['sumY']+Y \n",
    "#             nodes_dict[node]['sumSquY'] = nodes_dict[node]['sumSquY']+Y*Y \n",
    "            nodes_dict[node]['count'] = nodes_dict[node]['count']+1 \n",
    "            nodes_dict[node]['hist']= update_hist(nodes_dict[node]['hist'], Y)\n",
    "            \n",
    "            update_hist_list( nodes_dict[node]['hist_list'], Y)\n",
    "\n",
    "            \n",
    "            for i in range(0,numFeatures):    \n",
    "                feature_val= dvAndfeatures[i+1]\n",
    "                \n",
    "                if feature_val not in nodes_dict[node][i]:\n",
    "                    nodes_dict[node][i].update( { feature_val: {}   }  )\n",
    "    \n",
    "                    nodes_dict[node][i][feature_val].update( {'count_inFeatureValue':1} )\n",
    "                    nodes_dict[node][i][feature_val].update( {'hist_inFeatureValue': {} } )\n",
    "                    nodes_dict[node][i][feature_val]['hist_inFeatureValue'].update({Y:[Y,Y,1]})\n",
    "                \n",
    "                    nodes_dict[node][i][feature_val].update( {'hist_val_list': [] } )\n",
    "                    update_hist_list( nodes_dict[node][i][feature_val]['hist_val_list'], Y)\n",
    "                    \n",
    "                else:\n",
    "                    nodes_dict[node][i][feature_val]['count_inFeatureValue'] = \\\n",
    "                    nodes_dict[node][i][feature_val]['count_inFeatureValue']+1 \n",
    "                    nodes_dict[node][i][feature_val]['hist_inFeatureValue']=\\\n",
    "                    update_hist(nodes_dict[node][i][feature_val]['hist_inFeatureValue'], Y)\n",
    "                    \n",
    "                    update_hist_list( nodes_dict[node][i][feature_val]['hist_val_list'], Y)\n",
    "                \n",
    "        else:\n",
    "            nodes_dict.update( {node: {}})  \n",
    "            # new added: sum of y in a node\n",
    "            nodes_dict[node].update( { 'sumY': Y} )\n",
    "            nodes_dict[node].update( { 'count': 1} )\n",
    "            nodes_dict[node].update( { 'hist': {}} )\n",
    "            nodes_dict[node]['hist'].update({Y:[Y,Y,1]})  \n",
    "            \n",
    "            nodes_dict[node].update( { 'hist_list': []   } )  \n",
    "            update_hist_list( nodes_dict[node]['hist_list'], Y)\n",
    "    \n",
    "            for i in range(0,numFeatures):\n",
    "                feature_val= dvAndfeatures[i+1]\n",
    "                \n",
    "                nodes_dict[node].update( { i: {}} )    \n",
    "                nodes_dict[node][i].update( { feature_val: {}   }  )\n",
    "             \n",
    "                nodes_dict[node][i][feature_val].update( {'count_inFeatureValue':1} )\n",
    "                nodes_dict[node][i][feature_val].update( {'hist_inFeatureValue': {} } )\n",
    "                nodes_dict[node][i][feature_val]['hist_inFeatureValue'].update({Y:[Y,Y,1]}) \n",
    "                \n",
    "                nodes_dict[node][i][feature_val].update( {'hist_val_list': [] } )\n",
    "                update_hist_list( nodes_dict[node][i][feature_val]['hist_val_list'], Y)\n",
    "          \n",
    "    return zip(nodes_dict.keys(), nodes_dict.values())\n",
    "\n",
    "\n",
    "def merge_hist_list(hist1, hist2):\n",
    "    \n",
    "    tmphist=[]\n",
    "    p1=0\n",
    "    p2=0\n",
    "    c1=len(hist1)\n",
    "    c2=len(hist2)\n",
    "    cnt=0\n",
    "    \n",
    "    \n",
    "    if hist1[p1][0] < hist2[p2][0]:\n",
    "        tmphist.append( [0,0,0] )\n",
    "        tmphist[cnt][0]= hist1[p1][0]\n",
    "        tmphist[cnt][1]= hist1[p1][1]\n",
    "        tmphist[cnt][2]= hist1[p1][2]\n",
    "        p1=p1+1\n",
    "        cnt=cnt+1\n",
    "    else:\n",
    "        tmphist.append( [0,0,0] )\n",
    "        tmphist[cnt][0]= hist2[p2][0]\n",
    "        tmphist[cnt][1]= hist2[p2][1]\n",
    "        tmphist[cnt][2]= hist2[p2][2]\n",
    "        p2=p2+1\n",
    "        cnt=cnt+1\n",
    "    \n",
    "    minDis= 1000000000\n",
    "    merge_l=0\n",
    "    merge_r=0\n",
    "    \n",
    "    while p1<c1 and p2<c2:\n",
    "        \n",
    "        while p2<c2 and hist2[p2][0] <= tmphist[cnt-1][1]:\n",
    "            tmphist[cnt-1][1]= max(tmphist[cnt-1][1], hist2[p2][1])\n",
    "            tmphist[cnt-1][2]= tmphist[cnt-1][2] + hist2[p2][2]\n",
    "            p2=p2+1\n",
    "                \n",
    "        while p1<c1 and hist1[p1][0] <= tmphist[cnt-1][1]:\n",
    "            tmphist[cnt-1][1]= max(tmphist[cnt-1][1],  hist1[p1][1])\n",
    "            tmphist[cnt-1][2]= tmphist[cnt-1][2] + hist1[p1][2]\n",
    "            p1=p1+1\n",
    "        \n",
    "        if p1<c1 and p2<c2 and hist1[p1][0] < hist2[p2][0]:\n",
    "            tmphist.append( [0,0,0] )\n",
    "            tmphist[cnt][0]= hist1[p1][0]\n",
    "            tmphist[cnt][1]= hist1[p1][1]\n",
    "            tmphist[cnt][2]= hist1[p1][2]\n",
    "            p1=p1+1\n",
    "            cnt=cnt+1\n",
    "            \n",
    "            tmpdis=tmphist[cnt-1][0] - tmphist[cnt-2][1]\n",
    "            if tmpdis < minDis:\n",
    "                merge_l=cnt-2\n",
    "                merge_r=cnt-1\n",
    "                minDis= tmpdis\n",
    "                \n",
    "        elif p1<c1 and p2<c2 and hist1[p1][0] >= hist2[p2][0]:\n",
    "            tmphist.append( [0,0,0] )\n",
    "            tmphist[cnt][0]= hist2[p2][0]\n",
    "            tmphist[cnt][1]= hist2[p2][1]\n",
    "            tmphist[cnt][2]= hist2[p2][2]\n",
    "            p2=p2+1\n",
    "            cnt=cnt+1\n",
    "            \n",
    "            tmpdis=tmphist[cnt-1][0] - tmphist[cnt-2][1]\n",
    "            if tmpdis < minDis:\n",
    "                merge_l=cnt-2\n",
    "                merge_r=cnt-1\n",
    "                minDis= tmpdis\n",
    "                \n",
    "    while p1<c1:\n",
    "        \n",
    "        tmphist.append( [0,0,0] )\n",
    "        tmphist[cnt][0]= hist1[p1][0]\n",
    "        tmphist[cnt][1]= hist1[p1][1]\n",
    "        tmphist[cnt][2]= hist1[p1][2]\n",
    "        cnt=cnt+1\n",
    "        p1=p1+1\n",
    "        \n",
    "        tmpdis=tmphist[cnt-1][0] - tmphist[cnt-2][1]\n",
    "        if tmpdis < minDis:\n",
    "            merge_l=cnt-2\n",
    "            merge_r=cnt-1\n",
    "            minDis= tmpdis\n",
    "        \n",
    "    while p2<c2:\n",
    "        \n",
    "        tmphist.append( [0,0,0] )\n",
    "        tmphist[cnt][0]= hist2[p2][0]\n",
    "        tmphist[cnt][1]= hist2[p2][1]\n",
    "        tmphist[cnt][2]= hist2[p2][2]\n",
    "        cnt=cnt+1\n",
    "        p2=p2+1\n",
    "        \n",
    "        tmpdis=tmphist[cnt-1][0] - tmphist[cnt-2][1]\n",
    "        if tmpdis < minDis:\n",
    "            merge_l=cnt-2\n",
    "            merge_r=cnt-1\n",
    "            minDis= tmpdis\n",
    "\n",
    "    tmphist[merge_l][1]= max(tmphist[merge_l][1], tmphist[merge_r][1])\n",
    "    tmphist[merge_l][2]= tmphist[merge_l][2] + tmphist[merge_r][2]\n",
    "    tmphist.pop(merge_r)        \n",
    "            \n",
    "    return tmphist            \n",
    "\n",
    "# ****\n",
    "def merge_hist(hist1, hist2):\n",
    "    tmp=[]\n",
    "    concat_hist=hist1\n",
    "    \n",
    "    if len(hist1)==0:\n",
    "        return hist2\n",
    "    if len(hist2)==0:\n",
    "        return hist1\n",
    "\n",
    "    for i in concat_hist.keys():\n",
    "        if i in hist2.keys():\n",
    "            concat_hist.update( {i: [i,max( concat_hist[i][1], hist2[i][1]),concat_hist[i][2]+hist2[i][2]]})\n",
    "    \n",
    "    for i in hist2.keys():\n",
    "        if i not in concat_hist.keys():\n",
    "            concat_hist.update( {i:hist2[i]})\n",
    "\n",
    "    cnt_total= len(concat_hist)\n",
    "    if cnt_total <= bin_num:\n",
    "        return concat_hist\n",
    "    else:\n",
    "        bins=concat_hist.keys()\n",
    "        bins.sort()\n",
    "        \n",
    "        disl=[]\n",
    "        disr=[]\n",
    "        tmpdis=0\n",
    "        tmpleft=0\n",
    "        tmpright=0\n",
    "        \n",
    "        for i in range(0,cnt_total-1):\n",
    "            tmpleft= bins[i]\n",
    "            tmpright= bins[i+1]\n",
    "            tmpdis= tmpright-tmpleft \n",
    "            disl.append((tmpdis,tmpleft))\n",
    "            disr.append((tmpdis,tmpright))\n",
    "        disl.sort()\n",
    "        disr.sort()\n",
    "        \n",
    "        bin_num_toRemove=cnt_total - bin_num\n",
    "        disIdx=[]\n",
    "        cur=0\n",
    "        \n",
    "        for i in range(0,bin_num_toRemove):\n",
    "            while disl[cur][1] not in concat_hist.keys()  :\n",
    "                cur=cur+1\n",
    "            \n",
    "            tmphist1= concat_hist[disl[cur][1] ]\n",
    "            tmphist2= concat_hist[disr[cur][1] ]\n",
    "            tmpval1=tmphist1[1]\n",
    "            tmpval2=tmphist2[1]\n",
    "            tmphist1[1]= max(tmpval1, tmpval2)\n",
    "            tmphist1[2]=  tmphist1[2] + tmphist2[2]\n",
    "            del concat_hist[disr[cur][1]]\n",
    "            \n",
    "            cur=cur+1\n",
    "            \n",
    "    return concat_hist\n",
    "\n",
    "def merge_parttion_combiner_hist(nodeToFeatureToValue_1,nodeToFeatureToValue_2 ):\n",
    "# optimization: calculate median and amd for feature-value    \n",
    "    \n",
    "    # new added: sum of Y in a node\n",
    "    nodeToFeatureToValue_1['sumY']= nodeToFeatureToValue_1['sumY']+\\\n",
    "    nodeToFeatureToValue_2['sumY']\n",
    "    \n",
    "    nodeToFeatureToValue_1['count']= nodeToFeatureToValue_1['count']+\\\n",
    "    nodeToFeatureToValue_2['count'] \n",
    "    \n",
    "    nodeToFeatureToValue_1['hist']=merge_hist(nodeToFeatureToValue_1['hist'],nodeToFeatureToValue_2['hist'])\n",
    "    \n",
    "    nodeToFeatureToValue_1['hist_list']=merge_hist_list(nodeToFeatureToValue_1['hist_list'],\\\n",
    "                                                   nodeToFeatureToValue_2['hist_list'])\n",
    "    \n",
    "    for i in range(0, numFeatures): #feature\n",
    "        for j in nodeToFeatureToValue_1[i].keys(): #feature value\n",
    "            \n",
    "            feature_val=j\n",
    "            \n",
    "            if feature_val in nodeToFeatureToValue_2[i].keys():               \n",
    "                \n",
    "                nodeToFeatureToValue_1[i][j]['hist_inFeatureValue'] = \\\n",
    "                merge_hist(nodeToFeatureToValue_1[i][j]['hist_inFeatureValue'], \n",
    "                           nodeToFeatureToValue_2[i][j]['hist_inFeatureValue'])     \n",
    "                \n",
    "                nodeToFeatureToValue_1[i][j]['hist_list'] = \\\n",
    "                merge_hist_list(nodeToFeatureToValue_1[i][j]['hist_list'], \n",
    "                           nodeToFeatureToValue_2[i][j]['hist_list'])     \n",
    "                \n",
    "                \n",
    "                nodeToFeatureToValue_1[i][j]['count_inFeatureValue'] = nodeToFeatureToValue_1[i][j]['count_inFeatureValue']+nodeToFeatureToValue_2[i][j]['count_inFeatureValue']\n",
    "                \n",
    "    for i in range(0, numFeatures):\n",
    "        for j in nodeToFeatureToValue_2[i].keys():\n",
    "            \n",
    "            feature_val=j\n",
    "            \n",
    "            if feature_val not in nodeToFeatureToValue_1[i].keys():\n",
    "                nodeToFeatureToValue_1[i].update({feature_val: {} })\n",
    "                nodeToFeatureToValue_1[i][feature_val]= nodeToFeatureToValue_2[i][feature_val].copy()\n",
    "                              \n",
    "    return  nodeToFeatureToValue_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 210, 50], [234, 383, 155], [423, 783, 32]]\n"
     ]
    }
   ],
   "source": [
    "#debug\n",
    "bin_num=3\n",
    "\n",
    "tmplist1=[ [1,10,3],[ 34,113,12 ], [ 123,183,32 ],[ 423,783,32 ] ]\n",
    "\n",
    "tmplist2=[ [8,210,3],[ 234,263,12 ], [ 279,383,143 ] ]\n",
    "\n",
    "# for i in tmplist:\n",
    "print merge_hist_list(tmplist1, tmplist2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#debug: one-pass method\n",
    "dta_test = dta\n",
    "\n",
    "#tree=currentNode_split_fromMaster.value()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "statis_partitions = dta_test.mapPartitions( partition_combiner_hist )\n",
    "# local_statis_partition=statis_partitions.collect()\n",
    "aggre_nodes =statis_partitions.reduceByKey(lambda statis_partition_1, statis_partition_2: \n",
    "                              merge_parttion_combiner_hist(statis_partition_1,statis_partition_2 ))\n",
    "local_aggre_nodes= aggre_nodes.collect()\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "\n",
    "print elapsed,'sec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, [1, 10, 8]), (12, [12, 17, 4]), (20, [20, 25, 8])]\n",
      "1 12.0\n",
      "(14.0, 7.0)\n"
     ]
    }
   ],
   "source": [
    "# debug for absolute median deviation \n",
    "tmphis={  1:[1,10,8] , 20:[20,25,8],  12:[12,17,4]  }\n",
    "# print type(tmphis)\n",
    "# sorted_his= sorted(tmphis.items(), key= lambda line: line[0])\n",
    "\n",
    "print MAD_hist(tmphis, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split decision  [on the local side]\n",
    "\n",
    "#****    \n",
    "# MAD: median absolute median in a histogram   \n",
    "def MAD_hist(hist, cnt): \n",
    "\n",
    "    sorted_his= sorted(hist.items(), key= lambda line: line[0])\n",
    "    num_bins= len(hist)\n",
    "    \n",
    "    cntByBounds=0\n",
    "    curCnt=0 \n",
    "    midCnt= cnt/2.0\n",
    "    \n",
    "    mid_bin_idx=0\n",
    "    flag=0\n",
    "    \n",
    "    meanAbsDevi=0\n",
    "    median=0\n",
    "    \n",
    "    cntSum_beforeMid=0\n",
    "    \n",
    "    for i in range(0, num_bins): \n",
    "        \n",
    "        if flag==0:\n",
    "            tmp_curCnt=curCnt+ sorted_his[i][1][2]\n",
    "            tmp_cntByBounds =cntByBounds - \\\n",
    "        (sorted_his[i][1][0]+sorted_his[i][1][1])/2.0*sorted_his[i][1][2]\n",
    "        \n",
    "            if tmp_curCnt >= midCnt:\n",
    "                flag=1       \n",
    "                \n",
    "                mid_bin_idx= i\n",
    "                \n",
    "                cntSum_beforeMid = curCnt\n",
    "            else:    \n",
    "                curCnt=tmp_curCnt\n",
    "                cntByBounds=tmp_cntByBounds\n",
    "        else:\n",
    "            curCnt= curCnt - sorted_his[i][1][2]\n",
    "            cntByBounds= cntByBounds + \\\n",
    "            (sorted_his[i][1][0]+sorted_his[i][1][1])/2.0*sorted_his[i][1][2]\n",
    "    \n",
    "    if cnt == 1:\n",
    "        for tmpkey in hist.keys():\n",
    "            return (hist[tmpkey][0],0)\n",
    "    elif cnt ==0:\n",
    "        return (0,0)\n",
    "    else:        \n",
    "\n",
    "        sample_inMedBin = midCnt - cntSum_beforeMid     \n",
    "        \n",
    "        median= sorted_his[mid_bin_idx][1][0]*1.0 + \\\n",
    "        (1.0*sorted_his[mid_bin_idx][1][1]-1.0*sorted_his[mid_bin_idx][1][0])/(sorted_his[mid_bin_idx][1][2]+1)*sample_inMedBin\n",
    "        meanAbsDevi =  cntByBounds + curCnt*median \n",
    "        \n",
    "        sample_val= sorted_his[ mid_bin_idx][1][0]*1.0 \n",
    "        sample_interval=1.0*(sorted_his[mid_bin_idx][1][1]-sorted_his[mid_bin_idx][1][0])/(sorted_his[mid_bin_idx][1][2]\\\n",
    "                                                                          +1.0)\n",
    "        \n",
    "        for i in range(0, sorted_his[mid_bin_idx][1][2]):\n",
    "            sample_val= sample_val + sample_interval\n",
    "            meanAbsDevi=meanAbsDevi+ abs( sample_val-median)\n",
    "                        \n",
    "        #debug\n",
    "#         if meanAbsDevi <0:\n",
    "#             tmpsum=0\n",
    "#             for tmpkey in hist:\n",
    "#                 tmpsum= tmpsum+ hist[tmpkey][2]\n",
    "#             if tmpsum!=cnt:\n",
    "#                 print 'inside MAD calculation, number of data check:',tmpsum,cnt             \n",
    "#             print 'inside MAD calculation:', tmpMAD, cnt\n",
    "            \n",
    "        \n",
    "        return (median, meanAbsDevi*1.0/cnt)\n",
    "\n",
    "\n",
    "def local_merge_hist(hist1, hist2):\n",
    "    \n",
    "    local_hist= copy.deepcopy(hist1)\n",
    "    \n",
    "    for i in hist2.keys():\n",
    "        if i in local_hist.keys():\n",
    "            local_hist[i][2] = local_hist[i][2] +hist2[i][2]\n",
    "            local_hist[i][1]= max(local_hist[i][1], hist2[i][1])\n",
    "        else:\n",
    "            local_hist.update( {i: hist2[i]}  )\n",
    "    return local_hist\n",
    "    \n",
    "# ****\n",
    "def split_onOneFeature_hist( values_hist,node_data_cnt, node_hist):\n",
    "    \n",
    "    left_count=0\n",
    "    right_count= node_data_cnt\n",
    "    \n",
    "    if len(values_hist)<=1:\n",
    "        return [-1,-1,-1,-1]\n",
    "\n",
    "    #this is a bit-record variable\n",
    "    leftSplit_valueSet = 0 \n",
    "    leftSplit_count = 0\n",
    "    leftSplit_hist= {}\n",
    "    rightSplit_hist={}\n",
    "        \n",
    "    bestLeft=(-1,-1)\n",
    "    bestRight=(-1,-1)  \n",
    "    bestSplitMetric =0 \n",
    "    \n",
    "    \n",
    "    #sort feature values according to median \n",
    "    sorted_value_map=[]\n",
    "    \n",
    "    for i in values_hist.keys():\n",
    "        currentVal=i\n",
    "        currentVal_hist_count= values_hist[i]['count_inFeatureValue']\n",
    "        currentVal_hist= values_hist[i]['hist_inFeatureValue']\n",
    "        sorted_value_map.append((currentVal,MAD_hist(currentVal_hist,currentVal_hist_count)))\n",
    "            \n",
    "    sorted_value_map=sorted(sorted_value_map, key= lambda val: val[1][0] )\n",
    "    values_cnt= len(sorted_value_map)\n",
    "    \n",
    "    #--------- initialize the value set ----------------------\n",
    "    \n",
    "    # check the number of feature values\n",
    "    if values_cnt <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    \n",
    "    current_feature_value = sorted_value_map[0][0] \n",
    "    currentVal_count= values_hist[current_feature_value]['count_inFeatureValue']\n",
    "    currentVal_hist= values_hist[current_feature_value]['hist_inFeatureValue']\n",
    "\n",
    "    leftSplit_count = leftSplit_count+currentVal_count     \n",
    "    leftSplit_hist=local_merge_hist( leftSplit_hist, currentVal_hist)\n",
    "    \n",
    "    # histograms for the right values\n",
    "    rightSplit_hist={}\n",
    "    tmp_value=0\n",
    "    for j in range(0, values_cnt-1 ):\n",
    "        tmp_value = sorted_value_map[values_cnt-1-j][0]\n",
    "        rightSplit_hist= local_merge_hist( rightSplit_hist, values_hist[tmp_value]['hist_inFeatureValue'])      \n",
    "            \n",
    "    left = MAD_hist( leftSplit_hist, leftSplit_count)\n",
    "    right= MAD_hist( rightSplit_hist, (node_data_cnt - leftSplit_count)) \n",
    "        \n",
    "    #debug\n",
    "#     tmpcnt=0\n",
    "#     for i in leftSplit_hist.keys():\n",
    "#         tmpcnt= tmpcnt+ leftSplit_hist[i][2]\n",
    "#     if tmpcnt!= leftSplit_count:\n",
    "#         print '++++ problem in left split histogram', tmpcnt, leftSplit_count\n",
    "#     tmpcnt=0\n",
    "#     for i in rightSplit_hist.keys():\n",
    "#         tmpcnt= tmpcnt+ rightSplit_hist[i][2]\n",
    "#     if tmpcnt!= (node_data_cnt - leftSplit_count):\n",
    "#         print '???? problem in right split histogram', tmpcnt, (node_data_cnt - leftSplit_count)\n",
    "    \n",
    "    \n",
    "    leftMedian= left[0]\n",
    "    leftMetric= left[1]\n",
    "    rightMedian= right[0]\n",
    "    rightMetric= right[1]\n",
    "        \n",
    "        #debug\n",
    "#         if leftMedian<0 or rightMedian<0 or leftMetric <=0 or rightMetric <=0:\n",
    "#             print '$$$$ problem in MAD calculation',leftMedian,rightMedian,leftMetric,rightMetric,feature_id,\\\n",
    "#             current_feature_value\n",
    "            \n",
    "    current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "    bestSplitMetric=current_splitMetric     \n",
    "    bestLeft=left\n",
    "    bestRight=right\n",
    "    #bit variable to record which value is chosen for the left child\n",
    "    leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "    \n",
    "\n",
    "    #---------------------------------------------------------------\n",
    "\n",
    "    # scan the sorted feature values\n",
    "    for k in range(1,values_cnt-1):\n",
    "\n",
    "        current_feature_value=sorted_value_map[k][0]\n",
    "        currentVal_count= values_hist[current_feature_value]['count_inFeatureValue']\n",
    "        currentVal_hist= values_hist[current_feature_value]['hist_inFeatureValue']\n",
    "\n",
    "#         histograms for the left values\n",
    "        leftSplit_count = leftSplit_count+currentVal_count     \n",
    "        leftSplit_hist=local_merge_hist( leftSplit_hist, currentVal_hist)\n",
    "    \n",
    "#         histograms for the right values\n",
    "        rightSplit_hist={}\n",
    "        tmp_value=0\n",
    "        for j in range(0, values_cnt-1-k ):\n",
    "            tmp_value = sorted_value_map[values_cnt-1-j][0]\n",
    "            rightSplit_hist= local_merge_hist( rightSplit_hist, values_hist[tmp_value]['hist_inFeatureValue'])      \n",
    "            \n",
    "        left = MAD_hist( leftSplit_hist, leftSplit_count)\n",
    "        right= MAD_hist( rightSplit_hist, (node_data_cnt - leftSplit_count)) \n",
    "        \n",
    "        #debug\n",
    "#         tmpcnt=0\n",
    "#         for i in leftSplit_hist.keys():\n",
    "#             tmpcnt= tmpcnt+ leftSplit_hist[i][2]\n",
    "#         if tmpcnt!= leftSplit_count:\n",
    "#             print '++++ problem in left split histogram', tmpcnt, leftSplit_count\n",
    "#         tmpcnt=0\n",
    "#         for i in rightSplit_hist.keys():\n",
    "#             tmpcnt= tmpcnt+ rightSplit_hist[i][2]\n",
    "#         if tmpcnt!= (node_data_cnt - leftSplit_count):\n",
    "#             print '???? problem in right split histogram', tmpcnt, (node_data_cnt - leftSplit_count)\n",
    "                 \n",
    "        leftMedian= left[0]\n",
    "        leftMetric= left[1]\n",
    "        rightMedian= right[0]\n",
    "        rightMetric= right[1]\n",
    "        \n",
    "        #debug\n",
    "#         if leftMedian<0 or rightMedian<0 or leftMetric <=0 or rightMetric <=0:\n",
    "#             print '$$$$ problem in MAD calculation',leftMedian,rightMedian,leftMetric,rightMetric,feature_id,\\\n",
    "#             current_feature_value\n",
    "            \n",
    "        current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "        if current_splitMetric < bestSplitMetric:\n",
    "            \n",
    "            bestSplitMetric=current_splitMetric\n",
    "            bestLeft=left\n",
    "            bestRight=right\n",
    "            \n",
    "            #bit variable to record which value is chosen for the left child\n",
    "            leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)        \n",
    "    \n",
    "    return [leftSplit_valueSet, bestLeft[0], bestRight[0], bestSplitMetric ]\n",
    "\n",
    "\n",
    "def find_bestSplit_hist(local_aggre_nodes, current_NumNodes, nodes_tree, \\\n",
    "                        nodes_tree_test):\n",
    "        \n",
    "    #debug \n",
    "    tmpnode_cnt=[]\n",
    "    presplit=[]\n",
    "\n",
    "    \n",
    "    local_node_idx=0\n",
    "    \n",
    "#------------- grow the unbalanced tree------------------------    \n",
    "    cur_node_num= len(nodes_tree)\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "    for i in range(0, current_NumNodes):\n",
    "        \n",
    "        \n",
    "        #-------------------------------------------\n",
    "        \n",
    "        if i!= local_aggre_nodes[local_node_idx][0]:\n",
    "            continue\n",
    "        else:\n",
    "            local_node_idx=local_node_idx+1\n",
    "        \n",
    "        \n",
    "        #------------- grow the unbalanced tree ---------------------------------- \n",
    "        if cur_node_num >= 3:\n",
    "            parent_node = int(math.ceil(  (i + cur_node_num -2.0)*1.0/ 2 ))\n",
    "            parent_node_esti =nodes_tree_test[2* parent_node +1 ] \n",
    "            tmp_split_feature= nodes_tree[parent_node][0]\n",
    "            \n",
    "            if tmp_split_feature == -1:\n",
    "                nodes_tree.append(  (-1,  -1  )  )\n",
    "                nodes_tree_test.append( parent_node_esti  )\n",
    "                nodes_tree_test.append( parent_node_esti )\n",
    "                continue\n",
    "        #-------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         if local_aggre_nodes[ current_idx_nodes ][0] != i:\n",
    "#             nodes_layer.append( (-1,-1) )\n",
    "#             nodes_tree.append( (-1,-1) )\n",
    "#             continue\n",
    " \n",
    "        node_count= local_aggre_nodes[i][1]['count'] \n",
    "        node_hist=  local_aggre_nodes[i][1]['hist']           \n",
    "        \n",
    "#         (median, meanAbsDevi)\n",
    "        tmp_metric= MAD_hist( node_hist, node_count)\n",
    "        best_split_sofar= tmp_metric[1]\n",
    "        best_split=(-1,-1,-1,-1)\n",
    "        \n",
    "        #debug\n",
    "#         tmpnode_cnt.append(node_count)\n",
    "#         presplit.append( best_splitMetric_feature[1]/100000 )\n",
    "         \n",
    "            \n",
    "        best_split_feature=-1\n",
    "        best_split_featureValueSet=-1\n",
    "\n",
    "        for j in range(0,numFeatures):\n",
    "                     \n",
    "            statisticToValues = local_aggre_nodes[i][1][j]\n",
    "\n",
    "            split=split_onOneFeature_hist(statisticToValues,node_count, node_hist)\n",
    "            \n",
    "            #debug\n",
    "#             print split[1],split[2]\n",
    "#             [leftSplit_valueSet, bestLeft[0], https://zh.wikipedia.org/wiki/%E7%B9%94%E7%94%B0%E7%9C%9F%E5%AD%90bestRight[0], bestSplitMetric ]\n",
    "            \n",
    "            if  split[0]!=-1 and  split[3] < best_split_sofar:\n",
    "                best_split=split\n",
    "                best_split_sofar=split[3]\n",
    "                best_split_feature = j\n",
    "                best_split_featureValueSet= split[0]\n",
    "                \n",
    "        #------------- grow the unbalanced tree ---------------------------------- \n",
    "        if best_split_feature == -1:\n",
    "            nodes_tree.append(  (-1,  -1  )  )\n",
    "            nodes_tree_test.append( tmp_metric[0]  )\n",
    "            nodes_tree_test.append( tmp_metric[0] )\n",
    "            #debug\n",
    "#             print 'chosen:', tmp_metric[0],tmp_metric[0]\n",
    "            continue\n",
    "        #-------------------------------------------------------------------------        \n",
    "   \n",
    "        #debug \n",
    "#         if len(nodes_tree) ==217:\n",
    "#             print '--- debug---', best_split_feature,  best_split_featureValueSet,\\\n",
    "#             (local_aggre_nodes[i][1][0].keys()),(local_aggre_nodes[i][1][1].keys()),\\\n",
    "#             (local_aggre_nodes[i][1][2].keys())\n",
    "            \n",
    "#         if len(nodes_tree) == 435:\n",
    "#             print '--- debug---', best_split_feature,  best_split_featureValueSet,\\\n",
    "#             (local_aggre_nodes[i][1][0].keys()),(local_aggre_nodes[i][1][1].keys()),\\\n",
    "#             (local_aggre_nodes[i][1][2].keys())\n",
    "            \n",
    "#         if len(nodes_tree) == 872:\n",
    "#             print '--- debug---', best_split_feature,  best_split_featureValueSet,\\\n",
    "#             (local_aggre_nodes[i][1][0].keys()),(local_aggre_nodes[i][1][1].keys()),\\\n",
    "#             (local_aggre_nodes[i][1][2].keys())\n",
    "\n",
    "        #debug\n",
    "#         if current_NumNodes == 512:\n",
    "#             print '----- debug ----', len(nodes_tree), best_split_feature,best_split_featureValueSet,(local_aggre_nodes[i][1][0].keys()),\\\n",
    "#             (local_aggre_nodes[i][1][1].keys()),(local_aggre_nodes[i][1][2].keys())\n",
    "#             if len(local_aggre_nodes[i][1][0])==1 and len(local_aggre_nodes[i][1][1])==1 and len(local_aggre_nodes[i][1][2])==1:\n",
    "#                 print '-------------',local_aggre_nodes[i][1][0].keys(),local_aggre_nodes[i][1][1].keys(),local_aggre_nodes[i][1][2].keys(),\\\n",
    "#                 'split:', best_split_feature,  best_split_featureValueSet\n",
    "\n",
    "        #debug\n",
    "#         print 'chosen:', best_split[1],best_split[2]\n",
    "\n",
    "        # split on each node\n",
    "        nodes_tree.append(  (best_split_feature,  best_split_featureValueSet  )      )\n",
    "        \n",
    "        # tree for predicting\n",
    "        nodes_tree_test.append(best_split[1])\n",
    "        nodes_tree_test.append(best_split[2])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#statistic_infor_check for distributed information collection\n",
    "\n",
    "def statistic_infor_check(local_aggre_nodes):\n",
    "    \n",
    "    numNode = len(local_aggre_nodes)\n",
    "#     print numNode\n",
    "    tmpsum=0\n",
    "    for i in range(0, numNode):\n",
    "        tmpsum=tmpsum+local_aggre_nodes[i][1]['count']\n",
    "        \n",
    "#     print tmpsum\n",
    "    if tmpsum != 98974:\n",
    "        print 'problem in node count'\n",
    "\n",
    "    tmpsum=0\n",
    "    for i in range(0, numNode):\n",
    "        #check node hist\n",
    "        node_count = local_aggre_nodes[i][1]['count']\n",
    "        tmpsum=0\n",
    "        for j in local_aggre_nodes[i][1]['hist'].keys():\n",
    "            tmpsum= tmpsum+local_aggre_nodes[i][1]['hist'][j][2]\n",
    "        if tmpsum!= node_count:\n",
    "            print '$$$$ problem in the histogram of node'\n",
    "            \n",
    "        #check feature values\n",
    "        for j in range(0, numFeatures):\n",
    "            tmpsum=0\n",
    "            for k in local_aggre_nodes[i][1][j].keys():\n",
    "                value_hist= local_aggre_nodes[i][1][j][k]['hist_inFeatureValue']\n",
    "                value_cnt = local_aggre_nodes[i][1][j][k]['count_inFeatureValue']\n",
    "                tmpsum= tmpsum+value_cnt\n",
    "            \n",
    "#                 print len(value_hist)\n",
    "                if len(value_hist) > bin_num:\n",
    "                    print '???? bin num wrong', len(value_hist)\n",
    "            \n",
    "                #check feature-value hist\n",
    "                tmpsum1=0\n",
    "                for m in value_hist.keys():\n",
    "                    tmpsum1= tmpsum1 + value_hist[m][2]\n",
    "                if tmpsum1 != value_cnt:\n",
    "                    print '++++ problem in feature-value histogram', tmpsum1, value_cnt\n",
    "            \n",
    "            if tmpsum != node_count:\n",
    "                print '---- problem !!!! in feature:', tmpsum, node_count    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of feature-value combinations: 125\n",
      "70105 20046\n"
     ]
    }
   ],
   "source": [
    "# training and testing process: data preparation\n",
    "\n",
    "# with outliers\n",
    "tmpdta = dta.takeSample(False, 100000, 12243)\n",
    "dta_train_all = sc.parallelize(tmpdta).cache().sample(False, .7, 12345)\n",
    "dta_test_all = sc.parallelize(tmpdta).cache().sample(False, .2, 43243)\n",
    "\n",
    "# configurate extraction\n",
    "print 'number of feature-value combinations:',len(dta_train_all.map(lambda line:(line[1],line[2],line[3])).distinct().collect())\n",
    "\n",
    "print dta_train_all.count(), dta_test_all.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70105\n",
      "19843\n",
      "number of feature-value combinations: 125\n"
     ]
    }
   ],
   "source": [
    "# training and testing process: clearned or noisy data ?\n",
    "\n",
    "# 98974\n",
    "# 99987\n",
    "\n",
    "# 10134\n",
    "\n",
    "dta_train = dta_train_all\n",
    "# .filter(lambda line: line[0]<1000 )\n",
    "dta_test = dta_test_all.filter(lambda line: line[0]<1000 )\n",
    "\n",
    "print dta_train.count()\n",
    "print dta_test.count()\n",
    "print 'number of feature-value combinations:',len(dta_train.map(lambda line:(line[1],line[2],line[3])).distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "2 2\n",
      "4 4\n",
      "8 8\n",
      "16 16\n",
      "32 32\n",
      "64 64\n",
      "128 128\n",
      "256 256\n",
      "512 512\n",
      "error at tree height 10 : 0 0\n",
      "running time at tree height 10 : 230.220839977 230.209985971\n",
      "number of leaf nodes at tree height 10 : 1024\n"
     ]
    }
   ],
   "source": [
    "# training and testing process: main process\n",
    "\n",
    "#parameters\n",
    "maxdepth=10\n",
    "numFeatures=3\n",
    "trimm_ratio = 0.00\n",
    "# tmse (trimmed mse), lad, ma d, mse\n",
    "loss_func= 'tmse'\n",
    "bin_num=800\n",
    "\n",
    "# def tree_test_mapFunc_median(line):\n",
    "#     tmpnode = search_nodeToData(line[1:numFeatures+1], node_split)\n",
    "#     return (leaf_nodes[ tmpnode ]  - line[0])*(leaf_nodes[ tmpnode ]  - line[0])\n",
    "\n",
    "# def tree_test_mapFunc(line):\n",
    "#     tmpnode = search_nodeToData(line[1:numFeatures+1], node_split)\n",
    "#     return (leaf_nodes[ tmpnode ]  - line[0])*(leaf_nodes[ tmpnode ]  - line[0])\n",
    "    \n",
    "# def tree_test( testData_rdd ):\n",
    "#     err_rdd=testData_rdd.map( lambda line: tree_test_mapFunc(line))\n",
    "#     err_sum = err_rdd.reduce( lambda a,b: a+b)\n",
    "#     return err_sum / testData_rdd.count()\n",
    "\n",
    "# variables for the tree structure \n",
    "node_split=[]\n",
    "node_test=[]\n",
    "\n",
    "test_err = []\n",
    "train_err = []\n",
    "run_time=[]\n",
    "\n",
    "tree_history_split=[]\n",
    "tree_history_esti=[]\n",
    "tree_history_leaf=[]\n",
    "tree_history_runtime=[]\n",
    "\n",
    "#ini parameter on the cluster\n",
    "currentNode_split_fromMaster = sc.broadcast([])\n",
    "\n",
    "#prepare feature value set\n",
    "# feature_valueSet = data_featureValues_collect( dta_train )\n",
    "# feature_valueList=[]\n",
    "# for i in range(0, len(feature_valueSet)):\n",
    "#     feature_valueList.append( ( i, list(feature_valueSet[i][1]) ) )\n",
    "\n",
    "#     tune the starting depth\n",
    "for i in range(10,maxdepth+1):\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    del node_split[:]\n",
    "    del node_test[:]\n",
    "    currentNode_split_fromMaster = sc.broadcast([])\n",
    "    \n",
    "    for cur_depth in range(0,i):\n",
    "        \n",
    "        current_NumNodes= int( math.pow(2,  cur_depth)) \n",
    "        statis_partitions = dta_train.mapPartitions( partition_combiner_hist )\n",
    "        \n",
    "        aggre_nodes =statis_partitions.reduceByKey(lambda statis_partition_1, statis_partition_2: \n",
    "                              merge_parttion_combiner_hist(statis_partition_1,statis_partition_2 ))\n",
    "        \n",
    "        local_aggre_nodes= aggre_nodes.collect()\n",
    "        \n",
    "        #debug\n",
    "#         statistic_infor_check(local_aggre_nodes)\n",
    "\n",
    "        #debug\n",
    "        print len(local_aggre_nodes) , current_NumNodes\n",
    "        \n",
    "        end_cluster = time.time()\n",
    "        find_bestSplit_hist( sorted(local_aggre_nodes,key= lambda val:val[0])\\\n",
    "                                    ,current_NumNodes, node_split, node_test)\n",
    "        \n",
    "        #debug\n",
    "#         statistic_infor_check(local_aggre_nodes)\n",
    "        \n",
    "        currentNode_split_fromMaster = sc.broadcast(node_split)\n",
    "    \n",
    "    end = time.time()\n",
    "    leaf_nodes = node_test[ len(node_test)-  (int)( math.pow(2,i)): len(node_test)  ]\n",
    "    \n",
    "    tmp_test_err= 0 \n",
    "#     tree_test( dta_test )\n",
    "    tmp_train_err= 0\n",
    "#     tree_test( dta_train )\n",
    "#     test_err.append( tmp_test_err)\n",
    "#     train_err.append( tmp_train_err )\n",
    "\n",
    "    elapsed = end-start\n",
    "    run_time.append(elapsed)\n",
    "\n",
    "    tree_history_split.append(copy.deepcopy(node_split) )\n",
    "    tree_history_esti.append( copy.deepcopy(node_test)  )\n",
    "    tree_history_leaf.append( copy.deepcopy(leaf_nodes)  )\n",
    "    tree_history_runtime.append( copy.deepcopy(run_time)  )\n",
    "        \n",
    "    print \"error at tree height\", i,\":\",  tmp_test_err, tmp_train_err\n",
    "    print \"running time at tree height\", i,\":\",  elapsed,  end_cluster-start\n",
    "    print \"number of leaf nodes at tree height\", i,\":\",  len(leaf_nodes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86.6506817987, 3, 14, 104)\n",
      "0\n",
      "0\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "print dta_test.first()\n",
    "err_rdd=dta_test.map(lambda line:search_nodeToData(line[1:numFeatures+1], node_split) ) \n",
    "print len(node_split)\n",
    "print len(node_test)\n",
    "print len(leaf_nodes)\n",
    "\n",
    "# print node_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-a3dae14451c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m  \u001b[0msplit_cnt\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mnode_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m873\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mnode_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m885\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "split_cnt=len(node_split)\n",
    "\n",
    "tmpsplit = node_split[ split_cnt-512: split_cnt  ]\n",
    "print len(tmpsplit)\n",
    "# print tmpsplit\n",
    "\n",
    "for i in range(0, len(tmpsplit)):\n",
    "    if tmpsplit[i] == (1, 8192):\n",
    "        print i+  split_cnt-512\n",
    "\n",
    "print node_split[873]\n",
    "print node_split[885]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# training and testing process: extract all the feature-value combinations \n",
    "# in the training data set\n",
    "\n",
    "total_featureVal_set=[]\n",
    "for i in range(0, numFeatures):\n",
    "    featureValues = dta_train.map(lambda line: line[1+i]).distinct().collect()\n",
    "    total_featureVal_set.append( featureValues)\n",
    "\n",
    "print 'done'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[230.2208399772644]]\n"
     ]
    }
   ],
   "source": [
    "print tree_history_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1023\n",
      "1\n",
      "2046\n",
      "1\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "#---------------- LAD -------------------  data backup\n",
    "lad_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "lad_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "lad_tree_history_leaf =  copy.deepcopy( tree_history_leaf)\n",
    "lad_tree_history_runtime =  copy.deepcopy( tree_history_runtime)\n",
    "\n",
    "print len(lad_tree_history_split)\n",
    "print len(lad_tree_history_split[0])\n",
    "print len(lad_tree_history_esti)\n",
    "print len(lad_tree_history_esti[0])\n",
    "\n",
    "print len(lad_tree_history_leaf)\n",
    "print len(lad_tree_history_leaf[0])\n",
    "\n",
    "print tree_history_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error: []\n",
      "training error: []\n",
      "1\n",
      "non_leaf_node count at previous depth 0\n",
      "depth 0 : 1\n",
      "0 [[3, 6, 4, 2, 5], [12, 15, 13, 11, 14], [105, 102, 103, 104, 101]] non-leaf node, 0 [2, 5]\n",
      "non_leaf_node count at previous depth 1\n",
      "depth 1 : 2\n",
      "1 [[2, 5], [12, 15, 13, 11, 14], [105, 102, 103, 104, 101]] non-leaf node, 1 [12, 13, 14]\n",
      "2 [[3, 4, 6], [12, 15, 13, 11, 14], [105, 102, 103, 104, 101]] non-leaf node, 1 [11, 15]\n",
      "non_leaf_node count at previous depth 2\n",
      "depth 2 : 4\n",
      "3 [[2, 5], [12, 13, 14], [105, 102, 103, 104, 101]] non-leaf node, 0 [2]\n",
      "4 [[2, 5], [11, 15], [105, 102, 103, 104, 101]] non-leaf node, 2 [102, 103]\n",
      "5 [[3, 4, 6], [11, 15], [105, 102, 103, 104, 101]] non-leaf node, 2 [101, 105]\n",
      "6 [[3, 4, 6], [12, 13, 14], [105, 102, 103, 104, 101]] non-leaf node, 2 [101, 102, 103]\n",
      "non_leaf_node count at previous depth 4\n",
      "depth 3 : 8\n",
      "7 [[2], [12, 13, 14], [105, 102, 103, 104, 101]] non-leaf node, 2 [102, 103, 104]\n",
      "8 [[5], [12, 13, 14], [105, 102, 103, 104, 101]] non-leaf node, 1 [13]\n",
      "9 [[2, 5], [11, 15], [102, 103]] non-leaf node, 0 [5]\n",
      "10 [[2, 5], [11, 15], [104, 105, 101]] non-leaf node, 2 [101]\n",
      "11 [[3, 4, 6], [11, 15], [101, 105]] non-leaf node, 0 [3]\n",
      "12 [[3, 4, 6], [11, 15], [104, 102, 103]] non-leaf node, 0 [4]\n",
      "13 [[3, 4, 6], [12, 13, 14], [101, 102, 103]] non-leaf node, 1 [13, 14]\n",
      "14 [[3, 4, 6], [12, 13, 14], [104, 105]] non-leaf node, 0 [3, 4]\n",
      "non_leaf_node count at previous depth 8\n",
      "depth 4 : 16\n",
      "15 [[2], [12, 13, 14], [102, 103, 104]] non-leaf node, 2 [102]\n",
      "16 [[2], [12, 13, 14], [105, 101]] non-leaf node, 2 [105]\n",
      "17 [[5], [13], [105, 102, 103, 104, 101]] non-leaf node, 2 [101, 103, 105]\n",
      "18 [[5], [12, 14], [105, 102, 103, 104, 101]] non-leaf node, 2 [101, 105]\n",
      "19 [[5], [11, 15], [102, 103]] non-leaf node, 2 [103]\n",
      "20 [[2], [11, 15], [102, 103]] non-leaf node, 2 [103]\n",
      "21 [[2, 5], [11, 15], [101]] non-leaf node, 1 [15]\n",
      "22 [[2, 5], [11, 15], [104, 105]] non-leaf node, 0 [2]\n",
      "23 [[3], [11, 15], [101, 105]] non-leaf node, 2 [105]\n",
      "24 [[4, 6], [11, 15], [101, 105]] non-leaf node, 1 [11]\n",
      "25 [[4], [11, 15], [104, 102, 103]] non-leaf node, 1 [11]\n",
      "26 [[3, 6], [11, 15], [104, 102, 103]] non-leaf node, 1 [15]\n",
      "27 [[3, 4, 6], [13, 14], [101, 102, 103]] non-leaf node, 2 [103]\n",
      "28 [[3, 4, 6], [12], [101, 102, 103]] non-leaf node, 2 [102, 103]\n",
      "29 [[3, 4], [12, 13, 14], [104, 105]] non-leaf node, 1 [12, 13]\n",
      "30 [[6], [12, 13, 14], [104, 105]] non-leaf node, 2 [105]\n",
      "non_leaf_node count at previous depth 16\n",
      "depth 5 : 32\n",
      "31 [[2], [12, 13, 14], [102]] non-leaf node, 1 [13, 14]\n",
      "32 [[2], [12, 13, 14], [104, 103]] non-leaf node, 1 [12]\n",
      "33 [[2], [12, 13, 14], [105]] non-leaf node, 1 [14]\n",
      "34 [[2], [12, 13, 14], [101]] non-leaf node, 1 [12]\n",
      "35 [[5], [13], [101, 103, 105]] non-leaf node, 2 [101, 103]\n",
      "36 [[5], [13], [104, 102]] non-leaf node, 2 [104]\n",
      "37 [[5], [12, 14], [101, 105]] non-leaf node, 2 [101]\n",
      "38 [[5], [12, 14], [104, 102, 103]] non-leaf node, 1 [12]\n",
      "39 [[5], [11, 15], [103]] non-leaf node, 1 [11]\n",
      "40 [[5], [11, 15], [102]] non-leaf node, 1 [11]\n",
      "41 [[2], [11, 15], [103]] non-leaf node, 1 [15]\n",
      "42 [[2], [11, 15], [102]] non-leaf node, 1 [11]\n",
      "43 [[2, 5], [15], [101]] non-leaf node, 0 [2]\n",
      "44 [[2, 5], [11], [101]] non-leaf node, 0 [5]\n",
      "45 [[2], [11, 15], [104, 105]] non-leaf node, 2 [105]\n",
      "46 [[5], [11, 15], [104, 105]] non-leaf node, 2 [104]\n",
      "47 [[3], [11, 15], [105]] non-leaf node, 1 [11]\n",
      "48 [[3], [11, 15], [101]] non-leaf node, 1 [15]\n",
      "49 [[4, 6], [11], [101, 105]] non-leaf node, 0 [4]\n",
      "50 [[4, 6], [15], [101, 105]] non-leaf node, 2 [101]\n",
      "51 [[4], [11], [104, 102, 103]] non-leaf node, 2 [103, 104]\n",
      "52 [[4], [15], [104, 102, 103]] non-leaf node, 2 [104]\n",
      "53 [[3, 6], [15], [104, 102, 103]] non-leaf node, 0 [3]\n",
      "54 [[3, 6], [11], [104, 102, 103]] non-leaf node, 0 [6]\n",
      "55 [[3, 4, 6], [13, 14], [103]] non-leaf node, 1 [13]\n",
      "56 [[3, 4, 6], [13, 14], [101, 102]] non-leaf node, 0 [4, 6]\n",
      "57 [[3, 4, 6], [12], [102, 103]] non-leaf node, 0 [3, 6]\n",
      "58 [[3, 4, 6], [12], [101]] non-leaf node, 0 [4]\n",
      "59 [[3, 4], [12, 13], [104, 105]] non-leaf node, 1 [13]\n",
      "60 [[3, 4], [14], [104, 105]] non-leaf node, 2 [105]\n",
      "61 [[6], [12, 13, 14], [105]] non-leaf node, 1 [12, 14]\n",
      "62 [[6], [12, 13, 14], [104]] non-leaf node, 1 [13]\n",
      "non_leaf_node count at previous depth 32\n",
      "depth 6 : 64\n",
      "63 [[2], [13, 14], [102]] non-leaf node, 1 [13]\n",
      "64 [[2], [12], [102]] has the estimate: 48.0243830286 true: 47.97385033\n",
      "65 [[2], [12], [104, 103]] non-leaf node, 2 [103]\n",
      "66 [[2], [13, 14], [104, 103]] non-leaf node, 2 [104]\n",
      "67 [[2], [14], [105]] has the estimate: 22.1222774906 true: 22.0635963354\n",
      "68 [[2], [12, 13], [105]] non-leaf node, 1 [13]\n",
      "69 [[2], [12], [101]] has the estimate: 42.0241871119 true: 42.0399884896\n",
      "70 [[2], [13, 14], [101]] non-leaf node, 1 [14]\n",
      "71 [[5], [13], [101, 103]] non-leaf node, 2 [103]\n",
      "72 [[5], [13], [105]] has the estimate: 20.9590088733 true: 20.9147547664\n",
      "73 [[5], [13], [104]] has the estimate: 43.0265147967 true: 43.0180506505\n",
      "74 [[5], [13], [102]] has the estimate: 44.0025745763 true: 43.9684382376\n",
      "75 [[5], [12, 14], [101]] non-leaf node, 1 [12]\n",
      "76 [[5], [12, 14], [105]] non-leaf node, 1 [14]\n",
      "77 [[5], [12], [104, 102, 103]] non-leaf node, 2 [103]\n",
      "78 [[5], [14], [104, 102, 103]] non-leaf node, 2 [102]\n",
      "79 [[5], [11], [103]] has the estimate: 28.9973838125 true: 29.009137361\n",
      "80 [[5], [15], [103]] has the estimate: 30.9803020577 true: 30.9667348236\n",
      "81 [[5], [11], [102]] has the estimate: 30.977863003 true: 30.9821602159\n",
      "82 [[5], [15], [102]] has the estimate: 48.0073370817 true: 48.0182755276\n",
      "83 [[2], [15], [103]] has the estimate: 27.020330693 true: 27.0191079577\n",
      "84 [[2], [11], [103]] has the estimate: 61.0503157844 true: 61.0534383862\n",
      "85 [[2], [11], [102]] has the estimate: 57.0455063575 true: 57.0661152889\n",
      "86 [[2], [15], [102]] has the estimate: 61.0044135375 true: 60.9907553622\n",
      "87 [[2], [15], [101]] has the estimate: 39.0170984578 true: 39.0238763246\n",
      "88 [[5], [15], [101]] has the estimate: 50.9010028719 true: 50.9349802525\n",
      "89 [[5], [11], [101]] has the estimate: 53.00940996 true: 53.0021838632\n",
      "90 [[2], [11], [101]] has the estimate: 66.9515187796 true: 66.9326980925\n",
      "91 [[2], [11, 15], [105]] non-leaf node, 1 [11]\n",
      "92 [[2], [11, 15], [104]] non-leaf node, 1 [11]\n",
      "93 [[5], [11, 15], [104]] non-leaf node, 1 [11]\n",
      "94 [[5], [11, 15], [105]] non-leaf node, 1 [15]\n",
      "95 [[3], [11], [105]] has the estimate: 10.9611529908 true: 10.9681185874\n",
      "96 [[3], [15], [105]] has the estimate: 12.0140164965 true: 11.9905767796\n",
      "97 [[3], [15], [101]] has the estimate: 11.0699642684 true: 11.0770097051\n",
      "98 [[3], [11], [101]] has the estimate: 17.0353033167 true: 17.0314985779\n",
      "99 [[4], [11], [101, 105]] non-leaf node, 2 [105]\n",
      "100 [[6], [11], [101, 105]] non-leaf node, 2 [101]\n",
      "101 [[4, 6], [15], [101]] non-leaf node, 0 [4]\n",
      "102 [[4, 6], [15], [105]] non-leaf node, 0 [6]\n",
      "103 [[4], [11], [103, 104]] non-leaf node, 2 [104]\n",
      "104 [[4], [11], [102]] has the estimate: 34.9918931634 true: 35.0067026286\n",
      "105 [[4], [15], [104]] has the estimate: 27.0009361295 true: 26.9742368\n",
      "106 [[4], [15], [102, 103]] non-leaf node, 2 [103]\n",
      "107 [[3], [15], [104, 102, 103]] non-leaf node, 2 [102, 103]\n",
      "108 [[6], [15], [104, 102, 103]] non-leaf node, 2 [102, 103]\n",
      "109 [[6], [11], [104, 102, 103]] non-leaf node, 2 [102, 104]\n",
      "110 [[3], [11], [104, 102, 103]] non-leaf node, 2 [103, 104]\n",
      "111 [[3, 4, 6], [13], [103]] non-leaf node, 0 [3, 6]\n",
      "112 [[3, 4, 6], [14], [103]] non-leaf node, 0 [3, 4]\n",
      "113 [[4, 6], [13, 14], [101, 102]] non-leaf node, 2 [101]\n",
      "114 [[3], [13, 14], [101, 102]] non-leaf node, 1 [13]\n",
      "115 [[3, 6], [12], [102, 103]] non-leaf node, 2 [102]\n",
      "116 [[4], [12], [102, 103]] non-leaf node, 2 [103]\n",
      "117 [[4], [12], [101]] has the estimate: 48.9085210979 true: 48.891021327\n",
      "118 [[3, 6], [12], [101]] non-leaf node, 0 [6]\n",
      "119 [[3, 4], [13], [104, 105]] non-leaf node, 0 [3]\n",
      "120 [[3, 4], [12], [104, 105]] non-leaf node, 0 [4]\n",
      "121 [[3, 4], [14], [105]] non-leaf node, 0 [3]\n",
      "122 [[3, 4], [14], [104]] non-leaf node, 0 [3]\n",
      "123 [[6], [12, 14], [105]] non-leaf node, 1 [14]\n",
      "124 [[6], [13], [105]] has the estimate: 89.9878884965 true: 89.9927102187\n",
      "125 [[6], [13], [104]] has the estimate: 83.0909050763 true: 83.0986066698\n",
      "126 [[6], [12, 14], [104]] non-leaf node, 1 [14]\n",
      "non_leaf_node count at previous depth 37\n",
      "depth 7 : 74\n",
      "127 [[2], [13], [102]] has the estimate: 2.03349019029 true: 2.03126569214\n",
      "128 [[2], [14], [102]] has the estimate: 4.07105246632 true: 4.00013685176\n",
      "131 [[2], [12], [103]] has the estimate: 5.01484731157 true: 5.00729471167\n",
      "132 [[2], [12], [104]] has the estimate: 9.9360079949 true: 9.9604767535\n",
      "133 [[2], [13, 14], [104]] non-leaf node, 1 [14]\n",
      "134 [[2], [13, 14], [103]] non-leaf node, 1 [13]\n",
      "137 [[2], [13], [105]] has the estimate: 46.9884345582 true: 46.977265206\n",
      "138 [[2], [12], [105]] has the estimate: 54.0517520057 true: 54.0563287807\n",
      "141 [[2], [14], [101]] has the estimate: 66.0401422883 true: 65.9699799892\n",
      "142 [[2], [13], [101]] has the estimate: 90.1168144988 true: 90.052020801\n",
      "143 [[5], [13], [103]] has the estimate: 6.01870395255 true: 5.96709283552\n",
      "144 [[5], [13], [101]] has the estimate: 6.02435139205 true: 6.0135196374\n",
      "151 [[5], [12], [101]] has the estimate: 10.9654883487 true: 11.0325184683\n",
      "152 [[5], [14], [101]] has the estimate: 53.003775425 true: 52.9846779449\n",
      "153 [[5], [14], [105]] has the estimate: 40.0128079278 true: 40.0019055839\n",
      "154 [[5], [12], [105]] has the estimate: 47.0570999446 true: 47.0453892555\n",
      "155 [[5], [12], [103]] has the estimate: 45.0597879008 true: 45.0437802696\n",
      "156 [[5], [12], [104, 102]] non-leaf node, 2 [104]\n",
      "157 [[5], [14], [102]] has the estimate: 35.0155134828 true: 35.037338951\n",
      "158 [[5], [14], [104, 103]] non-leaf node, 2 [103]\n",
      "183 [[2], [11], [105]] has the estimate: 39.0402601082 true: 38.9959053713\n",
      "184 [[2], [15], [105]] has the estimate: 56.9806313209 true: 56.9927324121\n",
      "185 [[2], [11], [104]] has the estimate: 74.9109463893 true: 74.9778468058\n",
      "186 [[2], [15], [104]] has the estimate: 91.0169487952 true: 91.0320120883\n",
      "187 [[5], [11], [104]] has the estimate: 74.0386790627 true: 73.9774390483\n",
      "188 [[5], [15], [104]] has the estimate: 84.9669652544 true: 84.9605783129\n",
      "189 [[5], [15], [105]] has the estimate: 90.0865254924 true: 90.0543708052\n",
      "190 [[5], [11], [105]] has the estimate: 92.951380883 true: 92.9517633607\n",
      "199 [[4], [11], [105]] has the estimate: 35.005989219 true: 34.988052266\n",
      "200 [[4], [11], [101]] has the estimate: 56.0740197573 true: 56.0272972285\n",
      "201 [[6], [11], [101]] has the estimate: 27.0185961135 true: 27.0077310597\n",
      "202 [[6], [11], [105]] has the estimate: 61.0328202092 true: 60.9999637082\n",
      "203 [[4], [15], [101]] has the estimate: 20.9675617425 true: 21.0003222708\n",
      "204 [[6], [15], [101]] has the estimate: 86.1039389664 true: 86.0970581827\n",
      "205 [[6], [15], [105]] has the estimate: 9.09531161692 true: 9.0733780632\n",
      "206 [[4], [15], [105]] has the estimate: 93.944527472 true: 93.9758446108\n",
      "207 [[4], [11], [104]] has the estimate: 4.88726704974 true: 4.8897645333\n",
      "208 [[4], [11], [103]] has the estimate: 17.0448288752 true: 17.0250315697\n",
      "213 [[4], [15], [103]] has the estimate: 56.056853155 true: 56.0558785871\n",
      "214 [[4], [15], [102]] has the estimate: 67.0500295258 true: 66.9773836861\n",
      "215 [[3], [15], [102, 103]] non-leaf node, 2 [103]\n",
      "216 [[3], [15], [104]] has the estimate: 72.0694867828 true: 72.0404872446\n",
      "217 [[6], [15], [102, 103]] non-leaf node, 2 [102]\n",
      "218 [[6], [15], [104]] has the estimate: 58.9555233113 true: 58.9602779658\n",
      "219 [[6], [11], [102, 104]] non-leaf node, 2 [102]\n",
      "220 [[6], [11], [103]] has the estimate: 99.0294934804 true: 98.9979093654\n",
      "221 [[3], [11], [103, 104]] non-leaf node, 2 [104]\n",
      "222 [[3], [11], [102]] has the estimate: 88.0766923837 true: 88.062123276\n",
      "223 [[3, 6], [13], [103]] non-leaf node, 0 [3]\n",
      "224 [[4], [13], [103]] has the estimate: 79.0351661898 true: 78.9879607812\n",
      "225 [[3, 4], [14], [103]] non-leaf node, 0 [4]\n",
      "226 [[6], [14], [103]] has the estimate: 83.0319374904 true: 82.9938177073\n",
      "227 [[4, 6], [13, 14], [101]] non-leaf node, 0 [6]\n",
      "228 [[4, 6], [13, 14], [102]] non-leaf node, 0 [6]\n",
      "229 [[3], [13], [101, 102]] non-leaf node, 2 [102]\n",
      "230 [[3], [14], [101, 102]] non-leaf node, 2 [102]\n",
      "231 [[3, 6], [12], [102]] non-leaf node, 0 [6]\n",
      "232 [[3, 6], [12], [103]] non-leaf node, 0 [3]\n",
      "233 [[4], [12], [103]] has the estimate: 70.0076295125 true: 69.982786158\n",
      "234 [[4], [12], [102]] has the estimate: 88.0774601632 true: 87.9892282438\n",
      "237 [[6], [12], [101]] has the estimate: 77.074603315 true: 77.001864444\n",
      "238 [[3], [12], [101]] has the estimate: 81.9699218848 true: 81.9963657744\n",
      "239 [[3], [13], [104, 105]] non-leaf node, 2 [104]\n",
      "240 [[4], [13], [104, 105]] non-leaf node, 2 [104]\n",
      "241 [[4], [12], [104, 105]] non-leaf node, 2 [104]\n",
      "242 [[3], [12], [104, 105]] non-leaf node, 2 [104]\n",
      "243 [[3], [14], [105]] has the estimate: 13.9525150109 true: 13.9474307745\n",
      "244 [[4], [14], [105]] has the estimate: 76.0280878395 true: 76.0033059523\n",
      "245 [[3], [14], [104]] has the estimate: 87.0611000083 true: 87.0748929611\n",
      "246 [[4], [14], [104]] has the estimate: 90.9293405356 true: 90.9509084084\n",
      "247 [[6], [14], [105]] has the estimate: 62.0481695737 true: 62.0300917166\n",
      "248 [[6], [12], [105]] has the estimate: 75.0265215551 true: 75.0157849487\n",
      "253 [[6], [14], [104]] has the estimate: 97.1363363866 true: 97.1028301638\n",
      "254 [[6], [12], [104]] has the estimate: 99.0827444072 true: 99.0156492522\n",
      "non_leaf_node count at previous depth 20\n",
      "depth 8 : 40\n",
      "267 [[2], [14], [104]] has the estimate: 18.9750434771 true: 18.981266724\n",
      "268 [[2], [13], [104]] has the estimate: 19.8992501322 true: 19.9252582293\n",
      "269 [[2], [13], [103]] has the estimate: 27.9522429136 true: 27.9438526305\n",
      "270 [[2], [14], [103]] has the estimate: 34.0318682829 true: 34.036945162\n",
      "313 [[5], [12], [104]] has the estimate: 61.0245795416 true: 61.0129465056\n",
      "314 [[5], [12], [102]] has the estimate: 73.0238512652 true: 72.9827760071\n",
      "317 [[5], [14], [103]] has the estimate: 68.9402850741 true: 68.9213039295\n",
      "318 [[5], [14], [104]] has the estimate: 86.9253252925 true: 86.9917817805\n",
      "431 [[3], [15], [103]] has the estimate: 12.0102627878 true: 12.0156436977\n",
      "432 [[3], [15], [102]] has the estimate: 14.0259283366 true: 13.9965052966\n",
      "435 [[6], [15], [102]] has the estimate: 53.0271988348 true: 52.9919885002\n",
      "436 [[6], [15], [103]] has the estimate: 55.0311309262 true: 55.0115944015\n",
      "439 [[6], [11], [102]] has the estimate: 42.0615099889 true: 42.0254714052\n",
      "440 [[6], [11], [104]] has the estimate: 44.9731434544 true: 44.9838556551\n",
      "443 [[3], [11], [104]] has the estimate: 73.0295192824 true: 73.037663934\n",
      "444 [[3], [11], [103]] has the estimate: 75.0292481009 true: 75.0543129234\n",
      "447 [[3], [13], [103]] has the estimate: 1.97565250398 true: 1.98790737771\n",
      "448 [[6], [13], [103]] has the estimate: 19.0720025005 true: 19.0184742008\n",
      "451 [[4], [14], [103]] has the estimate: 20.0345206101 true: 20.0596404774\n",
      "452 [[3], [14], [103]] has the estimate: 43.0515086774 true: 43.0312289765\n",
      "455 [[6], [13, 14], [101]] non-leaf node, 1 [14]\n",
      "????? debug ???? 455 [[6], [14], [101]] [[6], [13], [101]]\n",
      "456 [[4], [13, 14], [101]] non-leaf node, 1 [13]\n",
      "????? debug ???? 456 [[4], [13], [101]] [[4], [14], [101]]\n",
      "457 [[6], [13, 14], [102]] non-leaf node, 1 [13]\n",
      "????? debug ???? 457 [[6], [13], [102]] [[6], [14], [102]]\n",
      "458 [[4], [13, 14], [102]] non-leaf node, 1 [14]\n",
      "????? debug ???? 458 [[4], [14], [102]] [[4], [13], [102]]\n",
      "459 [[3], [13], [102]] has the estimate: 61.0242654305 true: 60.995174632\n",
      "460 [[3], [13], [101]] has the estimate: 67.0500538639 true: 67.0326828879\n",
      "461 [[3], [14], [102]] has the estimate: 69.0257212141 true: 68.9870126476\n",
      "462 [[3], [14], [101]] has the estimate: 81.9460718465 true: 81.9678740829\n",
      "463 [[6], [12], [102]] has the estimate: 47.1126310891 true: 47.0358665981\n",
      "464 [[3], [12], [102]] has the estimate: 63.0176836404 true: 62.9924700553\n",
      "465 [[3], [12], [103]] has the estimate: 18.0413278008 true: 18.0312890264\n",
      "466 [[6], [12], [103]] has the estimate: 69.0877570755 true: 69.0778273736\n",
      "479 [[3], [13], [104]] has the estimate: 26.961704285 true: 26.9728238386\n",
      "480 [[3], [13], [105]] has the estimate: 41.0300507873 true: 41.0592518029\n",
      "481 [[4], [13], [104]] has the estimate: 51.0399724411 true: 51.0327958301\n",
      "482 [[4], [13], [105]] has the estimate: 80.0162589276 true: 80.0319759476\n",
      "483 [[4], [12], [104]] has the estimate: 51.9910462533 true: 51.9889467174\n",
      "484 [[4], [12], [105]] has the estimate: 63.0504428837 true: 62.9838993015\n",
      "485 [[3], [12], [104]] has the estimate: 63.0052513125 true: 62.9906650155\n",
      "486 [[3], [12], [105]] has the estimate: 85.9945980704 true: 85.996448209\n",
      "non_leaf_node count at previous depth 4\n",
      "depth 9 : 8\n",
      "911 [[6], [14], [101]] has the estimate: 37.0140355963 true: 36.9616526704\n",
      "912 [[6], [13], [101]] has the estimate: 51.093847611 true: 51.0256779032\n",
      "913 [[4], [13], [101]] has the estimate: 1.98590651734 true: 1.94852491808\n",
      "914 [[4], [14], [101]] has the estimate: 55.0416913695 true: 55.0330583579\n",
      "915 [[6], [13], [102]] has the estimate: 4.91694300265 true: 4.9195080455\n",
      "916 [[6], [14], [102]] has the estimate: 66.9980449776 true: 67.0081838515\n",
      "917 [[4], [14], [102]] has the estimate: 28.0569072015 true: 27.9914558627\n",
      "918 [[4], [13], [102]] has the estimate: 81.0129896109 true: 81.0014526099\n",
      "non_leaf_node count at previous depth 0\n",
      "bottom depth 10 : 0\n",
      "inter-node number: 125\n",
      " *******  number of identified configurations: 125\n",
      "125\n",
      "[0, 0, 0, 0, 0, 0, 27, 81, 117, 125, 125]\n",
      "0.00107252408384\n"
     ]
    }
   ],
   "source": [
    "# LAD result statistic: node infor.\n",
    "\n",
    "# lad_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "# lad_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "\n",
    "print 'test error:',test_err\n",
    "print 'training error:',train_err\n",
    "print len(lad_tree_history_split)\n",
    "\n",
    "recog_conf=bfs_tree(lad_tree_history_split[ 0 ], total_featureVal_set, maxdepth-1,  lad_tree_history_esti[0], dta_train )  \n",
    "\n",
    "print len(recog_conf[0])\n",
    "print recog_conf[1]\n",
    "print recog_conf[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error at depth 1 : 747.568102013\n",
      "test error at depth 2 : 707.775658871\n",
      "test error at depth 3 : 687.275984705\n",
      "test error at depth 4 : 475.695188655\n",
      "test error at depth 5 : 447.963404192\n",
      "test error at depth 6 : 356.355321915\n",
      "test error at depth 7 : 102.673845697\n",
      "test error at depth 8 : 70.8759157354\n",
      "test error at depth 9 : 1.00085951888\n",
      "test error at depth 10 : 1.00085951888\n",
      "[747.5681020132861, 707.7756588711635, 687.2759847051583, 475.69518865525345, 447.96340419215335, 356.3553219146761, 102.67384569695886, 70.87591573539906, 1.0008595188837408, 1.0008595188837408]\n"
     ]
    }
   ],
   "source": [
    "# LAD result statistic : test error\n",
    "\n",
    "pre=0\n",
    "tree_esti = lad_tree_history_esti[0]\n",
    "tree_split =  lad_tree_history_split[ 0 ]\n",
    "err_depth=[]\n",
    "\n",
    "for i in range(1,maxdepth+1):\n",
    "            \n",
    "    end_bound= pre+ (int)( math.pow(2,i)) \n",
    "    test_leaf_nodes = tree_esti[ pre: end_bound  ]\n",
    "    pre= end_bound        \n",
    "    test_tree_split= tree_split[ 0:(int)( math.pow(2,i)-1)  ]\n",
    "    \n",
    "#     print len(test_tree_split)\n",
    "#     print len(test_leaf_nodes)\n",
    "\n",
    "    err_depth.append(test_error_tree( i, dta_test  ))\n",
    "    \n",
    "\n",
    "print err_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data inspection for the nodes of the trained tree\n",
    "\n",
    "def feature_value_extract( valueSet):\n",
    "    \n",
    "    tmpval=1\n",
    "    tmp_valSet= valueSet\n",
    "    valSet_list=[]\n",
    "    cnt=0\n",
    "    \n",
    "    while tmpval <= tmp_valSet:\n",
    "        if tmpval & tmp_valSet !=0:\n",
    "            valSet_list.append(cnt)\n",
    "        tmpval=tmpval<<1\n",
    "        cnt=cnt+1\n",
    "        \n",
    "    return valSet_list\n",
    "\n",
    "\n",
    "def training_data_summary( train_rdd ):\n",
    "\n",
    "#     always remove outliers before\n",
    "    train_rdd_clean= train_rdd.filter(lambda line: line[0]<1000)\n",
    "    distinct_featureVals = train_rdd_clean.map(lambda line: (line[1], line[2],line[3]) ).distinct().collect()\n",
    "   \n",
    "    featureConfig_val={}\n",
    "    for i in distinct_featureVals:\n",
    "        \n",
    "        tmprdd=train_rdd_clean.filter(lambda line:(line[1], line[2],line[3]) ==i ).map(lambda line: line[0])\n",
    "        tmpval=tmprdd.reduce(lambda a,b:a+b)\n",
    "        tmpval= tmpval/tmprdd.count()*1.0\n",
    "        \n",
    "        featureConfig_val.update( {i:tmpval} )\n",
    "    \n",
    "#     print featureConfig_val\n",
    "    return featureConfig_val\n",
    "        \n",
    "\n",
    "def config_check( featureVal_set, feature_num):\n",
    "    \n",
    "    \n",
    "    for i in range(0, feature_num):\n",
    "        if len(featureVal_set[i]) >1 :\n",
    "            return -1\n",
    "    return 1\n",
    "\n",
    "def lookup_featureConfig_val( configToVal_dict, value_set, feature_num):\n",
    "    \n",
    "    tmptuple=()\n",
    "    for i in range(0, feature_num):\n",
    "        tmptuple = tmptuple + ( value_set[i][0]  ,)\n",
    "#     print '++++++++++++', tmptuple\n",
    "    return configToVal_dict[tmptuple]\n",
    "    \n",
    "def test_error_mapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], test_tree_split)\n",
    "    return (test_leaf_nodes[ tmpnode ]  - line[0])*(test_leaf_nodes[ tmpnode ]  - line[0])\n",
    "    \n",
    "def test_error_tree( current_depth, testRDD  ):\n",
    "    \n",
    "    test_cnt= testRDD.count()\n",
    "    err_rdd=testRDD.map( lambda line: test_error_mapFunc(line))\n",
    "    err_sum = err_rdd.reduce( lambda a,b: a+b)\n",
    "    \n",
    "    print 'test error at depth',current_depth, ':', err_sum*1.0/test_cnt\n",
    "    return err_sum*1.0/test_cnt\n",
    "    \n",
    "def bfs_tree(tree, total_featureVal_set, depth, tree_esti, trainRDD ):\n",
    "    \n",
    "    nodeNum=len(tree)   \n",
    "    if nodeNum == 0:\n",
    "        return\n",
    "    \n",
    "    feature_ConfigToVal = training_data_summary( trainRDD )\n",
    "    \n",
    "    interNode_cnt=0\n",
    "    non_leaf_cnt=0\n",
    "    recog_config_cnt=0\n",
    "    \n",
    "    qu=[]\n",
    "    current_depth=0\n",
    "    qu.append((0,total_featureVal_set,0))\n",
    "    \n",
    "    \n",
    "    recog_config_cnt_depth=[]\n",
    "    recog_conf=[]\n",
    "    \n",
    "    #debug\n",
    "    test_queue=[]\n",
    "    \n",
    "    while current_depth<= depth:\n",
    "        \n",
    "        print 'non_leaf_node count at previous depth', non_leaf_cnt\n",
    "        \n",
    "        print 'depth', current_depth,':', len(qu)\n",
    "        tmpqu=[]\n",
    "        \n",
    "        non_leaf_cnt=0\n",
    "        \n",
    "        for i in qu:\n",
    "            current_nodeIdx= i[0] \n",
    "            current_featureVal_set= i[1]\n",
    "            \n",
    "            split_feature= tree[current_nodeIdx][0]\n",
    "            split_valueSet= tree[current_nodeIdx][1]\n",
    "                 \n",
    "            if split_feature == -1: \n",
    "                \n",
    "                if config_check( current_featureVal_set, featureNum) ==1:\n",
    "                    recog_config_cnt= recog_config_cnt+1\n",
    "                    \n",
    "                    tmpval=lookup_featureConfig_val( feature_ConfigToVal, current_featureVal_set, featureNum)\n",
    "                    \n",
    "                    print current_nodeIdx,current_featureVal_set, 'has the estimate:',\\\n",
    "                    tree_esti[ current_nodeIdx*2+1],'true:',tmpval\n",
    "                    #result\n",
    "                    recog_conf.append( (tree_esti[ current_nodeIdx*2+1], tmpval) )\n",
    "                    \n",
    "                else:\n",
    "                    print current_nodeIdx,current_featureVal_set, 'has the estimate:',\\\n",
    "                    tree_esti[ current_nodeIdx*2+1],'!!! not seperated !!!'\n",
    "                    \n",
    "                    print '!!! debug !!!', current_nodeIdx,tree[current_nodeIdx], current_featureVal_set\n",
    "                \n",
    "                interNode_cnt=interNode_cnt+1\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                non_leaf_cnt=non_leaf_cnt+1\n",
    "                \n",
    "                featureValue_list = feature_value_extract( split_valueSet)\n",
    "                \n",
    "                print current_nodeIdx, current_featureVal_set,'non-leaf node,', split_feature, featureValue_list\n",
    "                \n",
    "                \n",
    "            #debug\n",
    "            if current_depth == 9:\n",
    "                print '+++ debug ++++',current_nodeIdx,tree[current_nodeIdx], current_featureVal_set, \\\n",
    "                featureValue_list,split_feature\n",
    "    \n",
    "            left_featureValue_set=[]\n",
    "            right_featureValue_set=[]\n",
    "            featureNum= len(current_featureVal_set)\n",
    "    \n",
    "            for i in range(0, featureNum):\n",
    "                if i != split_feature:\n",
    "                    left_featureValue_set.append(  copy.deepcopy(current_featureVal_set[i])   )\n",
    "                    right_featureValue_set.append(  copy.deepcopy(current_featureVal_set[i])   )\n",
    "                else:\n",
    "                    left_featureValue_set.append(  copy.deepcopy(featureValue_list )   )\n",
    "                    right_featureValue_set.append(copy.deepcopy(list(set(current_featureVal_set[i] )-set(featureValue_list)))   )\n",
    "            \n",
    "            \n",
    "            #debug\n",
    "            if current_depth==8:\n",
    "                if left_featureValue_set== [[2], [11, 13], [103]] or \\\n",
    "                right_featureValue_set==right_featureValue_set:\n",
    "                    print '????? debug ????', current_nodeIdx,left_featureValue_set,right_featureValue_set\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            tmpqu.append(  (current_nodeIdx*2+1,left_featureValue_set, current_nodeIdx)  )\n",
    "            tmpqu.append(  (current_nodeIdx*2+2,right_featureValue_set,current_nodeIdx) )\n",
    "        \n",
    "        qu=copy.deepcopy(tmpqu)\n",
    "        current_depth= current_depth+1\n",
    "        \n",
    "        #result\n",
    "        recog_config_cnt_depth.append( recog_config_cnt )\n",
    "    \n",
    "    \n",
    "# print out leaf nodes\n",
    "\n",
    "    print 'non_leaf_node count at previous depth', non_leaf_cnt\n",
    "    print 'bottom depth', current_depth,':', len(qu)\n",
    "    \n",
    "    for i in qu:\n",
    "        current_nodeIdx= i[0] \n",
    "        current_featureVal_set= i[1]\n",
    "        parent_nodeIdx= i[2]\n",
    "        \n",
    "        #debug \n",
    "        print '----debug----', current_featureVal_set\n",
    "        \n",
    "        if 2*parent_nodeIdx+1 == current_nodeIdx: \n",
    "#         parent_nodeIdx == (current_nodeIdx*2+1):\n",
    "            tree_esti_val= tree_esti[ parent_nodeIdx*2]\n",
    "        else:\n",
    "            tree_esti_val= tree_esti[ parent_nodeIdx*2+1] \n",
    "        \n",
    "        if config_check( current_featureVal_set, featureNum) ==1:\n",
    "            recog_config_cnt= recog_config_cnt+1\n",
    "            tmpval=lookup_featureConfig_val( feature_ConfigToVal, current_featureVal_set, featureNum)\n",
    "            \n",
    "            #result\n",
    "            recog_conf.append( (tree_esti_val, tmpval) )\n",
    "            \n",
    "            print current_nodeIdx,current_featureVal_set, 'has the estimate:', tree_esti_val,\\\n",
    "                    'true:',tmpval\n",
    "        else:\n",
    "            print current_nodeIdx,current_featureVal_set, 'has the estimate:', tree_esti_val\n",
    "    \n",
    "    #result\n",
    "    recog_config_cnt_depth.append( recog_config_cnt )\n",
    "        \n",
    "    print 'inter-node number:', interNode_cnt\n",
    "    print ' *******  number of identified configurations:', recog_config_cnt\n",
    "    \n",
    "    tmpval=0\n",
    "    for i in range(0,recog_config_cnt):\n",
    "        tmpval=tmpval+ (recog_conf[i][0]-recog_conf[i][1])*(recog_conf[i][0]-recog_conf[i][1])\n",
    "    \n",
    "    return (recog_conf, recog_config_cnt_depth, tmpval*1.0/recog_config_cnt)\n",
    "\n",
    "# len( tree_history_split)\n",
    "# len(tree_history_esti)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write leaf-nodes results to txt file\n",
    "text_file = open(\"regTree_binNumFixed.txt\", 'a')\n",
    "text_file.write('\\n \\nleaf nodes at depth ')\n",
    "text_file.write(\"%f: \\n\" % maxdepth)\n",
    "for item in leaf_nodes:\n",
    "    text_file.write(\"%f  \" % item)\n",
    "\n",
    "text_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
