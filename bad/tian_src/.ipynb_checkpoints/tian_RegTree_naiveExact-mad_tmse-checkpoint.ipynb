{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os \n",
    "import sys \n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "from datetime  import datetime \n",
    "\n",
    "import numpy as np  # learn \n",
    "import pandas as pd # learn\n",
    "from pandas import *\n",
    "from numpy import *\n",
    "\n",
    "from scipy import stats # look at scipy\n",
    "from scipy import linalg\n",
    "from scipy import *\n",
    "\n",
    " \n",
    "import matplotlib as mplt # learn matplolib \n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "import seaborn as sns \n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc={\"figure.figsize\": (14, 6)})\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import sklearn as sk\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import random\n",
    "import time\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328.0, 10, 10, 2, 2, 8, 1, 3)\n",
      "3315067\n"
     ]
    }
   ],
   "source": [
    "#BT data\n",
    "# dta_RDD = sc.textFile(\"file:///home/tguo/data/tian-test/udpjitter-H1april2014_regTree.csv\")\n",
    "\n",
    "dta_RDD = sc.textFile(\"hdfs://computer61.ant-net/user/tguo/udpjitter-H1april2014_regTree_mllib.csv\")\n",
    "\n",
    "#data format: dependent variable, feature values\n",
    "dta_splited = dta_RDD.map(lambda line: line.split(\",\")).map(lambda r:\n",
    "                                                                (float(r[0]),int( float(r[1])), \n",
    "                                                                 int(float(r[2])),int(float(r[3])),\n",
    "                                                                  int(float(r[4])),int(float(r[5])),\n",
    "                                                                 int(float(r[6])),int(float(r[7])) ))\n",
    "\n",
    "\n",
    "# dta_splited.first()\n",
    "dta= dta_splited\n",
    "dta.cache()\n",
    "print dta.first()\n",
    "print dta.count()\n",
    "\n",
    "# dta_filtered = dta.filter(lambda line: line[0] <2000 )\n",
    "# dta_filtered.cache()\n",
    "# dta_filtered.count()\n",
    "# print dta.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: (46.7446171548, 4, 13, 105)\n",
      "1000000\n"
     ]
    }
   ],
   "source": [
    "# synthetic data\n",
    "dta_RDD = sc.textFile(\"file:///home/tguo/tian_src/synthetic_data.txt\")\n",
    "dta_RDD.cache()\n",
    "\n",
    "dta_splited = dta_RDD.map(lambda line: line.split(\",\")).map(lambda r: ( float(r[3]),int(r[0]),int(r[1]),int(r[2])) )\n",
    "# ( float(r[2]),float(r[0]),float(r[1]) ) \n",
    "\n",
    "dta= dta_splited\n",
    "dta.cache()\n",
    "\n",
    "print 'original:',dta.first()\n",
    "print dta.count()\n",
    "\n",
    "# re-set index of categorical features\n",
    "\n",
    "# feature_dist=[]\n",
    "\n",
    "# tmp1 = dta.map(lambda line: line[1]).distinct().collect()\n",
    "# feature_dist.append(tmp1)\n",
    "\n",
    "# tmp1 = dta.map(lambda line: line[2]).distinct().collect()\n",
    "# feature_dist.append(tmp1)\n",
    "\n",
    "# tmp1 = dta.map(lambda line: line[3]).distinct().collect()\n",
    "# feature_dist.append(tmp1)\n",
    "\n",
    "# fea_cnt= len(feature_dist )\n",
    "# fea_map=[]\n",
    "\n",
    "# for i in range(0, fea_cnt):\n",
    "#     tmpcnt = len(feature_dist[i])\n",
    "#     val_map= dict( zip(feature_dist[i], range(0,tmpcnt)) )\n",
    "#     fea_map.append(val_map)\n",
    "\n",
    "# def reset_index( line ):\n",
    "#     tmp=[]\n",
    "#     tmp.append(line[0])\n",
    "#     for i in range(1,4):\n",
    "#         tmp.append(fea_map[i-1][ line[i] ] )\n",
    "#     return tmp\n",
    "\n",
    "# dta = dta.map( lambda line: reset_index(line)  )\n",
    "\n",
    "# print 'feature value re-indexed:',dta.first()\n",
    "# print dta.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# basic functions in one-pass method [on the cluster side]\n",
    "\n",
    "# assign each data instance to the node\n",
    "def search_nodeToData(features, tree):\n",
    "    nodeNum=len(tree)   \n",
    "    if nodeNum == 0:\n",
    "        return 0;\n",
    "    \n",
    "    current_nodeIdx=0\n",
    "    while current_nodeIdx< nodeNum:\n",
    "        split_feature= tree[current_nodeIdx][0]\n",
    "        split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "        # unbalanced tree grow\n",
    "        if split_feature==-1:\n",
    "            current_nodeIdx=current_nodeIdx*2+1+ random.randint(0, 1)\n",
    "            continue\n",
    "            \n",
    "        if (( 1<<features[split_feature]) &  split_valueSet) != 0 :\n",
    "            current_nodeIdx=current_nodeIdx*2+1\n",
    "        else:\n",
    "            current_nodeIdx=current_nodeIdx*2+2\n",
    "            \n",
    "    return current_nodeIdx - nodeNum\n",
    "\n",
    "def partition_dataToNode(list_dvAndfeatures):\n",
    "    dataToNode_map=[]\n",
    "    res=[]\n",
    "    for dvAndfeatures in list_dvAndfeatures:\n",
    "        \n",
    "        Y= dvAndfeatures[0]\n",
    "        node= search_nodeToData(dvAndfeatures[1:numFeatures+1], \\\n",
    "                                currentNode_split_fromMaster.value)\n",
    "        dataToNode_map.append(node)\n",
    "        res.append( (node, dvAndfeatures )   )\n",
    "    return res\n",
    "def dataToNode_assignment( data_rdd ):\n",
    "    dataToNode_map = data_rdd.mapPartitions( partition_dataToNode )\n",
    "    dataToNode_map.cache()\n",
    "    return dataToNode_map\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "# extract values for each feature\n",
    "def partition_featureValues(list_dvAndfeatures):\n",
    "\n",
    "    feature_valueSet={}\n",
    "    \n",
    "    for i in range(0,numFeatures):\n",
    "        feature_valueSet.update( {i: set()} )\n",
    "        \n",
    "    for dvAndfeatures in list_dvAndfeatures:\n",
    "        for i in range(0,numFeatures):\n",
    "            feature_val= dvAndfeatures[i+1]\n",
    "            feature_valueSet[i].add(feature_val)\n",
    "    return zip( feature_valueSet.keys(), feature_valueSet.values() )  \n",
    "\n",
    "def merge_featureValues(  valueSet1, valueSet2):\n",
    "    return valueSet1.union(valueSet2)\n",
    "    \n",
    "def data_featureValues_collect( data_rdd ):\n",
    "    feature_valueSet_part = data_rdd.mapPartitions( partition_featureValues )\n",
    "    feature_valueSet_local = \\\n",
    "    feature_valueSet_part.reduceByKey(lambda set1, set2: merge_featureValues(set1,set2 )).collect()\n",
    "    feature_valueSet_local.sort()\n",
    "    #test\n",
    "#     print feature_valueSet_local\n",
    "    return feature_valueSet_local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100, 1, 1), (45, 2, 3), (67, 3, 5), (83, 3, 4)]\n"
     ]
    }
   ],
   "source": [
    "#debug\n",
    "\n",
    "# tmpr=[  (100,[2,3]),(83,[3,4]),(67,[3,5]),(45,[2,3]) ]\n",
    "\n",
    "tmpr=[ (100,1,1),(83,3,4),(67,3,5),(45,2,3) ]\n",
    "\n",
    "tmprdd= sc.parallelize(tmpr)\n",
    "\n",
    "sorted_rdd = tmprdd.sortBy(lambda line: line[1], ascending=True)\n",
    "# .collect()\n",
    "\n",
    "print sorted_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split decision [on the local side]\n",
    "\n",
    "#implementation notes:\n",
    "# bit-operation to record the set of selected feature values in the left child\n",
    "\n",
    "#asecending order, upper: larger, lower: smaller\n",
    "def upper_tail_boundary( percentage, dat_rdd, tt_cnt):\n",
    "    tmpcnt= int(tt_cnt*percentage) \n",
    "    tmplist = dat_rdd.top( int(tt_cnt*percentage)  )\n",
    "    return tmplist[tmpcnt-1]\n",
    "\n",
    "def lower_tail_boundary( percentage, dat_rdd, tt_cnt):\n",
    "#     print tt_cnt,percentage, type(dat_rdd)\n",
    "    sorted_rdd = dat_rdd.sortBy(lambda line:line, ascending= True).cache()\n",
    "    tmpcnt= int(tt_cnt*percentage) \n",
    "    tmplist= dat_rdd.take( tmpcnt )\n",
    "    return tmplist[ tmpcnt-1 ]\n",
    "    \n",
    "def trimmed_MSE_cal(dat_rdd, dat_rdd_cnt, trim_percentage ):\n",
    "    \n",
    "    upper_bound = upper_tail_boundary( trim_percentage, dat_rdd, dat_rdd_cnt)\n",
    "#     lower_bound = lower_tail_boundary( trim_percentage, dat_rdd, dat_rdd_cnt)\n",
    "    \n",
    "    trimmed_rdd= dat_rdd.filter( lambda line: line < upper_bound ).cache()\n",
    "#     trimmed_rdd= trimmed_rdd.filter(lambda line: line > lower_bound).cache()\n",
    "    trimmed_rdd_cnt = trimmed_rdd.count()\n",
    "    \n",
    "    if trimmed_rdd_cnt ==0:\n",
    "        \n",
    "        print 'trimmed zeor happens!'\n",
    "        tmp_mean=0\n",
    "        tmp_mse=0\n",
    "    else:\n",
    "        tmp_mean = trimmed_rdd.reduce(lambda a,b:a+b) / trimmed_rdd_cnt\n",
    "        tmp_mse= trimmed_rdd.map(lambda line:(line-tmp_mean)*(line-tmp_mean)).reduce(lambda a,b:a+b)/trimmed_rdd_cnt\n",
    "    \n",
    "    return (trimmed_rdd_cnt, tmp_mean, tmp_mse,   trimmed_rdd)\n",
    "\n",
    "def trimmed_MSE( dataToSplit_rdd, feature_id, value_set ):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.filter(lambda l:l[1][feature_id+1] in value_set).map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "    if tmp_rdd_count ==0:\n",
    "        \n",
    "        print 'trimmed zeor happens!'\n",
    "        tmp_mean=0\n",
    "        tmp_mse=0\n",
    "    else:\n",
    "        tmp_mean = tmp_rdd.reduce(lambda a,b:a+b)*1.0 / tmp_rdd_count\n",
    "        tmp_mse= tmp_rdd.map(lambda line:(line-tmp_mean)*(line-tmp_mean)).reduce(lambda a,b:a+b)*1.0/tmp_rdd_count\n",
    "    \n",
    "    return (tmp_rdd_count, tmp_mean, tmp_mse )\n",
    "    \n",
    "def trimmed_MSE_node( dataToSplit_rdd ):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = tmp_rdd.count()\n",
    "    return trimmed_MSE_cal(tmp_rdd, tmp_rdd_count, trimm_ratio )\n",
    "\n",
    "def split_onOneFeature_exact_trimmedMSE(node_data, node_data_cnt, TMSE_node_ini, feature_valueList, feature_id):\n",
    "    \n",
    "    left_count=0\n",
    "    right_count= node_data_cnt\n",
    "    bestSplitMetric=  TMSE_node_ini \n",
    "    \n",
    "    #------check the number of feature values------------------\n",
    "    if len(feature_valueList) <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    #-----------------------\n",
    "\n",
    "    #this is a bit-record variable\n",
    "    leftSplit_valueSet =0 \n",
    "    leftSplit_count=0 \n",
    "    rightSplit_count=0\n",
    "    \n",
    "    bestLeft=(-1,-1)\n",
    "    bestRight=(-1,-1)\n",
    "    \n",
    "    #sort feature values according to median \n",
    "    sorted_value_map=[]\n",
    "    \n",
    "    for i in feature_valueList:\n",
    "        currentVal = i\n",
    "        #          adapt to different loss function \n",
    "#-----------------------------------        \n",
    "        tmp_rdd=node_data.filter(lambda l:l[1][feature_id+1] == i ).map(lambda l:l[1][0]).cache()  \n",
    "        tmp_rdd_count = tmp_rdd.count()\n",
    "        \n",
    "        if tmp_rdd_count==0:\n",
    "            continue\n",
    "        else:\n",
    "            tmp_mean = tmp_rdd.reduce(lambda a,b:a+b)*1.0 / tmp_rdd_count\n",
    "# ------------------------------------\n",
    "        sorted_value_map.append( (tmp_mean,i) )\n",
    "        \n",
    "        \n",
    "    sorted_value_map.sort()\n",
    "    len_values= len(sorted_value_map)\n",
    "\n",
    "    # scan the split positions    \n",
    "    left_value_set=[]\n",
    "    right_value_set= copy.deepcopy(feature_valueList)\n",
    "    \n",
    "    #--------- initialize the value set ----------------------\n",
    "    \n",
    "    # check the number of feature values\n",
    "    if len_values <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    \n",
    "    current_feature_value = sorted_value_map[0][1] \n",
    "\n",
    "    left_value_set.append(current_feature_value)\n",
    "    right_value_set.remove(current_feature_value)\n",
    "        \n",
    "    #          adapt to different loss function     \n",
    "    left = trimmed_MSE( node_data, feature_id, left_value_set )\n",
    "    right= trimmed_MSE( node_data, feature_id, right_value_set )\n",
    "        \n",
    "    leftSplit_count = left[0]\n",
    "    leftMedian= left[1]\n",
    "    leftMetric= left[2]\n",
    "        \n",
    "    rightSplit_count= right[0]\n",
    "    rightMedian= right[1]\n",
    "    rightMetric= right[2]  \n",
    "        \n",
    "    current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "    bestSplitMetric=current_splitMetric     \n",
    "    bestLeft=left\n",
    "    bestRight=right\n",
    "    #bit variable to record which value is chosen for the left child\n",
    "    leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "    \n",
    "    #--------------------------------------------------------------\n",
    "    \n",
    "    for k in range(1,len_values-1):\n",
    "        \n",
    "        current_feature_value = sorted_value_map[k][1] \n",
    "        \n",
    "        left_value_set.append(current_feature_value)\n",
    "        right_value_set.remove(current_feature_value)\n",
    "        \n",
    "#          adapt to different loss function \n",
    "        left = trimmed_MSE( node_data, feature_id, left_value_set )\n",
    "        right= trimmed_MSE( node_data, feature_id, right_value_set )\n",
    "        \n",
    "        leftSplit_count = left[0]\n",
    "        leftMedian= left[1]\n",
    "        leftMetric= left[2]\n",
    "        \n",
    "        rightSplit_count= right[0]\n",
    "        rightMedian= right[1]\n",
    "        rightMetric= right[2]  \n",
    "        \n",
    "        current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "        if current_splitMetric < bestSplitMetric:\n",
    "            bestSplitMetric=current_splitMetric     \n",
    "            bestLeft=left\n",
    "            bestRight=right\n",
    "            #bit variable to record which value is chosen for the left child\n",
    "            leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "\n",
    "    return [ leftSplit_valueSet, bestLeft[1], bestRight[1], bestSplitMetric  ] \n",
    "#          value_set, left median, right median, weighted mad\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def median_MSE( dataToSplit_rdd, feature_id, value_set ):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.filter(lambda l:l[1][feature_id+1] in value_set).map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "    if tmp_rdd_count==0:\n",
    "        return (10000, 100000, 10000000)\n",
    "    \n",
    "    tmp_mean = tmp_rdd.reduce( lambda a,b: a+b)*1.0/tmp_rdd_count\n",
    "    tmpMSE= tmp_rdd.map( lambda a: (a-tmp_mean)*(a-tmp_mean) ).reduce( lambda a,b: a+b)/tmp_rdd_count\n",
    "    \n",
    "    return (tmp_rdd_count, tmp_mean, tmpMSE)\n",
    "\n",
    "def median_MSE_node( dataToSplit_rdd):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = dataToSplit_rdd.count()\n",
    "\n",
    "    if tmp_rdd_count==0:\n",
    "        return (10000, 100000, 10000000)\n",
    "    \n",
    "    tmp_mean = tmp_rdd.reduce( lambda a,b: a+b)*1.0/tmp_rdd_count\n",
    "    tmpMSE= tmp_rdd.map( lambda a: (a-tmp_mean)*(a-tmp_mean) ).reduce( lambda a,b: a+b)*1.0/tmp_rdd_count\n",
    "    \n",
    "    return (tmp_rdd_count, tmp_mean, tmpMSE)\n",
    "\n",
    "def split_onOneFeature_exact_MSE(node_data, node_data_cnt, MSE_node_ini, feature_valueList, feature_id):\n",
    "    \n",
    "    left_count=0\n",
    "    right_count= node_data_cnt\n",
    "    bestSplitMetric=  MSE_node_ini \n",
    "    \n",
    "    #------check the number of feature values------------------\n",
    "    if len(feature_valueList) <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    #-----------------------\n",
    "\n",
    "    #this is a bit-record variable\n",
    "    leftSplit_valueSet =0 \n",
    "    leftSplit_count=0 \n",
    "    rightSplit_count=0\n",
    "    \n",
    "    bestLeft=(-1,-1)\n",
    "    bestRight=(-1,-1)\n",
    "    \n",
    "    #sort feature values according to mean \n",
    "    sorted_value_map=[]\n",
    "    \n",
    "    for i in feature_valueList:\n",
    "        currentVal = i\n",
    "#-----------------------------------        \n",
    "        tmp_rdd=node_data.filter(lambda l:l[1][feature_id+1] == i ).map(lambda l:l[1][0]).cache()  \n",
    "        tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "        if tmp_rdd_count==0:\n",
    "            continue\n",
    "        else:\n",
    "            tmp_mean = tmp_rdd.reduce( lambda a,b: a+b)*1.0/tmp_rdd_count\n",
    "       \n",
    "        tmp_mse=(tmp_rdd_count, tmp_mean, 0)\n",
    "# ------------------------------------\n",
    "        sorted_value_map.append( (tmp_mse[1],i) )\n",
    "        \n",
    "    sorted_value_map.sort()\n",
    "    len_values= len(sorted_value_map)\n",
    "\n",
    "    # scan the split positions    \n",
    "    left_value_set=[]\n",
    "    right_value_set= copy.deepcopy(feature_valueList)\n",
    "    \n",
    "    #--------- initialize the value set ----------------------\n",
    "    \n",
    "    # check the number of feature values\n",
    "    if len_values <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    \n",
    "    current_feature_value = sorted_value_map[0][1] \n",
    "\n",
    "    left_value_set.append(current_feature_value)\n",
    "    right_value_set.remove(current_feature_value)\n",
    "        \n",
    "    left = median_MSE( node_data, feature_id, left_value_set )\n",
    "    right= median_MSE( node_data, feature_id, right_value_set )\n",
    "        \n",
    "    leftSplit_count = left[0]\n",
    "    leftMedian= left[1]\n",
    "    leftMetric= left[2]\n",
    "        \n",
    "    rightSplit_count= right[0]\n",
    "    rightMedian= right[1]\n",
    "    rightMetric= right[2]  \n",
    "        \n",
    "    #debug\n",
    "    if leftSplit_count+rightSplit_count!=node_data_cnt:\n",
    "        print 'problem in left and right'\n",
    "        \n",
    "    current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "    bestSplitMetric=current_splitMetric     \n",
    "    bestLeft=left\n",
    "    bestRight=right\n",
    "    #bit variable to record which value is chosen for the left child\n",
    "    leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "    \n",
    "    #--------------------------------------------------------------\n",
    "    \n",
    "    for k in range(1,len_values-1):\n",
    "        \n",
    "        current_feature_value = sorted_value_map[k][1] \n",
    "        \n",
    "        left_value_set.append(current_feature_value)\n",
    "        right_value_set.remove(current_feature_value)\n",
    "        \n",
    "        left = median_MSE( node_data, feature_id, left_value_set )\n",
    "        right= median_MSE( node_data, feature_id, right_value_set )\n",
    "        \n",
    "        leftSplit_count = left[0]\n",
    "        leftMedian= left[1]\n",
    "        leftMetric= left[2]\n",
    "        \n",
    "        rightSplit_count= right[0]\n",
    "        rightMedian= right[1]\n",
    "        rightMetric= right[2]  \n",
    "        \n",
    "        #debug\n",
    "        if leftSplit_count+rightSplit_count!=node_data_cnt:\n",
    "            print 'problem in left and right'\n",
    "        \n",
    "        \n",
    "        current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "        if current_splitMetric < bestSplitMetric:\n",
    "            bestSplitMetric=current_splitMetric     \n",
    "            bestLeft=left\n",
    "            bestRight=right\n",
    "            #bit variable to record which value is chosen for the left child\n",
    "            leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "\n",
    "    return [ leftSplit_valueSet, bestLeft[1], bestRight[1], bestSplitMetric  ] \n",
    "#          value_set, left median, right median, weighted mad\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#calculate median and MAD for a set of data in dataToSplit_rdd\n",
    "\n",
    "def median_MAD_cal( dataToSplit_rdd ):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd\n",
    "#     .filter(lambda l:l[1][feature_id+1] in value_set).map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "    if tmp_rdd_count==0:\n",
    "        return (10000, 100000, 10000000)\n",
    "    \n",
    "    if(tmp_rdd_count%2 ==0):\n",
    "        tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "#         tmphalf.sort()\n",
    "        tmpcnt= len(tmphalf)\n",
    "        tmp_median = (tmphalf[tmpcnt-1] + tmphalf[tmpcnt-2])*1.0/2.0\n",
    "    else:\n",
    "        tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "#         tmphalf.sort()\n",
    "        tmpcnt= len(tmphalf)\n",
    "        tmp_median = tmphalf[tmpcnt-1]\n",
    "    \n",
    "    tmpMAD_rdd = tmp_rdd.map(lambda line: abs(line-tmp_median) ).cache()\n",
    "    \n",
    "    if(tmp_rdd_count%2 ==0):\n",
    "        tmphalf = tmpMAD_rdd.top( tmp_rdd_count/2+1  )\n",
    "        tmpcnt= len(tmphalf)\n",
    "        tmp_mad = (tmphalf[tmpcnt-1] + tmphalf[tmpcnt-2])*1.0/2.0\n",
    "    else:\n",
    "        tmphalf = tmpMAD_rdd.top( tmp_rdd_count/2+1  )\n",
    "        tmpcnt= len(tmphalf)\n",
    "        tmp_mad = tmphalf[tmpcnt-1]\n",
    "    \n",
    "    return (tmp_rdd_count, tmp_median, tmp_mad)\n",
    "\n",
    "\n",
    "def median_MAD( dataToSplit_rdd, feature_id, value_set ):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.filter(lambda l:l[1][feature_id+1] in value_set).map(lambda l:l[1][0]).cache()\n",
    "#     tmp_rdd_count = tmp_rdd.count()\n",
    "    return  median_MAD_cal( tmp_rdd )\n",
    "\n",
    "def median_MAD_node( dataToSplit_rdd):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.map(lambda l:l[1][0]).cache()\n",
    "#     tmp_rdd_count = tmp_rdd.count()\n",
    "    return  median_MAD_cal( tmp_rdd )\n",
    "\n",
    "def split_onOneFeature_exact_MAD(node_data, node_data_cnt, node_ini, feature_valueList, feature_id):\n",
    "    \n",
    "    left_count=0\n",
    "    right_count= node_data_cnt\n",
    "    bestSplitMetric=  node_ini \n",
    "    \n",
    "    #------check the number of feature values------------------\n",
    "    if len(feature_valueList) <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    #-----------------------\n",
    "\n",
    "    #this is a bit-record variable\n",
    "    leftSplit_valueSet =0 \n",
    "    leftSplit_count=0 \n",
    "    rightSplit_count=0\n",
    "    \n",
    "    bestLeft=(-1,-1)\n",
    "    bestRight=(-1,-1)\n",
    "    \n",
    "    #sort feature values according to median \n",
    "    sorted_value_map=[]\n",
    "    \n",
    "    for i in feature_valueList:\n",
    "        currentVal = i\n",
    "#         dataToFeatureValue_rdd=node_data.filter(lambda l:l[1][feature_id+1]==currentVal).map(lambda l:l[1][0]).cache()\n",
    "#         dataToFeatureValue_rdd_count = dataToFeatureValue_rdd.count()\n",
    "\n",
    "#-----------------------------------        \n",
    "        tmp_rdd=node_data.filter(lambda l:l[1][feature_id+1] == i ).map(lambda l:l[1][0]).cache()  \n",
    "        tmp_rdd_count = tmp_rdd.count()   \n",
    "        \n",
    "        if tmp_rdd_count == 0:\n",
    "            continue\n",
    "        \n",
    "        tmp_mad=  median_MAD_cal( tmp_rdd )\n",
    "# ------------------------------------\n",
    "        sorted_value_map.append( (tmp_mad[1],i) )\n",
    "        \n",
    "    sorted_value_map.sort()\n",
    "    len_values= len(sorted_value_map)\n",
    "\n",
    "    # scan the split positions    \n",
    "    left_value_set=[]\n",
    "    right_value_set= copy.deepcopy(feature_valueList)\n",
    "    \n",
    "    #--------- initialize the value set ----------------------\n",
    "    \n",
    "    # check the number of feature values\n",
    "    if len_values <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    \n",
    "    current_feature_value = sorted_value_map[0][1] \n",
    "\n",
    "    left_value_set.append(current_feature_value)\n",
    "    right_value_set.remove(current_feature_value)\n",
    "        \n",
    "    left = median_MAD( node_data, feature_id, left_value_set )\n",
    "    right= median_MAD( node_data, feature_id, right_value_set )\n",
    "        \n",
    "    leftSplit_count = left[0]\n",
    "    leftMedian= left[1]\n",
    "    leftMetric= left[2]\n",
    "        \n",
    "    rightSplit_count= right[0]\n",
    "    rightMedian= right[1]\n",
    "    rightMetric= right[2]\n",
    "    \n",
    "    \n",
    "    #debug\n",
    "    if node_data_cnt ==0:\n",
    "        print feature_id,node_ini, leftSplit_count,leftMetric,rightMetric,\n",
    "#         0 10000000 10000 10000000 10000000\n",
    "    \n",
    "        \n",
    "    current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "    bestSplitMetric=current_splitMetric     \n",
    "    bestLeft=left\n",
    "    bestRight=right\n",
    "    #bit variable to record which value is chosen for the left child\n",
    "    leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "    \n",
    "    #--------------------------------------------------------------\n",
    "    \n",
    "    for k in range(1,len_values-1):\n",
    "        \n",
    "        current_feature_value = sorted_value_map[k][1] \n",
    "        \n",
    "        left_value_set.append(current_feature_value)\n",
    "        right_value_set.remove(current_feature_value)\n",
    "        \n",
    "        left = median_MAD( node_data, feature_id, left_value_set )\n",
    "        right= median_MAD( node_data, feature_id, right_value_set )\n",
    "        \n",
    "        leftSplit_count = left[0]\n",
    "        leftMedian= left[1]\n",
    "        leftMetric= left[2]\n",
    "        \n",
    "        rightSplit_count= right[0]\n",
    "        rightMedian= right[1]\n",
    "        rightMetric= right[2]  \n",
    "        \n",
    "        current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "        if current_splitMetric < bestSplitMetric:\n",
    "            bestSplitMetric=current_splitMetric     \n",
    "            bestLeft=left\n",
    "            bestRight=right\n",
    "            #bit variable to record which value is chosen for the left child\n",
    "            leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "\n",
    "    return [ leftSplit_valueSet, bestLeft[1], bestRight[1], bestSplitMetric  ] \n",
    "#          value_set, left median, right median, weighted mad\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#calculate median and MAD for a set of data in dataToSplit_rdd\n",
    "def median_LAD( dataToSplit_rdd, feature_id, value_set ):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.filter(lambda l:l[1][feature_id+1] in value_set).map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "    if tmp_rdd_count==0:\n",
    "        return (10000, 100000, 10000000)\n",
    "    \n",
    "    if(tmp_rdd_count%2 ==0):\n",
    "        tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "        tmphalf.sort()\n",
    "        tmpcnt= len(tmphalf)\n",
    "        tmp_median = (tmphalf[0] + tmphalf[1])*1.0/2.0\n",
    "    else:\n",
    "        tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "        tmphalf.sort()\n",
    "        tmpcnt= len(tmphalf)\n",
    "        tmp_median = tmphalf[0]\n",
    "    \n",
    "    tmpMAD_rdd = tmp_rdd.map(lambda line: abs(line-tmp_median) ).cache()\n",
    "    tmpMAD = tmpMAD_rdd.reduce( lambda a,b: a+b)*1.0 / tmp_rdd_count\n",
    "    \n",
    "    return (tmp_rdd_count, tmp_median, tmpMAD)\n",
    "\n",
    "def median_LAD_node( dataToSplit_rdd):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = dataToSplit_rdd.count()\n",
    "\n",
    "    if tmp_rdd_count==0:\n",
    "        return (10000, 100000, 10000000)\n",
    "    \n",
    "    if(tmp_rdd_count%2 ==0):\n",
    "        tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "        tmphalf.sort()\n",
    "        tmpcnt= len(tmphalf)\n",
    "        tmp_median = (tmphalf[0] + tmphalf[1])*1.0/2.0\n",
    "    else:\n",
    "        tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "        tmphalf.sort()\n",
    "        tmpcnt= len(tmphalf)\n",
    "        tmp_median = tmphalf[0]\n",
    "    \n",
    "    tmpMAD_rdd = tmp_rdd.map(lambda line: abs(line-tmp_median) ).cache()\n",
    "    tmpMAD = tmpMAD_rdd.reduce( lambda a,b: a+b)*1.0 / tmp_rdd_count\n",
    "    \n",
    "    return (tmp_rdd_count, tmp_median, tmpMAD)\n",
    "\n",
    "def split_onOneFeature_exact_LAD(node_data, node_data_cnt, LAD_node_ini, feature_valueList, feature_id):\n",
    "    \n",
    "    left_count=0\n",
    "    right_count= node_data_cnt\n",
    "    bestSplitMetric=  LAD_node_ini \n",
    "    \n",
    "    #------check the number of feature values------------------\n",
    "    if len(feature_valueList) <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    #-----------------------\n",
    "\n",
    "    #this is a bit-record variable\n",
    "    leftSplit_valueSet =0 \n",
    "    leftSplit_count=0 \n",
    "    rightSplit_count=0\n",
    "    \n",
    "    bestLeft=(-1,-1)\n",
    "    bestRight=(-1,-1)\n",
    "    \n",
    "    #sort feature values according to median \n",
    "    sorted_value_map=[]\n",
    "    \n",
    "    for i in feature_valueList:\n",
    "        currentVal = i\n",
    "#         dataToFeatureValue_rdd=node_data.filter(lambda l:l[1][feature_id+1]==currentVal).map(lambda l:l[1][0]).cache()\n",
    "#         dataToFeatureValue_rdd_count = dataToFeatureValue_rdd.count()\n",
    "\n",
    "#-----------------------------------        \n",
    "        tmp_rdd=node_data.filter(lambda l:l[1][feature_id+1] == i ).map(lambda l:l[1][0]).cache()  \n",
    "        tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "        if tmp_rdd_count==0:\n",
    "            continue\n",
    "        else:\n",
    "            if(tmp_rdd_count%2 ==0):\n",
    "                tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "                tmphalf.sort()\n",
    "                tmpcnt= len(tmphalf)\n",
    "                tmp_median = (tmphalf[0] + tmphalf[1])*1.0/2.0\n",
    "            else:\n",
    "                tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "                tmphalf.sort()\n",
    "                tmpcnt= len(tmphalf)\n",
    "                tmp_median = tmphalf[0]\n",
    "    \n",
    "            tmpMAD_rdd = tmp_rdd.map(lambda line: abs(line-tmp_median) ).cache()\n",
    "            tmpMAD = tmpMAD_rdd.reduce( lambda a,b: a+b)\n",
    "       \n",
    "        tmp_mad=(tmp_rdd_count, tmp_median, tmpMAD)\n",
    "# ------------------------------------\n",
    "        sorted_value_map.append( (tmp_mad[1],i) )\n",
    "        \n",
    "    sorted_value_map.sort()\n",
    "    len_values= len(sorted_value_map)\n",
    "\n",
    "    # scan the split positions    \n",
    "    left_value_set=[]\n",
    "    right_value_set= copy.deepcopy(feature_valueList)\n",
    "    \n",
    "    #--------- initialize the value set ----------------------\n",
    "    \n",
    "    # check the number of feature values\n",
    "    if len_values <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    \n",
    "    current_feature_value = sorted_value_map[0][1] \n",
    "\n",
    "    left_value_set.append(current_feature_value)\n",
    "    right_value_set.remove(current_feature_value)\n",
    "        \n",
    "    left = median_LAD( node_data, feature_id, left_value_set )\n",
    "    right= median_LAD( node_data, feature_id, right_value_set )\n",
    "        \n",
    "    leftSplit_count = left[0]\n",
    "    leftMedian= left[1]\n",
    "    leftMetric= left[2]\n",
    "        \n",
    "    rightSplit_count= right[0]\n",
    "    rightMedian= right[1]\n",
    "    rightMetric= right[2]  \n",
    "        \n",
    "    current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "    bestSplitMetric=current_splitMetric     \n",
    "    bestLeft=left\n",
    "    bestRight=right\n",
    "    #bit variable to record which value is chosen for the left child\n",
    "    leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "    \n",
    "    #--------------------------------------------------------------\n",
    "    \n",
    "    for k in range(1,len_values-1):\n",
    "        \n",
    "        current_feature_value = sorted_value_map[k][1] \n",
    "        \n",
    "        left_value_set.append(current_feature_value)\n",
    "        right_value_set.remove(current_feature_value)\n",
    "        \n",
    "        left = median_LAD( node_data, feature_id, left_value_set )\n",
    "        right= median_LAD( node_data, feature_id, right_value_set )\n",
    "        \n",
    "        leftSplit_count = left[0]\n",
    "        leftMedian= left[1]\n",
    "        leftMetric= left[2]\n",
    "        \n",
    "        rightSplit_count= right[0]\n",
    "        rightMedian= right[1]\n",
    "        rightMetric= right[2]  \n",
    "        \n",
    "        current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "        if current_splitMetric < bestSplitMetric:\n",
    "            bestSplitMetric=current_splitMetric     \n",
    "            bestLeft=left\n",
    "            bestRight=right\n",
    "            #bit variable to record which value is chosen for the left child\n",
    "            leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "\n",
    "    return [ leftSplit_valueSet, bestLeft[1], bestRight[1], bestSplitMetric  ] \n",
    "#          value_set, left median, right median, weighted mad\n",
    "\n",
    "def find_bestSplit_exact( dataToNode, current_NumNodes, nodes_tree, nodes_tree_test, feature_valueList, loss_func):\n",
    "    \n",
    "    tmpnode_cnt=[]\n",
    "    presplit=[]\n",
    "        \n",
    "#------------- grow the unbalanced tree------------------------    \n",
    "    cur_node_num= len(nodes_tree)\n",
    "# -------------------------------------------------------------\n",
    "        \n",
    "    for i in range(0, current_NumNodes):\n",
    "    \n",
    "    #------------- grow the unbalanced tree ---------------------------------- \n",
    "        if cur_node_num >= 3:\n",
    "            parent_node = int(math.ceil(  (i + cur_node_num -2.0)*1.0/ 2 ))\n",
    "            parent_node_esti =nodes_tree_test[2* parent_node +1 ] \n",
    "            tmp_split_feature= nodes_tree[parent_node][0]\n",
    "            \n",
    "            if tmp_split_feature == -1:\n",
    "                nodes_tree.append(  (-1,  -1  )  )\n",
    "                nodes_tree_test.append( parent_node_esti  )\n",
    "                nodes_tree_test.append( parent_node_esti )\n",
    "                continue\n",
    "    #-------------------------------------------------------------------------\n",
    "        # prepare data for this node\n",
    "        current_node=i\n",
    "        current_node_data = dataToNode.filter( lambda line: line[0] == current_node ).cache()\n",
    "        current_node_data_cnt = current_node_data.count()\n",
    "        \n",
    "        # split initialization  \n",
    "        if loss_func == 'tmse':\n",
    "                tmp_metric=trimmed_MSE_node( current_node_data)\n",
    "                best_splitMetric_sofar= tmp_metric[2]\n",
    "        elif loss_func == 'lad':\n",
    "                tmp_metric = median_LAD_node( current_node_data)\n",
    "                best_splitMetric_sofar= tmp_metric[2]\n",
    "        elif loss_func == 'mse':\n",
    "                tmp_metric = median_MSE_node( current_node_data)\n",
    "                best_splitMetric_sofar= tmp_metric[2]\n",
    "        elif loss_func == 'mad':\n",
    "                tmp_metric = median_MAD_node( current_node_data)\n",
    "                best_splitMetric_sofar= tmp_metric[2]\n",
    "        \n",
    "        \n",
    "        best_split=(-1,-1,-1,-1)\n",
    "        best_split_feature=-1\n",
    "        best_split_featureValueSet=-1\n",
    "                \n",
    "        #debug\n",
    "        tmpnode_cnt.append( current_node_data.count()  )\n",
    "        presplit.append( best_splitMetric_sofar/100000 )\n",
    "        \n",
    "        for j in range(0,numFeatures):    \n",
    "                                    \n",
    "            if loss_func == 'tmse':\n",
    "                \n",
    "                current_node_data=tmp_metric[3]\n",
    "                current_node_data_cnt=tmp_metric[0]\n",
    "                cur_split=split_onOneFeature_exact_trimmedMSE(current_node_data,current_node_data_cnt,\\\n",
    "                                           best_splitMetric_sofar,feature_valueList[j][1],j)\n",
    "            elif loss_func == 'lad':\n",
    "                 cur_split=split_onOneFeature_exact_LAD(current_node_data,current_node_data_cnt,\\\n",
    "                                           best_splitMetric_sofar,feature_valueList[j][1],j)\n",
    "            elif loss_func == 'mad':\n",
    "                cur_split=split_onOneFeature_exact_MAD(current_node_data,current_node_data_cnt,\\\n",
    "                                           best_splitMetric_sofar,feature_valueList[j][1],j)\n",
    "                \n",
    "            elif loss_func == 'mse':\n",
    "                 cur_split=split_onOneFeature_exact_MSE(current_node_data,current_node_data_cnt,\\\n",
    "                                           best_splitMetric_sofar,feature_valueList[j][1],j)\n",
    "                \n",
    "                #debug\n",
    "            if  cur_split[0]!=-1  and cur_split[3] <= best_splitMetric_sofar:\n",
    "                best_split=cur_split\n",
    "                best_splitMetric_sofar=cur_split[3]\n",
    "                best_split_feature = j\n",
    "                best_split_featureValueSet= cur_split[0]                  \n",
    "                \n",
    "#------------- grow the unbalanced tree ---------------------------------- \n",
    "        if best_split_feature == -1:\n",
    "            nodes_tree.append(  (-1,  -1  )  )\n",
    "            nodes_tree_test.append( tmp_metric[1]  )\n",
    "            nodes_tree_test.append( tmp_metric[1] )\n",
    "            continue\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "        # tree for spliting nodes        \n",
    "        nodes_tree.append(  (best_split_feature,  best_split_featureValueSet  )  )\n",
    "        \n",
    "        # tree for predicting   \n",
    "        nodes_tree_test.append(best_split[1])\n",
    "        nodes_tree_test.append(best_split[2])\n",
    "        \n",
    "#-----debug---------\n",
    "#     print tmpnode_cnt\n",
    "#     print presplit\n",
    "    \n",
    "#     return nodes_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of feature-value combinations: 125\n"
     ]
    }
   ],
   "source": [
    "# training and testing process: data preparation\n",
    "\n",
    "# with outliers\n",
    "tmpdta = dta.takeSample(False, 1000000, 12243)\n",
    "dta_train_all = sc.parallelize(tmpdta).cache().sample(False, .1, 12345)\n",
    "dta_test_all = sc.parallelize(tmpdta).cache().sample(False, .1, 43243)\n",
    "\n",
    "# configurate extraction\n",
    "print 'number of feature-value combinations:',len(dta_train_all.map(lambda line:(line[1],line[2],line[3])).distinct().collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99987\n",
      "99176\n",
      "number of feature-value combinations: 125\n"
     ]
    }
   ],
   "source": [
    "# training and testing process: clearned or noisy data ?\n",
    "# 98974\n",
    "# 99987\n",
    "\n",
    "dta_train = dta_train_all\n",
    "# .filter(lambda line: line[0]<1000 )\n",
    "dta_test = dta_test_all.filter(lambda line: line[0]<1000 )\n",
    "\n",
    "print dta_train.count()\n",
    "print dta_test.count()\n",
    "print 'number of feature-value combinations:',len(dta_train.map(lambda line:(line[1],line[2],line[3])).distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# training and testing process: extract all the feature-value combinations \n",
    "# in the training data set\n",
    "\n",
    "total_featureVal_set=[]\n",
    "for i in range(0, numFeatures):\n",
    "    featureValues = dta_train.map(lambda line: line[1+i]).distinct().collect()\n",
    "    total_featureVal_set.append( featureValues)\n",
    "\n",
    "print 'done'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "2 2\n",
      "4 4\n",
      "8 8\n",
      "16 16\n",
      "32 32\n",
      "64 64\n",
      "128 128\n",
      "256 256\n",
      "512 512\n",
      "error at tree height 10 : 0 0\n",
      "running time at tree height 10 : 3270.06961489\n",
      "number of leaf nodes at tree height 10 : 1024\n"
     ]
    }
   ],
   "source": [
    "# training and testing process: main process\n",
    "\n",
    "#parameters\n",
    "maxdepth=10\n",
    "numFeatures=3\n",
    "trimm_ratio = 0.009\n",
    "# tmse (trimmed mse), lad, mad, mse\n",
    "loss_func= 'mad'\n",
    "\n",
    "def tree_test_mapFunc_median(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], node_split)\n",
    "    return (leaf_nodes[ tmpnode ]  - line[0])*(leaf_nodes[ tmpnode ]  - line[0])\n",
    "\n",
    "def tree_test_mapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], node_split)\n",
    "    return (leaf_nodes[ tmpnode ]  - line[0])*(leaf_nodes[ tmpnode ]  - line[0])\n",
    "    \n",
    "def tree_test( testData_rdd ):\n",
    "    err_rdd=testData_rdd.map( lambda line: tree_test_mapFunc(line))\n",
    "    err_sum = err_rdd.reduce( lambda a,b: a+b)\n",
    "    return err_sum / testData_rdd.count()\n",
    "\n",
    "# variables for the tree structure \n",
    "node_split=[]\n",
    "node_test=[]\n",
    "\n",
    "test_err = []\n",
    "train_err = []\n",
    "run_time=[]\n",
    "\n",
    "tree_history_split=[]\n",
    "tree_history_esti=[]\n",
    "tree_history_leaf=[]\n",
    "tree_history_runtime=[]\n",
    "\n",
    "   \n",
    "\n",
    "#ini parameter on the cluster\n",
    "currentNode_split_fromMaster = sc.broadcast([])\n",
    "\n",
    "\n",
    "#prepare feature value set\n",
    "feature_valueSet = data_featureValues_collect( dta_train )\n",
    "feature_valueList=[]\n",
    "for i in range(0, len(feature_valueSet)):\n",
    "    feature_valueList.append( ( i, list(feature_valueSet[i][1]) ) )\n",
    "    \n",
    "#     tune the starting depth\n",
    "for i in range(10,maxdepth+1):\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    del node_split[:]\n",
    "    del node_test[:]\n",
    "    currentNode_split_fromMaster = sc.broadcast([])\n",
    "    \n",
    "    for cur_depth in range(0,i):\n",
    "        \n",
    "        current_nodeNum= int( math.pow(2,  cur_depth)) \n",
    "        dataToNode = dataToNode_assignment( dta_train )\n",
    "        \n",
    "        #debug\n",
    "        print  dataToNode.map(lambda line:line[0]).distinct().count() , current_nodeNum\n",
    "        \n",
    "        find_bestSplit_exact(dataToNode,current_nodeNum, node_split, node_test, feature_valueList,loss_func)   \n",
    "#         ( dataToNode, current_NumNodes, nodes_tree, nodes_tree_test, feature_valueSet):\n",
    "        #for whole tree\n",
    "        currentNode_split_fromMaster = sc.broadcast(node_split)\n",
    "    \n",
    "    end = time.time()\n",
    "    leaf_nodes = node_test[ len(node_test)-  (int)( math.pow(2,i)): len(node_test)  ]\n",
    "    \n",
    "    tmp_test_err= 0\n",
    "#     tree_test( dta_test )\n",
    "    tmp_train_err= 0\n",
    "#     tree_test( dta_train )\n",
    "#     test_err.append( tmp_test_err)\n",
    "#     train_err.append( tmp_train_err )\n",
    "\n",
    "    elapsed = end-start\n",
    "    run_time.append(elapsed)\n",
    "\n",
    "    tree_history_split.append(copy.deepcopy(node_split) )\n",
    "    tree_history_esti.append( copy.deepcopy(node_test)  )\n",
    "    tree_history_leaf.append( copy.deepcopy(leaf_nodes)  )\n",
    "    tree_history_runtime.append( copy.deepcopy(run_time)  )\n",
    "\n",
    "        \n",
    "    print \"error at tree height\", i,\":\",  tmp_test_err, tmp_train_err\n",
    "    print \"running time at tree height\", i,\":\",  elapsed\n",
    "    print \"number of leaf nodes at tree height\", i,\":\",  len(leaf_nodes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print len(tree_history_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82.06865487, 3, 12, 103)\n",
      "1023\n",
      "2046\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "print dta_test.first()\n",
    "err_rdd=dta_test.map(lambda line:search_nodeToData(line[1:numFeatures+1], node_split) ) \n",
    "print len(node_split)\n",
    "print len(node_test)\n",
    "print len(leaf_nodes)\n",
    "\n",
    "# print node_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.995640799666\n"
     ]
    }
   ],
   "source": [
    "err_rdd=dta_test.map(lambda line:(line[0],leaf_nodes[search_nodeToData(line[1:numFeatures+1], node_split) ], search_nodeToData(line[1:numFeatures+1], node_split) ) )\n",
    "\n",
    "print err_rdd.map( lambda line: (line[0]-line[1])*(line[0]-line[1])  ).reduce(lambda a, b: a+b)/err_rdd.count()\n",
    "# err_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1023\n",
      "1\n",
      "2046\n",
      "1\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "#LAD data backup\n",
    "lad_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "lad_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "lad_tree_history_leaf =  copy.deepcopy( tree_history_leaf)\n",
    "lad_tree_history_runtime =  copy.deepcopy( tree_history_runtime)\n",
    "\n",
    "\n",
    "print len(lad_tree_history_split)\n",
    "print len(lad_tree_history_split[0])\n",
    "print len(lad_tree_history_esti)\n",
    "print len(lad_tree_history_esti[0])\n",
    "\n",
    "print len(lad_tree_history_leaf)\n",
    "print len(lad_tree_history_leaf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error: []\n",
      "training error: []\n",
      "1\n",
      "non_leaf_node count at previous depth 0\n",
      "depth 0 : 1\n",
      "[[3, 6, 4, 2, 5], [12, 15, 13, 11, 14], [105, 102, 103, 104, 101]] non-leaf node\n",
      "non_leaf_node count at previous depth 1\n",
      "depth 1 : 2\n",
      "[[3, 6, 4, 2, 5], [12, 15, 13, 11, 14], [103, 104, 105]] non-leaf node\n",
      "[[3, 6, 4, 2, 5], [12, 15, 13, 11, 14], [101, 102]] non-leaf node\n",
      "non_leaf_node count at previous depth 2\n",
      "depth 2 : 4\n",
      "[[3, 6, 4, 2, 5], [15], [103, 104, 105]] non-leaf node\n",
      "[[3, 6, 4, 2, 5], [11, 12, 13, 14], [103, 104, 105]] non-leaf node\n",
      "[[3], [12, 15, 13, 11, 14], [101, 102]] non-leaf node\n",
      "[[2, 4, 5, 6], [12, 15, 13, 11, 14], [101, 102]] non-leaf node\n",
      "non_leaf_node count at previous depth 4\n",
      "depth 3 : 8\n",
      "[[3, 6, 4, 2, 5], [15], [103]] non-leaf node\n",
      "[[3, 6, 4, 2, 5], [15], [104, 105]] non-leaf node\n",
      "[[2, 4, 6], [11, 12, 13, 14], [103, 104, 105]] non-leaf node\n",
      "[[3, 5], [11, 12, 13, 14], [103, 104, 105]] non-leaf node\n",
      "[[3], [12, 13, 15], [101, 102]] non-leaf node\n",
      "[[3], [11, 14], [101, 102]] non-leaf node\n",
      "[[5], [12, 15, 13, 11, 14], [101, 102]] non-leaf node\n",
      "[[2, 4, 6], [12, 15, 13, 11, 14], [101, 102]] non-leaf node\n",
      "non_leaf_node count at previous depth 8\n",
      "depth 4 : 16\n",
      "[[3, 4, 5, 6], [15], [103]] non-leaf node\n",
      "[[2], [15], [103]] has the estimate: 50.098156957 true: 50.096554595\n",
      "[[2, 3, 5], [15], [104, 105]] non-leaf node\n",
      "[[4, 6], [15], [104, 105]] non-leaf node\n",
      "[[2, 4, 6], [12, 13, 14], [103, 104, 105]] non-leaf node\n",
      "[[2, 4, 6], [11], [103, 104, 105]] non-leaf node\n",
      "[[3, 5], [11], [103, 104, 105]] non-leaf node\n",
      "[[3, 5], [12, 13, 14], [103, 104, 105]] non-leaf node\n",
      "[[3], [12, 15], [101, 102]] non-leaf node\n",
      "[[3], [13], [101, 102]] non-leaf node\n",
      "[[3], [14], [101, 102]] non-leaf node\n",
      "[[3], [11], [101, 102]] non-leaf node\n",
      "[[5], [11, 13], [101, 102]] non-leaf node\n",
      "[[5], [12, 14, 15], [101, 102]] non-leaf node\n",
      "[[2, 4, 6], [12, 13, 14, 15], [101, 102]] non-leaf node\n",
      "[[2, 4, 6], [11], [101, 102]] non-leaf node\n",
      "non_leaf_node count at previous depth 15\n",
      "depth 5 : 30\n",
      "[[3, 5, 6], [15], [103]] non-leaf node\n",
      "[[4], [15], [103]] has the estimate: 28.0719127586 true: 28.0414101739\n",
      "[[2, 3, 5], [15], [105]] non-leaf node\n",
      "[[2, 3, 5], [15], [104]] non-leaf node\n",
      "[[4, 6], [15], [104]] non-leaf node\n",
      "[[4, 6], [15], [105]] non-leaf node\n",
      "[[2], [12, 13, 14], [103, 104, 105]] non-leaf node\n",
      "[[4, 6], [12, 13, 14], [103, 104, 105]] non-leaf node\n",
      "[[2, 4, 6], [11], [104]] non-leaf node\n",
      "[[2, 4, 6], [11], [105, 103]] non-leaf node\n",
      "[[3, 5], [11], [105]] non-leaf node\n",
      "[[3, 5], [11], [104, 103]] non-leaf node\n",
      "[[3, 5], [12, 13, 14], [105]] non-leaf node\n",
      "[[3, 5], [12, 13, 14], [104, 103]] non-leaf node\n",
      "[[3], [12, 15], [101]] non-leaf node\n",
      "[[3], [12, 15], [102]] non-leaf node\n",
      "[[3], [13], [101]] has the estimate: 42.9677692236 true: 42.9267778604\n",
      "[[3], [13], [102]] has the estimate: 44.0076343926 true: 43.9771010288\n",
      "[[3], [14], [101]] has the estimate: 64.0650099639 true: 63.9998848045\n",
      "[[3], [14], [102]] has the estimate: 98.1495199224 true: 98.0827886987\n",
      "[[3], [11], [102]] has the estimate: 9.01486253082 true: 9.00938849647\n",
      "[[3], [11], [101]] has the estimate: 95.9559627625 true: 95.9642227771\n",
      "[[5], [11, 13], [102]] non-leaf node\n",
      "[[5], [11, 13], [101]] non-leaf node\n",
      "[[5], [12, 14, 15], [101]] non-leaf node\n",
      "[[5], [12, 14, 15], [102]] non-leaf node\n",
      "[[2, 4, 6], [14, 15], [101, 102]] non-leaf node\n",
      "[[2, 4, 6], [12, 13], [101, 102]] non-leaf node\n",
      "[[2, 4, 6], [11], [101]] non-leaf node\n",
      "[[2, 4, 6], [11], [102]] non-leaf node\n",
      "non_leaf_node count at previous depth 23\n",
      "depth 6 : 46\n",
      "[[5], [15], [103]] has the estimate: 18.9929420599 true: 18.9588365801\n",
      "[[3, 6], [15], [103]] non-leaf node\n",
      "[[2], [15], [105]] has the estimate: 4.01622025203 true: 4.00219652747\n",
      "[[3, 5], [15], [105]] non-leaf node\n",
      "[[3, 5], [15], [104]] non-leaf node\n",
      "[[2], [15], [104]] has the estimate: 68.9490680038 true: 68.9629558988\n",
      "[[6], [15], [104]] has the estimate: 20.0454893734 true: 20.0117960529\n",
      "[[4], [15], [104]] has the estimate: 39.9918406863 true: 40.0153444063\n",
      "[[4], [15], [105]] has the estimate: 46.0796506042 true: 46.0838238983\n",
      "[[6], [15], [105]] has the estimate: 56.0030014348 true: 55.9782638144\n",
      "[[2], [13, 14], [103, 104, 105]] non-leaf node\n",
      "[[2], [12], [103, 104, 105]] non-leaf node\n",
      "[[4, 6], [12, 13, 14], [103]] non-leaf node\n",
      "[[4, 6], [12, 13, 14], [104, 105]] non-leaf node\n",
      "[[2, 6], [11], [104]] non-leaf node\n",
      "[[4], [11], [104]] has the estimate: 73.9162235843 true: 73.9282383826\n",
      "[[4], [11], [105, 103]] non-leaf node\n",
      "[[2, 6], [11], [105, 103]] non-leaf node\n",
      "[[3], [11], [105]] has the estimate: 5.98885478944 true: 5.94799312481\n",
      "[[5], [11], [105]] has the estimate: 23.0994261507 true: 23.0617723397\n",
      "[[3], [11], [104, 103]] non-leaf node\n",
      "[[5], [11], [104, 103]] non-leaf node\n",
      "[[3], [12, 13, 14], [105]] non-leaf node\n",
      "[[5], [12, 13, 14], [105]] non-leaf node\n",
      "[[3, 5], [12], [104, 103]] non-leaf node\n",
      "[[3, 5], [13, 14], [104, 103]] non-leaf node\n",
      "[[3], [12], [101]] has the estimate: 0.978846112798 true: 0.995016382114\n",
      "[[3], [15], [101]] has the estimate: 25.0229663173 true: 24.9945160202\n",
      "[[3], [15], [102]] has the estimate: 23.9944136831 true: 23.9625677351\n",
      "[[3], [12], [102]] has the estimate: 87.0298044703 true: 87.0175606518\n",
      "[[5], [13], [102]] has the estimate: 14.9561542632 true: 14.9587456538\n",
      "[[5], [11], [102]] has the estimate: 31.0491668888 true: 30.9791007428\n",
      "[[5], [11], [101]] has the estimate: 52.9530866186 true: 52.9720505533\n",
      "[[5], [13], [101]] has the estimate: 78.9462321508 true: 78.9781796176\n",
      "[[5], [14, 15], [101]] non-leaf node\n",
      "[[5], [12], [101]] has the estimate: 98.1103603032 true: 98.0778430528\n",
      "[[5], [12], [102]] has the estimate: 51.9395410304 true: 51.9356296313\n",
      "[[5], [14, 15], [102]] non-leaf node\n",
      "[[6], [14, 15], [101, 102]] non-leaf node\n",
      "[[2, 4], [14, 15], [101, 102]] non-leaf node\n",
      "[[4], [12, 13], [101, 102]] non-leaf node\n",
      "[[2, 6], [12, 13], [101, 102]] non-leaf node\n",
      "[[4], [11], [101]] has the estimate: 65.0126033919 true: 65.0169132598\n",
      "[[2, 6], [11], [101]] non-leaf node\n",
      "[[6], [11], [102]] has the estimate: 39.0031770198 true: 38.9968112979\n",
      "[[2, 4], [11], [102]] non-leaf node\n",
      "non_leaf_node count at previous depth 24\n",
      "depth 7 : 48\n",
      "[[6], [15], [103]] has the estimate: 20.9959461137 true: 21.0143744806\n",
      "[[3], [15], [103]] has the estimate: 22.00192963 true: 22.0028669441\n",
      "[[3], [15], [105]] has the estimate: 33.0007883144 true: 32.9357737994\n",
      "[[5], [15], [105]] has the estimate: 38.0205112565 true: 37.9844489418\n",
      "[[5], [15], [104]] has the estimate: 31.0185473542 true: 31.0231697459\n",
      "[[3], [15], [104]] has the estimate: 36.0232692597 true: 36.0281796575\n",
      "[[2], [13, 14], [104]] non-leaf node\n",
      "[[2], [13, 14], [105, 103]] non-leaf node\n",
      "[[2], [12], [104, 105]] non-leaf node\n",
      "[[2], [12], [103]] has the estimate: 78.0321293761 true: 77.9958115171\n",
      "[[4, 6], [13, 14], [103]] non-leaf node\n",
      "[[4, 6], [12], [103]] non-leaf node\n",
      "[[4, 6], [13, 14], [104, 105]] non-leaf node\n",
      "[[4, 6], [12], [104, 105]] non-leaf node\n",
      "[[2], [11], [104]] has the estimate: 41.0705096362 true: 41.0353254845\n",
      "[[6], [11], [104]] has the estimate: 42.1091447039 true: 42.1019409905\n",
      "[[4], [11], [103]] has the estimate: 23.9944056617 true: 23.9830225896\n",
      "[[4], [11], [105]] has the estimate: 58.9543003693 true: 58.9558428993\n",
      "[[2, 6], [11], [103]] non-leaf node\n",
      "[[2, 6], [11], [105]] non-leaf node\n",
      "[[3], [11], [104]] has the estimate: 43.0519677194 true: 43.0189743283\n",
      "[[3], [11], [103]] has the estimate: 44.009757979 true: 44.0271996417\n",
      "[[5], [11], [103]] has the estimate: 65.9990921296 true: 65.9755831538\n",
      "[[5], [11], [104]] has the estimate: 97.1385614942 true: 97.0549977642\n",
      "[[3], [12, 13], [105]] non-leaf node\n",
      "[[3], [14], [105]] has the estimate: 95.0119792969 true: 95.0063896209\n",
      "[[5], [14], [105]] has the estimate: 18.9792826732 true: 18.9605451895\n",
      "[[5], [12, 13], [105]] non-leaf node\n",
      "[[5], [12], [104, 103]] non-leaf node\n",
      "[[3], [12], [104, 103]] non-leaf node\n",
      "[[3, 5], [13, 14], [104]] non-leaf node\n",
      "[[3, 5], [13, 14], [103]] non-leaf node\n",
      "[[5], [14], [101]] has the estimate: 33.0212493181 true: 33.0241456091\n",
      "[[5], [15], [101]] has the estimate: 59.0080915032 true: 59.0264689632\n",
      "[[5], [15], [102]] has the estimate: 72.0703118902 true: 72.0525420135\n",
      "[[5], [14], [102]] has the estimate: 85.987239036 true: 85.9819077959\n",
      "[[6], [15], [101, 102]] non-leaf node\n",
      "[[6], [14], [101, 102]] non-leaf node\n",
      "[[2, 4], [14, 15], [101]] non-leaf node\n",
      "[[2, 4], [14, 15], [102]] non-leaf node\n",
      "[[4], [12, 13], [102]] non-leaf node\n",
      "[[4], [12, 13], [101]] non-leaf node\n",
      "[[2, 6], [12, 13], [102]] non-leaf node\n",
      "[[2, 6], [12, 13], [101]] non-leaf node\n",
      "[[2], [11], [101]] has the estimate: 85.9585811583 true: 85.9944274041\n",
      "[[6], [11], [101]] has the estimate: 94.9999508839 true: 94.9873515467\n",
      "[[4], [11], [102]] has the estimate: 90.0192858197 true: 90.0372206241\n",
      "[[2], [11], [102]] has the estimate: 91.9176245931 true: 91.9335367433\n",
      "non_leaf_node count at previous depth 23\n",
      "depth 8 : 46\n",
      "[[2], [14], [104]] has the estimate: 3.00438112413 true: 3.0027508578\n",
      "[[2], [13], [104]] has the estimate: 7.01838588944 true: 7.03395506407\n",
      "[[2], [13, 14], [105]] non-leaf node\n",
      "[[2], [13, 14], [103]] non-leaf node\n",
      "[[2], [12], [105]] has the estimate: 40.0616697529 true: 40.0313041475\n",
      "[[2], [12], [104]] has the estimate: 44.0817547819 true: 44.0633750192\n",
      "[[4, 6], [14], [103]] non-leaf node\n",
      "[[4, 6], [13], [103]] non-leaf node\n",
      "[[6], [12], [103]] has the estimate: 43.0311592882 true: 42.9990841506\n",
      "[[4], [12], [103]] has the estimate: 43.9226353598 true: 43.9519402509\n",
      "[[4, 6], [13], [104, 105]] non-leaf node\n",
      "[[4, 6], [14], [104, 105]] non-leaf node\n",
      "[[4, 6], [12], [105]] non-leaf node\n",
      "[[4, 6], [12], [104]] non-leaf node\n",
      "[[2], [11], [103]] has the estimate: 66.0400634614 true: 66.0087560856\n",
      "[[6], [11], [103]] has the estimate: 79.0012739643 true: 78.9669522955\n",
      "[[6], [11], [105]] has the estimate: 60.019723945 true: 60.0222207182\n",
      "[[2], [11], [105]] has the estimate: 72.0407735939 true: 72.0114719371\n",
      "[[3], [12], [105]] has the estimate: 36.9535858943 true: 36.9572677153\n",
      "[[3], [13], [105]] has the estimate: 45.9490269664 true: 45.9884461035\n",
      "[[5], [13], [105]] has the estimate: 71.0397888464 true: 70.9897816187\n",
      "[[5], [12], [105]] has the estimate: 87.9908041887 true: 87.9905451245\n",
      "[[5], [12], [104]] has the estimate: 18.9814612795 true: 18.98030566\n",
      "[[5], [12], [103]] has the estimate: 46.9558290428 true: 46.9440763592\n",
      "[[3], [12], [103]] has the estimate: 83.0064100726 true: 82.9730159638\n",
      "[[3], [12], [104]] has the estimate: 88.0346404815 true: 88.01639687\n",
      "[[3, 5], [14], [104]] non-leaf node\n",
      "[[3, 5], [13], [104]] non-leaf node\n",
      "[[3, 5], [13], [103]] non-leaf node\n",
      "[[3, 5], [14], [103]] non-leaf node\n",
      "[[6], [15], [102]] has the estimate: 12.0561381286 true: 12.0472007918\n",
      "[[6], [15], [101]] has the estimate: 38.0653817977 true: 38.0707082793\n",
      "[[6], [14], [102]] has the estimate: 43.9734633609 true: 44.0005878945\n",
      "[[6], [14], [101]] has the estimate: 66.0511551892 true: 66.0243315817\n",
      "[[2, 4], [14], [101]] non-leaf node\n",
      "[[2, 4], [15], [101]] non-leaf node\n",
      "[[2], [14, 15], [102]] non-leaf node\n",
      "[[4], [14, 15], [102]] non-leaf node\n",
      "[[4], [12], [102]] has the estimate: 31.0508857939 true: 31.0322361829\n",
      "[[4], [13], [102]] has the estimate: 79.0734560422 true: 79.0491984266\n",
      "[[4], [13], [101]] has the estimate: 23.9371724338 true: 23.995414092\n",
      "[[4], [12], [101]] has the estimate: 67.0330916215 true: 66.9984492051\n",
      "[[2], [12, 13], [102]] non-leaf node\n",
      "[[6], [12, 13], [102]] non-leaf node\n",
      "[[2, 6], [13], [101]] non-leaf node\n",
      "[[2, 6], [12], [101]] non-leaf node\n",
      "non_leaf_node count at previous depth 20\n",
      "depth 9 : 40\n",
      "[[2], [14], [105]] has the estimate: 14.9402057228 true: 14.9163855783\n",
      "[[2], [13], [105]] has the estimate: 18.0349235535 true: 18.0361054367\n",
      "[[2], [13], [103]] has the estimate: 7.05630014559 true: 7.04921513468\n",
      "[[2], [14], [103]] has the estimate: 93.9929735338 true: 93.9537311672\n",
      "[[4], [14], [103]] has the estimate: 18.9333741508 true: 18.9846489999\n",
      "[[6], [14], [103]] has the estimate: 43.0285254846 true: 42.9899536304\n",
      "[[6], [13], [103]] has the estimate: 24.9630060435 true: 24.9568590066\n",
      "[[4], [13], [103]] has the estimate: 30.0182579781 true: 30.0339704142\n",
      "[[4, 6], [13], [104]] non-leaf node\n",
      "[[4, 6], [13], [105]] non-leaf node\n",
      "[[6], [14], [104, 105]] non-leaf node\n",
      "[[4], [14], [104, 105]] non-leaf node\n",
      "[[4], [12], [105]] has the estimate: 37.0697921909 true: 37.0283167459\n",
      "[[6], [12], [105]] has the estimate: 59.037404075 true: 59.016648045\n",
      "[[6], [12], [104]] has the estimate: 55.1129713468 true: 55.1041205432\n",
      "[[4], [12], [104]] has the estimate: 84.0015749076 true: 83.9568700641\n",
      "[[5], [14], [104]] has the estimate: 9.94756762824 true: 9.96780065981\n",
      "[[3], [14], [104]] has the estimate: 64.0861686592 true: 64.0198044223\n",
      "[[5], [13], [104]] has the estimate: 85.0392475632 true: 85.0166748893\n",
      "[[3], [13], [104]] has the estimate: 94.9922938549 true: 94.9959237263\n",
      "[[5], [13], [103]] has the estimate: 87.0149160865 true: 86.9948139793\n",
      "[[3], [13], [103]] has the estimate: 92.0418826193 true: 92.0067499012\n",
      "[[3], [14], [103]] has the estimate: 95.9821321689 true: 95.9595642024\n",
      "[[5], [14], [103]] has the estimate: 98.9532153953 true: 98.9040786072\n",
      "[[2], [14], [101]] has the estimate: 33.0332279112 true: 33.0401838621\n",
      "[[4], [14], [101]] has the estimate: 67.0159901724 true: 67.0040520374\n",
      "[[4], [15], [101]] has the estimate: 10.0455164115 true: 10.0284561992\n",
      "[[2], [15], [101]] has the estimate: 74.9587920262 true: 74.9369559498\n",
      "[[2], [14], [102]] has the estimate: 27.0134566369 true: 26.9738916025\n",
      "[[2], [15], [102]] has the estimate: 69.0532671231 true: 69.0159606318\n",
      "[[4], [14], [102]] has the estimate: 74.9808017271 true: 74.9689172281\n",
      "[[4], [15], [102]] has the estimate: 92.0689197815 true: 92.0311198812\n",
      "[[2], [13], [102]] has the estimate: 49.0013267883 true: 48.9884006393\n",
      "[[2], [12], [102]] has the estimate: 65.0227830277 true: 65.0402774817\n",
      "[[6], [12], [102]] has the estimate: 69.9966715145 true: 70.0071138717\n",
      "[[6], [13], [102]] has the estimate: 76.0418652929 true: 76.0566138823\n",
      "[[6], [13], [101]] has the estimate: 69.0227044149 true: 69.0242027564\n",
      "[[2], [13], [101]] has the estimate: 83.0712069704 true: 83.0641514957\n",
      "[[2], [12], [101]] has the estimate: 80.0322623187 true: 79.9788623623\n",
      "[[6], [12], [101]] has the estimate: 99.0746602586 true: 99.0429218156\n",
      "bottom depth 10 : 8\n",
      "[[4], [13], [104]] has the estimate: 41.0934448024 true: 41.0516592466\n",
      "[[6], [13], [104]] has the estimate: 47.0719092886 true: 47.0243781273\n",
      "[[4], [13], [105]] has the estimate: 46.9441677213 true: 46.9871285229\n",
      "[[6], [13], [105]] has the estimate: 56.0496350882 true: 56.0145726411\n",
      "!!!  debug !!!: 665\n",
      "[[6], [14], [105]] has the estimate: 0.9486662643 true: 0.932495420567\n",
      "[[6], [14], [104]] has the estimate: 60.0650160787 true: 60.0257131569\n",
      "[[4], [14], [105]] has the estimate: 50.9287216517 true: 50.9585987436\n",
      "[[4], [14], [104]] has the estimate: 51.0061819087 true: 51.0202238742\n",
      "inter-node number: 117\n",
      " *******  number of identified configurations: 125\n",
      "125\n",
      "[0, 0, 0, 0, 1, 8, 30, 55, 81, 117, 125]\n",
      "0.00087839306213\n"
     ]
    }
   ],
   "source": [
    "# LAD result statistic: node infor.\n",
    "\n",
    "lad_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "lad_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "\n",
    "print 'test error:',test_err\n",
    "print 'training error:',train_err\n",
    "print len(lad_tree_history_split)\n",
    "\n",
    "recog_conf=bfs_tree(lad_tree_history_split[ 0 ], total_featureVal_set, maxdepth-1,  lad_tree_history_esti[0], dta_train )  \n",
    "print len(recog_conf[0])\n",
    "print recog_conf[1]\n",
    "print recog_conf[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(recog_conf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error at depth 1 : 761.866041467\n",
      "test error at depth 2 : 721.819082896\n",
      "test error at depth 3 : 685.477573829\n",
      "test error at depth 4 : 672.802274731\n",
      "test error at depth 5 : 550.63447638\n",
      "test error at depth 6 : 380.088389974\n",
      "test error at depth 7 : 232.571042001\n",
      "test error at depth 8 : 178.360779432\n",
      "test error at depth 9 : 27.3744642355\n",
      "test error at depth 10 : 0.994985641852\n",
      "[761.8660414673642, 721.8190828959343, 685.4775738288265, 672.8022747307433, 550.6344763795593, 380.08838997374625, 232.5710420009363, 178.3607794316177, 27.374464235463364, 0.9949856418517173]\n"
     ]
    }
   ],
   "source": [
    "# LAD result statistic : test error\n",
    "\n",
    "pre=0\n",
    "tree_esti = lad_tree_history_esti[0]\n",
    "tree_split =  lad_tree_history_split[ 0 ]\n",
    "test_error_depth=[]\n",
    "\n",
    "for i in range(1,maxdepth+1):\n",
    "            \n",
    "    end_bound= pre+ (int)( math.pow(2,i)) \n",
    "    test_leaf_nodes = tree_esti[ pre: end_bound  ]\n",
    "    pre= end_bound        \n",
    "    test_tree_split= tree_split[ 0:(int)( math.pow(2,i)-1)  ]\n",
    "    \n",
    "#     print len(test_tree_split)\n",
    "#     print len(test_leaf_nodes)\n",
    "#     test_error_tree( i, dta_test  )\n",
    "    test_error_depth.append( test_error_tree( i, dta_test  ) )\n",
    "\n",
    "print test_error_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1023\n",
      "1\n",
      "2046\n",
      "1\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "#MAD result backup\n",
    "\n",
    "mad_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "mad_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "mad_tree_history_leaf =  copy.deepcopy( tree_history_leaf)\n",
    "\n",
    "print len(mad_tree_history_split)\n",
    "print len(mad_tree_history_split[0])\n",
    "print len(mad_tree_history_esti)\n",
    "print len(mad_tree_history_esti[0])\n",
    "\n",
    "print len(mad_tree_history_leaf)\n",
    "print len(mad_tree_history_leaf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error: []\n",
      "training error: []\n",
      "1\n",
      "non_leaf_node count at previous depth 0\n",
      "depth 0 : 1\n",
      "[[3, 6, 4, 2, 5], [12, 15, 13, 11, 14], [105, 102, 103, 104, 101]] non-leaf node\n",
      "non_leaf_node count at previous depth 1\n",
      "depth 1 : 2\n",
      "[[3, 6, 4, 2, 5], [15], [105, 102, 103, 104, 101]] non-leaf node\n",
      "[[3, 6, 4, 2, 5], [11, 12, 13, 14], [105, 102, 103, 104, 101]] non-leaf node\n",
      "non_leaf_node count at previous depth 2\n",
      "depth 2 : 4\n",
      "[[4, 6], [15], [105, 102, 103, 104, 101]] non-leaf node\n",
      "[[2, 3, 5], [15], [105, 102, 103, 104, 101]] non-leaf node\n",
      "[[2, 6], [11, 12, 13, 14], [105, 102, 103, 104, 101]] non-leaf node\n",
      "[[3, 4, 5], [11, 12, 13, 14], [105, 102, 103, 104, 101]] non-leaf node\n",
      "non_leaf_node count at previous depth 4\n",
      "depth 3 : 8\n",
      "[[6], [15], [105, 102, 103, 104, 101]] non-leaf node\n",
      "[[4], [15], [105, 102, 103, 104, 101]] non-leaf node\n",
      "[[3, 5], [15], [105, 102, 103, 104, 101]] non-leaf node\n",
      "[[2], [15], [105, 102, 103, 104, 101]] non-leaf node\n",
      "[[2, 6], [11, 12, 13, 14], [102, 104]] non-leaf node\n",
      "[[2, 6], [11, 12, 13, 14], [105, 101, 103]] non-leaf node\n",
      "[[3, 4, 5], [11, 12, 13, 14], [101, 105]] has the estimate: 52.1792114313 !!! not seperated !!!\n",
      "[[3, 4, 5], [11, 12, 13, 14], [104, 102, 103]] non-leaf node\n",
      "non_leaf_node count at previous depth 7\n",
      "depth 4 : 14\n",
      "[[6], [15], [102, 103, 104]] non-leaf node\n",
      "[[6], [15], [105, 101]] non-leaf node\n",
      "[[4], [15], [101]] has the estimate: 10.0455164115 true: 10.0284561992\n",
      "[[4], [15], [104, 105, 102, 103]] non-leaf node\n",
      "[[3, 5], [15], [103]] non-leaf node\n",
      "[[3, 5], [15], [104, 105, 101, 102]] has the estimate: 34.9495528156 !!! not seperated !!!\n",
      "[[2], [15], [103, 105]] non-leaf node\n",
      "[[2], [15], [104, 101, 102]] non-leaf node\n",
      "[[2, 6], [11, 13, 14], [102, 104]] has the estimate: 43.0445639608 !!! not seperated !!!\n",
      "[[2, 6], [12], [102, 104]] non-leaf node\n",
      "[[2, 6], [11, 12, 13, 14], [105]] non-leaf node\n",
      "[[2, 6], [11, 12, 13, 14], [101, 103]] non-leaf node\n",
      "[[4], [11, 12, 13, 14], [104, 102, 103]] non-leaf node\n",
      "[[3, 5], [11, 12, 13, 14], [104, 102, 103]] non-leaf node\n",
      "non_leaf_node count at previous depth 11\n",
      "depth 5 : 22\n",
      "[[6], [15], [102]] has the estimate: 12.0561381286 true: 12.0472007918\n",
      "[[6], [15], [104, 103]] non-leaf node\n",
      "[[6], [15], [101]] has the estimate: 38.0653817977 true: 38.0707082793\n",
      "[[6], [15], [105]] has the estimate: 56.0030014348 true: 55.9782638144\n",
      "[[4], [15], [103, 104]] non-leaf node\n",
      "[[4], [15], [105, 102]] non-leaf node\n",
      "[[5], [15], [103]] has the estimate: 18.9929420599 true: 18.9588365801\n",
      "[[3], [15], [103]] has the estimate: 22.00192963 true: 22.0028669441\n",
      "[[2], [15], [105]] has the estimate: 4.01622025203 true: 4.00219652747\n",
      "[[2], [15], [103]] has the estimate: 50.098156957 true: 50.096554595\n",
      "[[2], [15], [102, 104]] non-leaf node\n",
      "[[2], [15], [101]] has the estimate: 74.9587920262 true: 74.9369559498\n",
      "[[2, 6], [12], [104]] non-leaf node\n",
      "[[2, 6], [12], [102]] non-leaf node\n",
      "[[2, 6], [14], [105]] non-leaf node\n",
      "[[2, 6], [11, 12, 13], [105]] non-leaf node\n",
      "[[2, 6], [14], [101, 103]] non-leaf node\n",
      "[[2, 6], [11, 12, 13], [101, 103]] has the estimate: 78.3791212534 !!! not seperated !!!\n",
      "[[4], [11, 12, 13, 14], [103]] non-leaf node\n",
      "[[4], [11, 12, 13, 14], [104, 102]] non-leaf node\n",
      "[[3, 5], [11], [104, 102, 103]] non-leaf node\n",
      "[[3, 5], [12, 13, 14], [104, 102, 103]] non-leaf node\n",
      "non_leaf_node count at previous depth 13\n",
      "depth 6 : 26\n",
      "[[6], [15], [104]] has the estimate: 20.0454893734 true: 20.0117960529\n",
      "[[6], [15], [103]] has the estimate: 20.9959461137 true: 21.0143744806\n",
      "[[4], [15], [103]] has the estimate: 28.0719127586 true: 28.0414101739\n",
      "[[4], [15], [104]] has the estimate: 39.9918406863 true: 40.0153444063\n",
      "[[4], [15], [105]] has the estimate: 46.0796506042 true: 46.0838238983\n",
      "[[4], [15], [102]] has the estimate: 92.0689197815 true: 92.0311198812\n",
      "[[2], [15], [104]] has the estimate: 68.9490680038 true: 68.9629558988\n",
      "[[2], [15], [102]] has the estimate: 69.0532671231 true: 69.0159606318\n",
      "[[2], [12], [104]] has the estimate: 44.0817547819 true: 44.0633750192\n",
      "[[6], [12], [104]] has the estimate: 55.1129713468 true: 55.1041205432\n",
      "[[2], [12], [102]] has the estimate: 65.0227830277 true: 65.0402774817\n",
      "[[6], [12], [102]] has the estimate: 69.9966715145 true: 70.0071138717\n",
      "[[6], [14], [105]] has the estimate: 0.9486662643 true: 0.932495420567\n",
      "[[2], [14], [105]] has the estimate: 14.9402057228 true: 14.9163855783\n",
      "[[2, 6], [13], [105]] non-leaf node\n",
      "[[2, 6], [11, 12], [105]] non-leaf node\n",
      "[[2, 6], [14], [101]] non-leaf node\n",
      "[[2, 6], [14], [103]] non-leaf node\n",
      "[[4], [13, 14], [103]] non-leaf node\n",
      "[[4], [11, 12], [103]] non-leaf node\n",
      "[[4], [13, 14], [104, 102]] non-leaf node\n",
      "[[4], [11, 12], [104, 102]] non-leaf node\n",
      "[[3, 5], [11], [102]] non-leaf node\n",
      "[[3, 5], [11], [104, 103]] non-leaf node\n",
      "[[3, 5], [12, 13], [104, 102, 103]] non-leaf node\n",
      "[[3, 5], [14], [104, 102, 103]] has the estimate: 93.5714566207 !!! not seperated !!!\n",
      "non_leaf_node count at previous depth 11\n",
      "depth 7 : 22\n",
      "[[2], [13], [105]] has the estimate: 18.0349235535 true: 18.0361054367\n",
      "[[6], [13], [105]] has the estimate: 56.0496350882 true: 56.0145726411\n",
      "[[6], [11, 12], [105]] non-leaf node\n",
      "[[2], [11, 12], [105]] non-leaf node\n",
      "[[2], [14], [101]] has the estimate: 33.0332279112 true: 33.0401838621\n",
      "[[6], [14], [101]] has the estimate: 66.0511551892 true: 66.0243315817\n",
      "[[6], [14], [103]] has the estimate: 43.0285254846 true: 42.9899536304\n",
      "[[2], [14], [103]] has the estimate: 93.9929735338 true: 93.9537311672\n",
      "[[4], [14], [103]] has the estimate: 18.9333741508 true: 18.9846489999\n",
      "[[4], [13], [103]] has the estimate: 30.0182579781 true: 30.0339704142\n",
      "[[4], [11], [103]] has the estimate: 23.9944056617 true: 23.9830225896\n",
      "[[4], [12], [103]] has the estimate: 43.9226353598 true: 43.9519402509\n",
      "[[4], [13, 14], [104]] non-leaf node\n",
      "[[4], [13, 14], [102]] non-leaf node\n",
      "[[4], [11, 12], [102]] non-leaf node\n",
      "[[4], [11, 12], [104]] non-leaf node\n",
      "[[3], [11], [102]] has the estimate: 9.01486253082 true: 9.00938849647\n",
      "[[5], [11], [102]] has the estimate: 31.0491668888 true: 30.9791007428\n",
      "[[3], [11], [104, 103]] non-leaf node\n",
      "[[5], [11], [104, 103]] non-leaf node\n",
      "[[3, 5], [12], [104, 102, 103]] non-leaf node\n",
      "[[3, 5], [13], [104, 102, 103]] non-leaf node\n",
      "non_leaf_node count at previous depth 10\n",
      "depth 8 : 20\n",
      "[[6], [12], [105]] has the estimate: 59.037404075 true: 59.016648045\n",
      "[[6], [11], [105]] has the estimate: 60.019723945 true: 60.0222207182\n",
      "[[2], [12], [105]] has the estimate: 40.0616697529 true: 40.0313041475\n",
      "[[2], [11], [105]] has the estimate: 72.0407735939 true: 72.0114719371\n",
      "[[4], [13], [104]] has the estimate: 41.0934448024 true: 41.0516592466\n",
      "[[4], [14], [104]] has the estimate: 51.0061819087 true: 51.0202238742\n",
      "[[4], [14], [102]] has the estimate: 74.9808017271 true: 74.9689172281\n",
      "[[4], [13], [102]] has the estimate: 79.0734560422 true: 79.0491984266\n",
      "[[4], [12], [102]] has the estimate: 31.0508857939 true: 31.0322361829\n",
      "[[4], [11], [102]] has the estimate: 90.0192858197 true: 90.0372206241\n",
      "[[4], [11], [104]] has the estimate: 73.9162235843 true: 73.9282383826\n",
      "[[4], [12], [104]] has the estimate: 84.0015749076 true: 83.9568700641\n",
      "[[3], [11], [104]] has the estimate: 43.0519677194 true: 43.0189743283\n",
      "[[3], [11], [103]] has the estimate: 44.009757979 true: 44.0271996417\n",
      "[[5], [11], [103]] has the estimate: 65.9990921296 true: 65.9755831538\n",
      "[[5], [11], [104]] has the estimate: 97.1385614942 true: 97.0549977642\n",
      "[[5], [12], [104, 102, 103]] non-leaf node\n",
      "[[3], [12], [104, 102, 103]] non-leaf node\n",
      "[[5], [13], [104, 102, 103]] non-leaf node\n",
      "[[3], [13], [104, 102, 103]] non-leaf node\n",
      "non_leaf_node count at previous depth 4\n",
      "depth 9 : 8\n",
      "[[5], [12], [104]] has the estimate: 18.9814612795 true: 18.98030566\n",
      "[[5], [12], [102, 103]] non-leaf node\n",
      "[[3], [12], [103]] has the estimate: 83.0064100726 true: 82.9730159638\n",
      "[[3], [12], [104, 102]] non-leaf node\n",
      "[[5], [13], [102]] has the estimate: 14.9561542632 true: 14.9587456538\n",
      "[[5], [13], [104, 103]] non-leaf node\n",
      "[[3], [13], [102]] has the estimate: 44.0076343926 true: 43.9771010288\n",
      "[[3], [13], [104, 103]] non-leaf node\n",
      "bottom depth 10 : 8\n",
      "[[5], [12], [103]] has the estimate: 46.9558290428 true: 46.9440763592\n",
      "[[5], [12], [102]] has the estimate: 51.9395410304 true: 51.9356296313\n",
      "[[3], [12], [102]] has the estimate: 87.0298044703 true: 87.0175606518\n",
      "[[3], [12], [104]] has the estimate: 88.0346404815 true: 88.01639687\n",
      "[[5], [13], [104]] has the estimate: 85.0392475632 true: 85.0166748893\n",
      "[[5], [13], [103]] has the estimate: 87.0149160865 true: 86.9948139793\n",
      "[[3], [13], [103]] has the estimate: 92.0418826193 true: 92.0067499012\n",
      "[[3], [13], [104]] has the estimate: 94.9922938549 true: 94.9959237263\n",
      "inter-node number: 60\n",
      " *******  number of identified configurations: 63\n",
      "63\n",
      "[0, 0, 0, 0, 1, 9, 23, 35, 51, 55, 63]\n",
      "0.00072720810899\n"
     ]
    }
   ],
   "source": [
    "# MAD result statistic\n",
    "\n",
    "print 'test error:',test_err\n",
    "print 'training error:',train_err\n",
    "print len(mad_tree_history_split)\n",
    "\n",
    "recog_conf = bfs_tree(mad_tree_history_split[ 0 ], total_featureVal_set, maxdepth-1,  mad_tree_history_esti[0], dta_train )  \n",
    "\n",
    "\n",
    "print len(recog_conf[0])\n",
    "print recog_conf[1]\n",
    "print recog_conf[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error at depth 1 : 724.66234824\n",
      "test error at depth 2 : 823.350806179\n",
      "test error at depth 3 : 817.638707244\n",
      "test error at depth 4 : 710.780446904\n",
      "test error at depth 5 : 592.913447513\n",
      "test error at depth 6 : 566.120821371\n",
      "test error at depth 7 : 507.235064184\n",
      "test error at depth 8 : 428.118960011\n",
      "test error at depth 9 : 363.740997535\n",
      "test error at depth 10 : 363.585296244\n",
      "[724.6623482398529, 823.3508061791998, 817.6387072436961, 710.780446904384, 592.9134475132386, 566.120821370631, 507.2350641839725, 428.1189600112966, 363.7409975347517, 363.58529624398716]\n"
     ]
    }
   ],
   "source": [
    "# MAD result statistic : test error\n",
    "\n",
    "pre=0\n",
    "tree_esti = mad_tree_history_esti[0]\n",
    "tree_split = mad_tree_history_split[ 0 ]\n",
    "test_error_depth=[]\n",
    "\n",
    "for i in range(1,maxdepth+1):\n",
    "            \n",
    "    end_bound= pre+ (int)( math.pow(2,i)) \n",
    "    test_leaf_nodes = tree_esti[ pre: end_bound  ]\n",
    "    pre= end_bound        \n",
    "    test_tree_split= tree_split[ 0:(int)( math.pow(2,i)-1)  ]\n",
    "    \n",
    "#     print len(test_tree_split)\n",
    "#     print len(test_leaf_nodes)\n",
    "#     test_error_tree( i, dta_test  )\n",
    "    \n",
    "    test_error_depth.append( test_error_tree( i, dta_test  ) )\n",
    "\n",
    "print test_error_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MSE result backup\n",
    "\n",
    "mse_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "mse_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "mse_tree_history_leaf =  copy.deepcopy( tree_history_leaf)\n",
    "\n",
    "print len(mse_tree_history_split)\n",
    "print len(mse_tree_history_split[0])\n",
    "print len(mse_tree_history_esti)\n",
    "print len(mse_tree_history_esti[0])\n",
    "\n",
    "print len(mse_tree_history_leaf)\n",
    "print len(mse_tree_history_leaf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error: [2407.469996222687]\n",
      "training error: [2349.9728802445043]\n",
      "1\n",
      "non_leaf_node count at previous depth 0\n",
      "depth 0 : 1\n",
      "[[3, 6, 4, 2, 5], [12, 15, 13, 11, 14], [105, 102, 103, 104, 101]] non-leaf node\n",
      "non_leaf_node count at previous depth 1\n",
      "depth 1 : 2\n",
      "[[3, 6, 4, 2, 5], [12, 15, 13, 11, 14], [103, 104, 105]] non-leaf node\n",
      "[[3, 6, 4, 2, 5], [12, 15, 13, 11, 14], [101, 102]] non-leaf node\n",
      "non_leaf_node count at previous depth 2\n",
      "depth 2 : 4\n",
      "[[3, 6, 4, 2, 5], [15], [103, 104, 105]] non-leaf node\n",
      "[[3, 6, 4, 2, 5], [11, 12, 13, 14], [103, 104, 105]] non-leaf node\n",
      "[[3, 5], [12, 15, 13, 11, 14], [101, 102]] non-leaf node\n",
      "[[2, 4, 6], [12, 15, 13, 11, 14], [101, 102]] non-leaf node\n",
      "non_leaf_node count at previous depth 4\n",
      "depth 3 : 8\n",
      "[[3, 6, 4, 2, 5], [15], [103]] non-leaf node\n",
      "[[3, 6, 4, 2, 5], [15], [104, 105]] non-leaf node\n",
      "[[2, 4, 6], [11, 12, 13, 14], [103, 104, 105]] non-leaf node\n",
      "[[3, 5], [11, 12, 13, 14], [103, 104, 105]] non-leaf node\n",
      "[[3, 5], [11, 13, 15], [101, 102]] non-leaf node\n",
      "[[3, 5], [12, 14], [101, 102]] non-leaf node\n",
      "[[2, 4, 6], [12, 13, 14, 15], [101, 102]] non-leaf node\n",
      "[[2, 4, 6], [11], [101, 102]] non-leaf node\n",
      "non_leaf_node count at previous depth 8\n",
      "depth 4 : 16\n",
      "[[3, 4, 5, 6], [15], [103]] non-leaf node\n",
      "[[2], [15], [103]] has the estimate: 50.2814961842 true: 50.1542483803\n",
      "[[3, 4, 5, 6], [15], [104, 105]] non-leaf node\n",
      "[[2], [15], [104, 105]] non-leaf node\n",
      "[[2, 4, 6], [13, 14], [103, 104, 105]] non-leaf node\n",
      "[[2, 4, 6], [11, 12], [103, 104, 105]] non-leaf node\n",
      "[[3, 5], [11], [103, 104, 105]] non-leaf node\n",
      "[[3, 5], [12, 13, 14], [103, 104, 105]] non-leaf node\n",
      "[[3, 5], [11, 13, 15], [102]] non-leaf node\n",
      "[[3, 5], [11, 13, 15], [101]] non-leaf node\n",
      "[[3, 5], [12, 14], [101]] non-leaf node\n",
      "[[3, 5], [12, 14], [102]] non-leaf node\n",
      "[[2, 4, 6], [14], [101, 102]] non-leaf node\n",
      "[[2, 4, 6], [12, 13, 15], [101, 102]] non-leaf node\n",
      "[[4], [11], [101, 102]] non-leaf node\n",
      "[[2, 6], [11], [101, 102]] non-leaf node\n",
      "non_leaf_node count at previous depth 15\n",
      "depth 5 : 30\n",
      "[[3, 5, 6], [15], [103]] non-leaf node\n",
      "[[4], [15], [103]] has the estimate: 28.136311455 true: 28.0444748008\n",
      "[[3, 5, 6], [15], [104, 105]] non-leaf node\n",
      "[[4], [15], [104, 105]] non-leaf node\n",
      "[[2], [15], [105]] has the estimate: 5.26000186623 true: 5.40201131687\n",
      "[[2], [15], [104]] has the estimate: 69.0360653815 true: 69.0633933789\n",
      "[[2], [13, 14], [103, 104, 105]] non-leaf node\n",
      "[[4, 6], [13, 14], [103, 104, 105]] non-leaf node\n",
      "[[2, 4, 6], [12], [103, 104, 105]] non-leaf node\n",
      "[[2, 4, 6], [11], [103, 104, 105]] non-leaf node\n",
      "[[3, 5], [11], [105]] non-leaf node\n",
      "[[3, 5], [11], [104, 103]] non-leaf node\n",
      "[[3, 5], [12, 13, 14], [105]] non-leaf node\n",
      "[[3, 5], [12, 13, 14], [104, 103]] non-leaf node\n",
      "[[3], [11, 13, 15], [102]] non-leaf node\n",
      "[[5], [11, 13, 15], [102]] non-leaf node\n",
      "[[3], [11, 13, 15], [101]] non-leaf node\n",
      "[[5], [11, 13, 15], [101]] non-leaf node\n",
      "[[3, 5], [14], [101]] non-leaf node\n",
      "[[3, 5], [12], [101]] has the estimate: 97.9644128499 !!! not seperated !!!\n",
      "[[5], [12, 14], [102]] non-leaf node\n",
      "[[3], [12, 14], [102]] non-leaf node\n",
      "[[2], [14], [101, 102]] non-leaf node\n",
      "[[4, 6], [14], [101, 102]] non-leaf node\n",
      "[[4], [12, 13, 15], [101, 102]] non-leaf node\n",
      "[[2, 6], [12, 13, 15], [101, 102]] non-leaf node\n",
      "[[4], [11], [101]] has the estimate: 65.0405171149 true: 65.0847506012\n",
      "[[4], [11], [102]] has the estimate: 89.9760142129 true: 89.9453530404\n",
      "[[2, 6], [11], [102]] non-leaf node\n",
      "[[2, 6], [11], [101]] non-leaf node\n",
      "non_leaf_node count at previous depth 24\n",
      "depth 6 : 48\n",
      "[[5], [15], [103]] has the estimate: 19.0793798593 true: 19.1267123158\n",
      "[[3, 6], [15], [103]] non-leaf node\n",
      "[[3, 5, 6], [15], [104]] non-leaf node\n",
      "[[3, 5, 6], [15], [105]] non-leaf node\n",
      "[[4], [15], [104]] has the estimate: 40.0974875321 true: 40.0600336424\n",
      "[[4], [15], [105]] has the estimate: 45.8814045218 true: 45.9445415891\n",
      "[[2], [13, 14], [103, 104]] non-leaf node\n",
      "[[2], [13, 14], [105]] non-leaf node\n",
      "[[4, 6], [13, 14], [103]] non-leaf node\n",
      "[[4, 6], [13, 14], [104, 105]] non-leaf node\n",
      "[[2, 4, 6], [12], [103, 105]] non-leaf node\n",
      "[[2, 4, 6], [12], [104]] non-leaf node\n",
      "[[2, 4, 6], [11], [104]] non-leaf node\n",
      "[[2, 4, 6], [11], [105, 103]] non-leaf node\n",
      "[[3], [11], [105]] has the estimate: 6.15473756703 true: 6.18460909318\n",
      "[[5], [11], [105]] has the estimate: 22.9478852096 true: 23.0019210725\n",
      "[[3], [11], [104, 103]] non-leaf node\n",
      "[[5], [11], [104, 103]] non-leaf node\n",
      "[[3], [12, 13, 14], [105]] non-leaf node\n",
      "[[5], [12, 13, 14], [105]] non-leaf node\n",
      "[[3, 5], [12, 14], [104, 103]] non-leaf node\n",
      "[[3, 5], [13], [104, 103]] non-leaf node\n",
      "[[3], [11, 15], [102]] non-leaf node\n",
      "[[3], [13], [102]] has the estimate: 43.7760485026 true: 43.7873844788\n",
      "[[5], [11, 13], [102]] non-leaf node\n",
      "[[5], [15], [102]] has the estimate: 72.1208609766 true: 71.9597620243\n",
      "[[3], [13, 15], [101]] non-leaf node\n",
      "[[3], [11], [101]] has the estimate: 95.9654449004 true: 96.0594627584\n",
      "[[5], [11, 15], [101]] non-leaf node\n",
      "[[5], [13], [101]] has the estimate: 78.7359038898 true: 78.8258497725\n",
      "[[5], [14], [101]] has the estimate: 32.9980386212 true: 33.0122066436\n",
      "[[3], [14], [101]] has the estimate: 64.0193442731 true: 64.0089483761\n",
      "[[5], [12], [102]] has the estimate: 51.9371527814 true: 51.8430266052\n",
      "[[5], [14], [102]] has the estimate: 86.1866472463 true: 86.2115977499\n",
      "[[3], [12], [102]] has the estimate: 87.0128579189 true: 86.9570034372\n",
      "[[3], [14], [102]] has the estimate: 98.1944956221 true: 98.1049944494\n",
      "[[2], [14], [102]] has the estimate: 26.8837430827 true: 27.0131307451\n",
      "[[2], [14], [101]] has the estimate: 32.9386307868 true: 33.0057349654\n",
      "[[6], [14], [101, 102]] non-leaf node\n",
      "[[4], [14], [101, 102]] non-leaf node\n",
      "[[4], [12, 13, 15], [101]] non-leaf node\n",
      "[[4], [12, 13, 15], [102]] non-leaf node\n",
      "[[2, 6], [12, 13, 15], [102]] non-leaf node\n",
      "[[2, 6], [12, 13, 15], [101]] non-leaf node\n",
      "[[6], [11], [102]] has the estimate: 39.0044929606 true: 39.1061466109\n",
      "[[2], [11], [102]] has the estimate: 92.2600778195 true: 92.198421709\n",
      "[[2], [11], [101]] has the estimate: 85.7474657611 true: 85.8461817207\n",
      "[[6], [11], [101]] has the estimate: 95.2791393251 true: 95.1324382568\n",
      "non_leaf_node count at previous depth 27\n",
      "depth 7 : 54\n",
      "[[6], [15], [103]] has the estimate: 20.8646343081 true: 20.8671861329\n",
      "[[3], [15], [103]] has the estimate: 21.9779241908 true: 21.9560746705\n",
      "[[6], [15], [104]] has the estimate: 19.7963640948 true: 19.8974526756\n",
      "[[3, 5], [15], [104]] non-leaf node\n",
      "[[3, 5], [15], [105]] non-leaf node\n",
      "[[6], [15], [105]] has the estimate: 56.195328187 true: 56.090071079\n",
      "[[2], [13], [103, 104]] non-leaf node\n",
      "[[2], [14], [103, 104]] non-leaf node\n",
      "[[2], [14], [105]] has the estimate: 15.0838014487 true: 15.0687905007\n",
      "[[2], [13], [105]] has the estimate: 17.723008487 true: 17.8180486796\n",
      "[[4], [13, 14], [103]] non-leaf node\n",
      "[[6], [13, 14], [103]] non-leaf node\n",
      "[[4], [13, 14], [104, 105]] non-leaf node\n",
      "[[6], [13, 14], [104, 105]] non-leaf node\n",
      "[[2, 4, 6], [12], [105]] non-leaf node\n",
      "[[2, 4, 6], [12], [103]] non-leaf node\n",
      "[[2, 6], [12], [104]] non-leaf node\n",
      "[[4], [12], [104]] has the estimate: 84.0901219167 true: 84.0494002074\n",
      "[[2, 6], [11], [104]] non-leaf node\n",
      "[[4], [11], [104]] has the estimate: 74.2311960499 true: 74.1460245804\n",
      "[[4], [11], [105, 103]] non-leaf node\n",
      "[[2, 6], [11], [105, 103]] non-leaf node\n",
      "[[3], [11], [104]] has the estimate: 42.9487247189 true: 42.9452037314\n",
      "[[3], [11], [103]] has the estimate: 43.9308156316 true: 43.9558344651\n",
      "[[5], [11], [103]] has the estimate: 65.9839225896 true: 66.039818055\n",
      "[[5], [11], [104]] has the estimate: 96.8171035969 true: 97.0132965575\n",
      "[[3], [12, 13], [105]] non-leaf node\n",
      "[[3], [14], [105]] has the estimate: 94.8131922308 true: 94.7313081496\n",
      "[[5], [14], [105]] has the estimate: 18.9049787472 true: 18.843546738\n",
      "[[5], [12, 13], [105]] non-leaf node\n",
      "[[5], [12, 14], [104, 103]] non-leaf node\n",
      "[[3], [12, 14], [104, 103]] non-leaf node\n",
      "[[5], [13], [104, 103]] non-leaf node\n",
      "[[3], [13], [104, 103]] non-leaf node\n",
      "[[3], [11], [102]] has the estimate: 8.99601952406 true: 9.06628301132\n",
      "[[3], [15], [102]] has the estimate: 23.9419631052 true: 23.9722817362\n",
      "[[5], [13], [102]] has the estimate: 14.9667520429 true: 15.0405524099\n",
      "[[5], [11], [102]] has the estimate: 31.1277542275 true: 31.1372562809\n",
      "[[3], [15], [101]] has the estimate: 25.0175021513 true: 25.0127764983\n",
      "[[3], [13], [101]] has the estimate: 42.8045598875 true: 42.7829407651\n",
      "[[5], [11], [101]] has the estimate: 52.9458196284 true: 52.8353507875\n",
      "[[5], [15], [101]] has the estimate: 59.0955644447 true: 59.0259875004\n",
      "[[6], [14], [102]] has the estimate: 44.0077924715 true: 43.947823395\n",
      "[[6], [14], [101]] has the estimate: 66.2128299196 true: 66.1973779353\n",
      "[[4], [14], [101]] has the estimate: 67.024451503 true: 67.0586725014\n",
      "[[4], [14], [102]] has the estimate: 75.052977853 true: 75.1269714329\n",
      "[[4], [13, 15], [101]] non-leaf node\n",
      "[[4], [12], [101]] has the estimate: 66.9783421994 true: 66.986629664\n",
      "[[4], [12], [102]] has the estimate: 31.0909521236 true: 31.1540156806\n",
      "[[4], [13, 15], [102]] non-leaf node\n",
      "[[2], [12, 13, 15], [102]] non-leaf node\n",
      "[[6], [12, 13, 15], [102]] non-leaf node\n",
      "[[6], [12, 13, 15], [101]] non-leaf node\n",
      "[[2], [12, 13, 15], [101]] non-leaf node\n",
      "non_leaf_node count at previous depth 26\n",
      "depth 8 : 52\n",
      "[[5], [15], [104]] has the estimate: 30.921142493 true: 30.9259577565\n",
      "[[3], [15], [104]] has the estimate: 35.8550748021 true: 35.8746920788\n",
      "[[3], [15], [105]] has the estimate: 32.972722109 true: 32.9916538477\n",
      "[[5], [15], [105]] has the estimate: 37.934939313 true: 37.9097263771\n",
      "[[2], [13], [104]] has the estimate: 6.95710930338 true: 7.02774262599\n",
      "[[2], [13], [103]] has the estimate: 7.02944192395 true: 7.00863870195\n",
      "[[2], [14], [104]] has the estimate: 5.20224635319 true: 5.19059258747\n",
      "[[2], [14], [103]] has the estimate: 93.8530869706 true: 93.9323226671\n",
      "[[4], [14], [103]] has the estimate: 18.9035991861 true: 19.0288321223\n",
      "[[4], [13], [103]] has the estimate: 30.1676012109 true: 30.1005758099\n",
      "[[6], [13], [103]] has the estimate: 24.7387513498 true: 24.8449669469\n",
      "[[6], [14], [103]] has the estimate: 42.9860357148 true: 43.0801635557\n",
      "[[4], [13], [104, 105]] non-leaf node\n",
      "[[4], [14], [104, 105]] non-leaf node\n",
      "[[6], [13], [104, 105]] non-leaf node\n",
      "[[6], [14], [104, 105]] has the estimate: 59.9808405686 !!! not seperated !!!\n",
      "[[2, 4], [12], [105]] non-leaf node\n",
      "[[6], [12], [105]] has the estimate: 59.129674797 true: 59.0130815491\n",
      "[[4, 6], [12], [103]] non-leaf node\n",
      "[[2], [12], [103]] has the estimate: 77.7502892386 true: 77.728330352\n",
      "[[2], [12], [104]] has the estimate: 44.2854142513 true: 44.1578141003\n",
      "[[6], [12], [104]] has the estimate: 54.7740174703 true: 54.7883841419\n",
      "[[2], [11], [104]] has the estimate: 41.1296791791 true: 41.0670285182\n",
      "[[6], [11], [104]] has the estimate: 41.8104124443 true: 41.9218763802\n",
      "[[4], [11], [103]] has the estimate: 24.1969041139 true: 24.1501488088\n",
      "[[4], [11], [105]] has the estimate: 59.0611140743 true: 59.0946563799\n",
      "[[6], [11], [105, 103]] non-leaf node\n",
      "[[2], [11], [105, 103]] non-leaf node\n",
      "[[3], [12], [105]] has the estimate: 37.0001023997 true: 36.9813458801\n",
      "[[3], [13], [105]] has the estimate: 46.1493565702 true: 46.1409358731\n",
      "[[5], [13], [105]] has the estimate: 71.1646185945 true: 71.0810402945\n",
      "[[5], [12], [105]] has the estimate: 87.9217435217 true: 87.9848000909\n",
      "[[5], [12, 14], [104]] non-leaf node\n",
      "[[5], [12, 14], [103]] non-leaf node\n",
      "[[3], [14], [104, 103]] non-leaf node\n",
      "[[3], [12], [104, 103]] non-leaf node\n",
      "[[5], [13], [104]] has the estimate: 84.9716076224 true: 85.0074779315\n",
      "[[5], [13], [103]] has the estimate: 87.0139843554 true: 86.9947933415\n",
      "[[3], [13], [103]] has the estimate: 91.9938506729 true: 91.9863736502\n",
      "[[3], [13], [104]] has the estimate: 95.0526307227 true: 95.1047853921\n",
      "[[4], [15], [101]] has the estimate: 9.85169110453 true: 9.92521378632\n",
      "[[4], [13], [101]] has the estimate: 23.7491239372 true: 23.7621813425\n",
      "[[4], [13], [102]] has the estimate: 79.2634465565 true: 79.1946334367\n",
      "[[4], [15], [102]] has the estimate: 91.889989617 true: 91.8897984929\n",
      "[[2], [13], [102]] has the estimate: 49.0517553728 true: 48.897101736\n",
      "[[2], [12, 15], [102]] non-leaf node\n",
      "[[6], [15], [102]] has the estimate: 12.2279131542 true: 12.125945248\n",
      "[[6], [12, 13], [102]] non-leaf node\n",
      "[[6], [15], [101]] has the estimate: 38.036638365 true: 38.0309563399\n",
      "[[6], [12, 13], [101]] non-leaf node\n",
      "[[2], [15], [101]] has the estimate: 74.879868605 true: 74.9047455794\n",
      "[[2], [12, 13], [101]] non-leaf node\n",
      "non_leaf_node count at previous depth 15\n",
      "depth 9 : 30\n",
      "[[4], [13], [104]] has the estimate: 41.0191743676 true: 41.0336623691\n",
      "[[4], [13], [105]] has the estimate: 47.1321983021 true: 47.0988170971\n",
      "[[4], [14], [104]] has the estimate: 50.8817363398 true: 50.9230383184\n",
      "[[4], [14], [105]] has the estimate: 50.9359072341 true: 50.8435185247\n",
      "[[6], [13], [104]] has the estimate: 46.9840041211 true: 46.9827360737\n",
      "[[6], [13], [105]] has the estimate: 56.1928756918 true: 56.0642163157\n",
      "[[4], [12], [105]] has the estimate: 36.9132313058 true: 36.8595052482\n",
      "[[2], [12], [105]] has the estimate: 39.95622069 true: 39.9642719849\n",
      "[[6], [12], [103]] has the estimate: 42.6930910298 true: 42.8440811623\n",
      "[[4], [12], [103]] has the estimate: 43.7289481859 true: 43.8714302394\n",
      "[[6], [11], [105]] has the estimate: 59.9337950281 true: 59.8632135748\n",
      "[[6], [11], [103]] has the estimate: 78.9563391529 true: 78.9808125976\n",
      "[[2], [11], [103]] has the estimate: 65.8225726291 true: 65.8903177614\n",
      "[[2], [11], [105]] has the estimate: 72.0910661365 true: 72.1228999896\n",
      "[[5], [14], [104]] has the estimate: 10.1170899317 true: 9.98457188408\n",
      "[[5], [12], [104]] has the estimate: 18.9997987214 true: 18.9648137117\n",
      "[[5], [12], [103]] has the estimate: 46.9690761685 true: 46.9064962066\n",
      "[[5], [14], [103]] has the estimate: 98.7873063754 true: 98.8409362129\n",
      "[[3], [14], [104]] has the estimate: 64.0972719556 true: 64.1596204077\n",
      "[[3], [14], [103]] has the estimate: 96.091581214 true: 96.0575574761\n",
      "[[3], [12], [103]] has the estimate: 83.2606065831 true: 83.1988709175\n",
      "[[3], [12], [104]] has the estimate: 87.9732015472 true: 88.0882111663\n",
      "[[2], [12], [102]] has the estimate: 64.8000901322 true: 64.9667384777\n",
      "[[2], [15], [102]] has the estimate: 69.1415440526 true: 69.1554072398\n",
      "[[6], [12], [102]] has the estimate: 69.9505213141 true: 69.9602670904\n",
      "[[6], [13], [102]] has the estimate: 75.9545497499 true: 76.0036570789\n",
      "[[6], [13], [101]] has the estimate: 69.1318265893 true: 69.1569574065\n",
      "[[6], [12], [101]] has the estimate: 99.200699415 true: 99.0771280211\n",
      "[[2], [12], [101]] has the estimate: 79.8104116614 true: 79.8463882582\n",
      "[[2], [13], [101]] has the estimate: 82.8930271639 true: 82.9312013473\n",
      "bottom depth 10 : 0\n",
      "inter-node number: 123\n",
      " *******  number of identified configurations: 121\n"
     ]
    }
   ],
   "source": [
    "# MSE result statistic\n",
    "\n",
    "mse_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "mse_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "\n",
    "# for i in range(1, maxdepth):\n",
    "#     print '-------tree with depth: ',i,'-----------'\n",
    "#     bfs_tree(mse_tree_history_esti[i], total_featureVal_set, i)\n",
    "\n",
    "print 'test error:',test_err\n",
    "print 'training error:',train_err\n",
    "print len(mse_tree_history_split)\n",
    "\n",
    "bfs_tree(mse_tree_history_split[ 0 ], total_featureVal_set, maxdepth-1,  mse_tree_history_esti[0], dta_train )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MSE result statistic : test error\n",
    "\n",
    "pre=0\n",
    "tree_esti = mse_tree_history_esti[0]\n",
    "tree_split =  mse_tree_history_split[ 0 ]\n",
    "\n",
    "for i in range(1,maxdepth+1):\n",
    "            \n",
    "    end_bound= pre+ (int)( math.pow(2,i)) \n",
    "    test_leaf_nodes = tree_esti[ pre: end_bound  ]\n",
    "    pre= end_bound        \n",
    "    test_tree_split= tree_split[ 0:(int)( math.pow(2,i)-1)  ]\n",
    "    \n",
    "#     print len(test_tree_split)\n",
    "#     print len(test_leaf_nodes)\n",
    "\n",
    "    test_error_tree( i, dta_test  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TMSE result backup\n",
    "\n",
    "tmse_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "tmse_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "tmse_tree_history_leaf =  copy.deepcopy( tree_history_leaf)\n",
    "\n",
    "print len(tmse_tree_history_split)\n",
    "print len(tmse_tree_history_split[0])\n",
    "print len(tmse_tree_history_esti)\n",
    "print len(tmse_tree_history_esti[0])\n",
    "\n",
    "print len(tmse_tree_history_leaf)\n",
    "print len(tmse_tree_history_leaf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error: [1071395.612185999, 1070774.9126108503, 1070774.9126108503, 1070774.9126108503, 1070774.9126108503, 1070774.9126108503]\n",
      "training error: [1032797.2430061261, 1032368.6423613247, 1032368.6423613247, 1032368.6423613247, 1032368.6423613247, 1032368.6423613247]\n",
      "6\n",
      "depth 0 : 1\n",
      "[[3, 6, 4, 2, 5], [12, 15, 13, 11, 14], [105, 102, 103, 104, 101]]\n",
      "depth 1 : 2\n",
      "[[3, 6, 4, 2, 5], [12, 15, 13, 11, 14], [103]]\n",
      "[[3, 6, 4, 2, 5], [12, 15, 13, 11, 14], [104, 105, 101, 102]]\n",
      "depth 2 : 0\n",
      "depth 3 : 0\n",
      "depth 4 : 0\n",
      "depth 5 : 0\n",
      "depth 6 : 0\n",
      "inter-node number: 2\n"
     ]
    }
   ],
   "source": [
    "# TMSE result statistic\n",
    "\n",
    "tmse_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "tmse_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "\n",
    "# for i in range(1, maxdepth):\n",
    "#     print '-------tree with depth: ',i,'-----------'\n",
    "#     bfs_tree(mse_tree_history_esti[i], total_featureVal_set, i)\n",
    "\n",
    "print 'test error:',test_err\n",
    "print 'training error:',train_err\n",
    "print len(tmse_tree_history_split)\n",
    "\n",
    "bfs_tree(tmse_tree_history_split[ maxdepth-1 ], total_featureVal_set, maxdepth-1)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tmse_tree_history_esti' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-7ced8910d6a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpre\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtree_esti\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmse_tree_history_esti\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtree_split\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mtmse_tree_history_split\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tmse_tree_history_esti' is not defined"
     ]
    }
   ],
   "source": [
    "# TMSE result statistic : test error\n",
    "\n",
    "pre=0\n",
    "tree_esti = tmse_tree_history_esti[0]\n",
    "tree_split =  tmse_tree_history_split[ 0 ]\n",
    "\n",
    "test_error_depth=[]\n",
    "\n",
    "for i in range(1,maxdepth+1):\n",
    "            \n",
    "    end_bound= pre+ (int)( math.pow(2,i)) \n",
    "    test_leaf_nodes = tree_esti[ pre: end_bound  ]\n",
    "    pre= end_bound        \n",
    "    test_tree_split= tree_split[ 0:(int)( math.pow(2,i)-1)  ]\n",
    "    \n",
    "#     print len(test_tree_split)\n",
    "#     print len(test_leaf_nodes)\n",
    "\n",
    "    test_error_tree( i, dta_test  )\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data inspection for the nodes of the trained tree\n",
    "\n",
    "def feature_value_extract( valueSet):\n",
    "    \n",
    "    tmpval=1\n",
    "    tmp_valSet= valueSet\n",
    "    valSet_list=[]\n",
    "    cnt=0\n",
    "    \n",
    "    while tmpval <= tmp_valSet:\n",
    "        if tmpval & tmp_valSet !=0:\n",
    "            valSet_list.append(cnt)\n",
    "        tmpval=tmpval<<1\n",
    "        cnt=cnt+1\n",
    "        \n",
    "    return valSet_list\n",
    "\n",
    "def dfs_tree(tree, current_nodeIdx, current_featureVal_set, current_depth):\n",
    "    \n",
    "    nodeNum=len(tree)   \n",
    "    if nodeNum == 0:\n",
    "        return\n",
    "    \n",
    "    split_feature= tree[current_nodeIdx][0]\n",
    "    split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "    if split_feature==-1:\n",
    "        print current_featureVal_set\n",
    "        return;\n",
    "    \n",
    "    featureValue_list = feature_value_extract( split_valueSet)\n",
    "    \n",
    "    left_featureValue_set=[]\n",
    "    right_featureValue_set=[]\n",
    "    featureNum= len(current_featureVal_set)\n",
    "    \n",
    "    for i in range(0, featureNum):\n",
    "        if i != split_feature:\n",
    "            left_featureValue_set.append(  copy.deepcopy(current_featureVal_set[i])   )\n",
    "            right_featureValue_set.append(  copy.deepcopy(current_featureVal_set[i])   )\n",
    "        else:\n",
    "            left_featureValue_set.append(  copy.deepcopy(featureValue_list )   )\n",
    "            right_featureValue_set.append(copy.deepcopy(list(set(current_featureVal_set[i] )-set(featureValue_list)))   )\n",
    "            \n",
    "        \n",
    "    traverse_tree(tree, current_nodeIdx*2+1,left_featureValue_set,current_depth+1)\n",
    "    traverse_tree(tree, current_nodeIdx*2+2,right_featureValue_set, current_depth+1)\n",
    "    \n",
    "    return \n",
    "\n",
    "def training_data_summary( train_rdd ):\n",
    "\n",
    "#     always remove outliers before\n",
    "    train_rdd_clean= train_rdd.filter(lambda line: line[0]<1000)\n",
    "    distinct_featureVals = train_rdd_clean.map(lambda line: (line[1], line[2],line[3]) ).distinct().collect()\n",
    "   \n",
    "    featureConfig_val={}\n",
    "    for i in distinct_featureVals:\n",
    "        \n",
    "        tmprdd=train_rdd_clean.filter(lambda line:(line[1], line[2],line[3]) ==i ).map(lambda line: line[0])\n",
    "        tmpval=tmprdd.reduce(lambda a,b:a+b)\n",
    "        tmpval= tmpval/tmprdd.count()*1.0\n",
    "        \n",
    "        featureConfig_val.update( {i:tmpval} )\n",
    "    \n",
    "#     print featureConfig_val\n",
    "    return featureConfig_val\n",
    "        \n",
    "\n",
    "def config_check( featureVal_set, feature_num):\n",
    "    \n",
    "    for i in range(0, feature_num):\n",
    "        if len(featureVal_set[i]) >1 :\n",
    "            return -1\n",
    "    return 1\n",
    "\n",
    "def lookup_featureConfig_val( configToVal_dict, value_set, feature_num):\n",
    "    \n",
    "    tmptuple=()\n",
    "    for i in range(0, feature_num):\n",
    "        tmptuple = tmptuple + ( value_set[i][0]  ,)\n",
    "#     print '++++++++++++', tmptuple\n",
    "    return configToVal_dict[tmptuple]\n",
    "    \n",
    "def test_error_mapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], test_tree_split)\n",
    "    return (test_leaf_nodes[ tmpnode ]  - line[0])*(test_leaf_nodes[ tmpnode ]  - line[0])\n",
    "    \n",
    "def test_error_tree( current_depth, testRDD  ):\n",
    "    \n",
    "    test_cnt= testRDD.count()\n",
    "    err_rdd=testRDD.map( lambda line: test_error_mapFunc(line))\n",
    "    err_sum = err_rdd.reduce( lambda a,b: a+b)\n",
    "    \n",
    "    print 'test error at depth',current_depth, ':', err_sum*1.0/test_cnt\n",
    "    return err_sum*1.0/test_cnt\n",
    "    \n",
    "def bfs_tree(tree, total_featureVal_set, depth, tree_esti, trainRDD ):\n",
    "    \n",
    "    nodeNum=len(tree)   \n",
    "    if nodeNum == 0:\n",
    "        return\n",
    "    \n",
    "    feature_ConfigToVal = training_data_summary( trainRDD )\n",
    "    \n",
    "    interNode_cnt=0\n",
    "    non_leaf_cnt=0\n",
    "    recog_config_cnt=0\n",
    "    \n",
    "    qu=[]\n",
    "    current_depth=0\n",
    "    qu.append((0,total_featureVal_set,0))\n",
    "    \n",
    "    \n",
    "    recog_config_cnt_depth=[]\n",
    "    recog_conf=[]\n",
    "    \n",
    "    while current_depth<= depth:\n",
    "        \n",
    "        print 'non_leaf_node count at previous depth', non_leaf_cnt\n",
    "        \n",
    "        print 'depth', current_depth,':', len(qu)\n",
    "        tmpqu=[]\n",
    "        \n",
    "        non_leaf_cnt=0\n",
    "        \n",
    "        for i in qu:\n",
    "            current_nodeIdx= i[0] \n",
    "            current_featureVal_set= i[1]\n",
    "            \n",
    "            split_feature= tree[current_nodeIdx][0]\n",
    "            split_valueSet= tree[current_nodeIdx][1]\n",
    "                 \n",
    "            if split_feature==-1: \n",
    "                \n",
    "                if config_check( current_featureVal_set, featureNum) ==1:\n",
    "                    recog_config_cnt= recog_config_cnt+1\n",
    "                    tmpval=lookup_featureConfig_val( feature_ConfigToVal, current_featureVal_set, featureNum)\n",
    "                    \n",
    "                    print current_featureVal_set, 'has the estimate:', tree_esti[ current_nodeIdx*2+1],\\\n",
    "                    'true:',tmpval\n",
    "                    #result\n",
    "                    recog_conf.append( (tree_esti[ current_nodeIdx*2+1], tmpval) )\n",
    "                    \n",
    "                else:\n",
    "                    print current_featureVal_set, 'has the estimate:', tree_esti[ current_nodeIdx*2+1],'!!! not seperated !!!'\n",
    "                \n",
    "                interNode_cnt=interNode_cnt+1\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                non_leaf_cnt=non_leaf_cnt+1\n",
    "                print current_featureVal_set,'non-leaf node'\n",
    "                \n",
    "                \n",
    "            featureValue_list = feature_value_extract( split_valueSet)\n",
    "    \n",
    "            left_featureValue_set=[]\n",
    "            right_featureValue_set=[]\n",
    "            featureNum= len(current_featureVal_set)\n",
    "    \n",
    "            for i in range(0, featureNum):\n",
    "                if i != split_feature:\n",
    "                    left_featureValue_set.append(  copy.deepcopy(current_featureVal_set[i])   )\n",
    "                    right_featureValue_set.append(  copy.deepcopy(current_featureVal_set[i])   )\n",
    "                else:\n",
    "                    left_featureValue_set.append(  copy.deepcopy(featureValue_list )   )\n",
    "                    right_featureValue_set.append(copy.deepcopy(list(set(current_featureVal_set[i] )-set(featureValue_list)))   )\n",
    "            \n",
    "            tmpqu.append(  (current_nodeIdx*2+1,left_featureValue_set, current_nodeIdx)  )\n",
    "            tmpqu.append(  (current_nodeIdx*2+2,right_featureValue_set,current_nodeIdx) )\n",
    "        \n",
    "        qu=copy.deepcopy(tmpqu)\n",
    "        current_depth= current_depth+1\n",
    "        \n",
    "        #result\n",
    "        recog_config_cnt_depth.append( recog_config_cnt )\n",
    "    \n",
    "    \n",
    "# print out leaf nodes\n",
    "    print 'bottom depth', current_depth,':', len(qu)\n",
    "    \n",
    "    for i in qu:\n",
    "        current_nodeIdx= i[0] \n",
    "        current_featureVal_set= i[1]\n",
    "        parent_nodeIdx= i[2]\n",
    "        \n",
    "        if 2*parent_nodeIdx+1 == current_nodeIdx:\n",
    "            tree_esti_val= tree_esti[ parent_nodeIdx*2]\n",
    "        else:\n",
    "            tree_esti_val= tree_esti[ parent_nodeIdx*2+1] \n",
    "        \n",
    "        if config_check( current_featureVal_set, featureNum) ==1:\n",
    "            recog_config_cnt= recog_config_cnt+1\n",
    "            tmpval=lookup_featureConfig_val( feature_ConfigToVal, current_featureVal_set, featureNum)\n",
    "            \n",
    "            \n",
    "            if current_featureVal_set == [[6], [14], [105]]:\n",
    "                print '!!!  debug !!!:', parent_nodeIdx\n",
    "            \n",
    "            \n",
    "            \n",
    "            #result\n",
    "            recog_conf.append( (tree_esti_val, tmpval) )\n",
    "            \n",
    "            print current_featureVal_set, 'has the estimate:', tree_esti_val,\\\n",
    "                    'true:',tmpval\n",
    "        else:\n",
    "            print current_featureVal_set, 'has the estimate:', tree_esti_val\n",
    "    \n",
    "    #result\n",
    "    recog_config_cnt_depth.append( recog_config_cnt )\n",
    "        \n",
    "    print 'inter-node number:', interNode_cnt\n",
    "    print ' *******  number of identified configurations:', recog_config_cnt\n",
    "    \n",
    "    tmpval=0\n",
    "    for i in range(0,recog_config_cnt):\n",
    "        tmpval=tmpval+  (recog_conf[i][0]-recog_conf[i][1])*(recog_conf[i][0]-recog_conf[i][1])\n",
    "    \n",
    "    return (recog_conf, recog_config_cnt_depth, tmpval*1.0/recog_config_cnt)\n",
    "\n",
    "# len( tree_history_split)\n",
    "# len(tree_history_esti)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 6, 4, 2, 5], [12, 15, 13, 11, 14], [105, 102, 103, 104, 101]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_featureVal_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# singal-run training  process  \n",
    "dta_train = dta.sample(False, .7, 12345)\n",
    "dta_test = dta.sample(False, .3, 43243)\n",
    "dta_test = dta_test.filter(lambda line: line[0] <1000 )\n",
    "\n",
    "#parameters\n",
    "maxdepth=\n",
    "numFeatures=3\n",
    "\n",
    "node_split=[]\n",
    "node_test=[]\n",
    "#ini parameter\n",
    "currentNode_split_fromMaster = sc.broadcast([])\n",
    "\n",
    "#prepare feature value set\n",
    "feature_valueSet = data_featureValues_collect( dta_train )\n",
    "feature_valueList=[]\n",
    "for i in range(0, len(feature_valueSet)):\n",
    "    feature_valueList.append( ( i, list(feature_valueSet[i][1]) ) )\n",
    "    \n",
    "\n",
    "for i in range(0,maxdepth):\n",
    "    \n",
    "    start = time.time()\n",
    "    current_nodeNum= int( math.pow(2,  i))\n",
    "    \n",
    "    print 'current split decision:',currentNode_split_fromMaster.value\n",
    "\n",
    "    dataToNode = dataToNode_assignment( dta_train )\n",
    "    cluster_end = time.time() \n",
    "    find_bestSplit_exact(dataToNode,current_nodeNum,node_split,node_test, feature_valueList)   \n",
    "#   ( dataToNode, current_NumNodes, nodes_tree, nodes_tree_test, feature_valueSet):\n",
    "    currentNode_split_fromMaster = sc.broadcast(node_split)\n",
    "  \n",
    "    end = time.time() \n",
    "\n",
    "    print i,'-th level running time: ', cluster_end - start,'sec', end- cluster_end, 'sec'\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69015"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#select one feature-value\n",
    "feature_id=1\n",
    "feature_val=11\n",
    "\n",
    "dataToFeatureValue_rdd=node_data.filter(lambda line:line[1][feature_id+1]==feature_val).map(lambda line:line[1][0])\n",
    "\n",
    "dataToFeatureValue_rdd_count = dataToFeatureValue_rdd.count()\n",
    "        \n",
    "# med_tmpLarge= dataToFeatureValue_rdd.top( dataToFeatureValue_rdd_count/2+1  )\n",
    "med_tmp = dataToFeatureValue_rdd.top( dataToFeatureValue_rdd_count/2  )\n",
    "med_tmp.sort()\n",
    "tmpcnt= len(med_tmp)\n",
    "print med_tmp[ tmpcnt-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437547\n"
     ]
    }
   ],
   "source": [
    "#select several feature-values\n",
    "feature_values=[1,2,3]\n",
    "\n",
    "dataToFeatureValue_rdd=node_data.filter(lambda line:line[1][feature_id+1] in feature_values).map(lambda line:line[1][0])\n",
    "\n",
    "dataToFeatureValue_rdd_count = dataToFeatureValue_rdd.count()\n",
    "\n",
    "print dataToFeatureValue_rdd_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
