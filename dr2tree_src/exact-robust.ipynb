{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os \n",
    "import sys \n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "from datetime  import datetime \n",
    "\n",
    "import numpy as np  # learn \n",
    "import pandas as pd # learn\n",
    "from pandas import *\n",
    "from numpy import *\n",
    "\n",
    "from scipy import stats # look at scipy\n",
    "from scipy import linalg\n",
    "from scipy import *\n",
    "\n",
    " \n",
    "import matplotlib as mplt # learn matplolib \n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "import seaborn as sns \n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc={\"figure.figsize\": (14, 6)})\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import sklearn as sk\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import random\n",
    "import time\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328.0, 10, 10, 2, 2, 8, 1, 3)\n",
      "3315067\n"
     ]
    }
   ],
   "source": [
    "#BT data\n",
    "# dta_RDD = sc.textFile(\"file:///home/tguo/data/tian-test/udpjitter-H1april2014_regTree.csv\")\n",
    "\n",
    "dta_RDD = sc.textFile(\"hdfs://computer61.ant-net/user/tguo/udpjitter-H1april2014_regTree_mllib.csv\")\n",
    "\n",
    "#data format: dependent variable, feature values\n",
    "dta_splited = dta_RDD.map(lambda line: line.split(\",\")).map(lambda r:\n",
    "                                                                (float(r[0]),int( float(r[1])), \n",
    "                                                                 int(float(r[2])),int(float(r[3])),\n",
    "                                                                  int(float(r[4])),int(float(r[5])),\n",
    "                                                                 int(float(r[6])),int(float(r[7])) ))\n",
    "\n",
    "\n",
    "# dta_splited.first()\n",
    "dta= dta_splited\n",
    "dta.cache()\n",
    "print dta.first()\n",
    "print dta.count()\n",
    "\n",
    "# dta_filtered = dta.filter(lambda line: line[0] <2000 )\n",
    "# dta_filtered.cache()\n",
    "# dta_filtered.count()\n",
    "# print dta.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: [(78.676903536, 5, 13, 101, 1005)]\n",
      "2000000\n"
     ]
    }
   ],
   "source": [
    "# synthetic data\n",
    "dta_RDD = sc.textFile(\"file:///home/tguo/tian_src/synData_7f_6zeros_0.01percen.txt\")\n",
    "# synthetic_data_6zeros_0.01percen.txt\n",
    "\n",
    "dta_RDD.cache()\n",
    "\n",
    "dta_splited = dta_RDD.map(lambda line: line.split(\",\")).map(lambda r: ( float(r[7]),int(r[0]),int(r[1]),int(r[2])\\\n",
    "                                                                       ,int(r[3]) )\\\n",
    "                                                           ) \n",
    "# ( float(r[2]),float(r[0]),float(r[1]) ) \n",
    "\n",
    "dta= dta_splited\n",
    "dta.cache()\n",
    "\n",
    "print 'original:',dta.take(1)\n",
    "print dta.count()\n",
    "\n",
    "# re-set index of categorical features\n",
    "\n",
    "# feature_dist=[]\n",
    "\n",
    "# tmp1 = dta.map(lambda line: line[1]).distinct().collect()\n",
    "# feature_dist.append(tmp1)\n",
    "\n",
    "# tmp1 = dta.map(lambda line: line[2]).distinct().collect()\n",
    "# feature_dist.append(tmp1)\n",
    "\n",
    "# tmp1 = dta.map(lambda line: line[3]).distinct().collect()\n",
    "# feature_dist.append(tmp1)\n",
    "\n",
    "# fea_cnt= len(feature_dist )\n",
    "# fea_map=[]\n",
    "\n",
    "# for i in range(0, fea_cnt):\n",
    "#     tmpcnt = len(feature_dist[i])\n",
    "#     val_map= dict( zip(feature_dist[i], range(0,tmpcnt)) )\n",
    "#     fea_map.append(val_map)\n",
    "\n",
    "# def reset_index( line ):\n",
    "#     tmp=[]\n",
    "#     tmp.append(line[0])\n",
    "#     for i in range(1,4):\n",
    "#         tmp.append(fea_map[i-1][ line[i] ] )\n",
    "#     return tmp\n",
    "\n",
    "# dta = dta.map( lambda line: reset_index(line)  )\n",
    "\n",
    "# print 'feature value re-indexed:',dta.first()\n",
    "# print dta.count()\n",
    "\n",
    "\n",
    "\n",
    "# total_featureVal_set=[]\n",
    "# for i in range(0, numFeatures):\n",
    "#     featureValues = dta_train.map(lambda line: line[1+i]).distinct().collect()\n",
    "#     total_featureVal_set.append( featureValues)\n",
    "\n",
    "# print total_featureVal_set    \n",
    "    \n",
    "# print 'done'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# basic functions in one-pass method [on the cluster side]\n",
    "\n",
    "# assign each data instance to the node\n",
    "def search_nodeToData(features, tree):\n",
    "    nodeNum=len(tree)   \n",
    "    if nodeNum == 0:\n",
    "        return 0;\n",
    "    \n",
    "    current_nodeIdx=0\n",
    "    while current_nodeIdx< nodeNum:\n",
    "        split_feature= tree[current_nodeIdx][0]\n",
    "        split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "        # unbalanced tree grow\n",
    "        if split_feature==-1:\n",
    "            current_nodeIdx=current_nodeIdx*2+1+ random.randint(0, 1)\n",
    "            continue\n",
    "            \n",
    "        if (( 1<<features[split_feature]) &  split_valueSet) != 0 :\n",
    "            current_nodeIdx=current_nodeIdx*2+1\n",
    "        else:\n",
    "            current_nodeIdx=current_nodeIdx*2+2\n",
    "            \n",
    "    return current_nodeIdx - nodeNum\n",
    "\n",
    "def partition_dataToNode(list_dvAndfeatures):\n",
    "    dataToNode_map=[]\n",
    "    res=[]\n",
    "    for dvAndfeatures in list_dvAndfeatures:\n",
    "        \n",
    "        Y= dvAndfeatures[0]\n",
    "        node= search_nodeToData(dvAndfeatures[1:numFeatures+1], \\\n",
    "                                currentNode_split_fromMaster.value)\n",
    "        dataToNode_map.append(node)\n",
    "        res.append( (node, dvAndfeatures )   )\n",
    "    return res\n",
    "def dataToNode_assignment( data_rdd ):\n",
    "    dataToNode_map = data_rdd.mapPartitions( partition_dataToNode )\n",
    "    dataToNode_map.cache()\n",
    "    return dataToNode_map\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "# extract values for each feature\n",
    "def partition_featureValues(list_dvAndfeatures):\n",
    "\n",
    "    feature_valueSet={}\n",
    "    \n",
    "    for i in range(0,numFeatures):\n",
    "        feature_valueSet.update( {i: set()} )\n",
    "        \n",
    "    for dvAndfeatures in list_dvAndfeatures:\n",
    "        for i in range(0,numFeatures):\n",
    "            feature_val= dvAndfeatures[i+1]\n",
    "            feature_valueSet[i].add(feature_val)\n",
    "    return zip( feature_valueSet.keys(), feature_valueSet.values() )  \n",
    "\n",
    "def merge_featureValues(  valueSet1, valueSet2):\n",
    "    return valueSet1.union(valueSet2)\n",
    "    \n",
    "def data_featureValues_collect( data_rdd ):\n",
    "    feature_valueSet_part = data_rdd.mapPartitions( partition_featureValues )\n",
    "    feature_valueSet_local = \\\n",
    "    feature_valueSet_part.reduceByKey(lambda set1, set2: merge_featureValues(set1,set2 )).collect()\n",
    "    feature_valueSet_local.sort()\n",
    "    #test\n",
    "#     print feature_valueSet_local\n",
    "    return feature_valueSet_local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split decision [on the local side]\n",
    "\n",
    "#implementation notes:\n",
    "# bit-operation to record the set of selected feature values in the left child\n",
    "\n",
    "#asecending order, upper: larger, lower: smaller\n",
    "def upper_tail_boundary( percentage, dat_rdd, tt_cnt):\n",
    "    tmpcnt= int(tt_cnt*percentage) \n",
    "    tmplist = dat_rdd.top( int(tt_cnt*percentage)  )\n",
    "    \n",
    "    \n",
    "    return tmplist[tmpcnt-1]\n",
    "\n",
    "def lower_tail_boundary( percentage, dat_rdd, tt_cnt):\n",
    "#     print tt_cnt,percentage, type(dat_rdd)\n",
    "    sorted_rdd = dat_rdd.sortBy(lambda line:line, ascending= True).cache()\n",
    "    tmpcnt= int(tt_cnt*percentage) \n",
    "    tmplist= dat_rdd.take( tmpcnt )\n",
    "    return tmplist[ tmpcnt-1 ]\n",
    "    \n",
    "# def trimmed_MSE_cal(dat_rdd, dat_rdd_cnt, trim_percentage ):\n",
    "    \n",
    "#     upper_bound = upper_tail_boundary( trim_percentage, dat_rdd, dat_rdd_cnt)\n",
    "# #     lower_bound = lower_tail_boundary( trim_percentage, dat_rdd, dat_rdd_cnt)\n",
    "    \n",
    "#     trimmed_rdd= dat_rdd.filter( lambda line: line < upper_bound ).cache()\n",
    "# #     trimmed_rdd= trimmed_rdd.filter(lambda line: line > lower_bound).cache()\n",
    "#     trimmed_rdd_cnt = trimmed_rdd.count()\n",
    "    \n",
    "#     if trimmed_rdd_cnt ==0:\n",
    "        \n",
    "#         print 'trimmed zeor happens!'\n",
    "#         tmp_mean=0\n",
    "#         tmp_mse=0\n",
    "        \n",
    "#     else:\n",
    "#         tmp_mean = trimmed_rdd.reduce(lambda a,b:a+b) / trimmed_rdd_cnt\n",
    "#         tmp_mse= trimmed_rdd.map(lambda line:(line-tmp_mean)*(line-tmp_mean)).reduce(lambda a,b:a+b)/trimmed_rdd_cnt\n",
    "    \n",
    "#     return (trimmed_rdd_cnt, tmp_mean, tmp_mse, trimmed_rdd)\n",
    "\n",
    "# def trimmed_MSE( dataToSplit_rdd, feature_id, value_set ):\n",
    "    \n",
    "#     tmp_rdd=dataToSplit_rdd.filter(lambda l:l[1][feature_id+1] in value_set).map(lambda l:l[1][0]).cache()\n",
    "#     tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "#     if tmp_rdd_count ==0:\n",
    "        \n",
    "#         print 'trimmed zeor happens!'\n",
    "#         tmp_mean=0\n",
    "#         tmp_mse=0\n",
    "#     else:\n",
    "#         tmp_mean = tmp_rdd.reduce(lambda a,b:a+b)*1.0 / tmp_rdd_count\n",
    "#         tmp_mse= tmp_rdd.map(lambda line:(line-tmp_mean)*(line-tmp_mean)).reduce(lambda a,b:a+b)*1.0/tmp_rdd_count\n",
    "    \n",
    "#     return (tmp_rdd_count, tmp_mean, tmp_mse )\n",
    "    \n",
    "# def trimmed_MSE_node( dataToSplit_rdd, trim_percentage ):\n",
    "    \n",
    "#     tmp_rdd=dataToSplit_rdd.map(lambda l:l[1][0]).cache()\n",
    "#     tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "#     upper_bound = upper_tail_boundary( trim_percentage, tmp_rdd, tmp_rdd_count)\n",
    "# #     lower_bound = lower_tail_boundary( trim_percentage, dat_rdd, dat_rdd_cnt)\n",
    "#     tmp_trim_rdd= tmp_rdd.filter( lambda line: line < upper_bound ).cache()\n",
    " \n",
    "#     trimmed_rdd= dataToSplit_rdd.filter( lambda line: line[1][0] < upper_bound ).cache()\n",
    "# #     trimmed_rdd= trimmed_rdd.filter(lambda line: line > lower_bound).cache()\n",
    "#     trimmed_rdd_cnt = trimmed_rdd.count()\n",
    "    \n",
    "#     if trimmed_rdd_cnt ==0:\n",
    "        \n",
    "#         print 'trimmed zeor happens!'\n",
    "#         tmp_mean=0\n",
    "#         tmp_mse=0\n",
    "        \n",
    "#     else:\n",
    "#         tmp_mean = tmp_trim_rdd.reduce(lambda a,b:a+b) / trimmed_rdd_cnt\n",
    "#         tmp_mse= tmp_trim_rdd.map(lambda line:(line-tmp_mean)*(line-tmp_mean)).reduce(lambda a,b:a+b)/trimmed_rdd_cnt\n",
    "    \n",
    "#     return (trimmed_rdd_cnt, tmp_mean, tmp_mse, trimmed_rdd)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def median_LAD( dataToSplit_rdd, feature_id, value_set ):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.filter(lambda l:l[1][feature_id+1] in value_set).map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "    if tmp_rdd_count==0:\n",
    "        return (10000, 100000, 10000000)\n",
    "    \n",
    "#----------median computation optimization---------------------------------\n",
    "    sorted_rdd = tmp_rdd.sortBy(lambda line: line).zipWithIndex().map(lambda line: (line[1], line[0]) )\n",
    "    if(tmp_rdd_count%2 ==0):\n",
    "        tmp1=sorted_rdd.lookup( tmp_rdd_count/2-1  )[0]\n",
    "        tmp2=sorted_rdd.lookup( tmp_rdd_count/2  )[0]\n",
    "        tmp_median = (tmp1 + tmp1)*1.0/2.0\n",
    "    else:\n",
    "        tmp_median = sorted_rdd.lookup( tmp_rdd_count/2  )[0]\n",
    "#-------------------------------------------------------------\n",
    "    \n",
    "#     if(tmp_rdd_count%2 ==0):\n",
    "#         tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "#         tmphalf.sort()\n",
    "#         tmpcnt= len(tmphalf)\n",
    "#         tmp_median = (tmphalf[0] + tmphalf[1])*1.0/2.0\n",
    "#     else:\n",
    "#         tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "#         tmphalf.sort()\n",
    "#         tmpcnt= len(tmphalf)\n",
    "#         tmp_median = tmphalf[0]\n",
    "    \n",
    "    tmpMAD_rdd = tmp_rdd.map(lambda line: abs(line-tmp_median) ).cache()\n",
    "    tmpMAD = tmpMAD_rdd.reduce( lambda a,b: a+b)*1.0 / tmp_rdd_count\n",
    "    \n",
    "    return (tmp_rdd_count, tmp_median, tmpMAD)\n",
    "\n",
    "def median_LAD_node( dataToSplit_rdd):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = dataToSplit_rdd.count()\n",
    " \n",
    "    if tmp_rdd_count==0:\n",
    "        return (10000, 100000, 10000000)\n",
    "    \n",
    "    #----------median computation optimization---------------------------------\n",
    "    sorted_rdd = tmp_rdd.sortBy(lambda line: line).zipWithIndex().map(lambda line: (line[1], line[0]) )\n",
    "    if(tmp_rdd_count%2 ==0):\n",
    "        tmp1=sorted_rdd.lookup( tmp_rdd_count/2-1  )[0]\n",
    "        tmp2=sorted_rdd.lookup( tmp_rdd_count/2  )[0]\n",
    "        tmp_median = (tmp1 + tmp1)*1.0/2.0\n",
    "    else:\n",
    "        tmp_median = sorted_rdd.lookup( tmp_rdd_count/2  )[0]\n",
    "    #-------------------------------------------------------------\n",
    "    \n",
    "#     if(tmp_rdd_count%2 ==0):\n",
    "#         tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "#         tmphalf.sort()\n",
    "#         tmpcnt= len(tmphalf)\n",
    "#         tmp_median = (tmphalf[0] + tmphalf[1])*1.0/2.0\n",
    "#     else:\n",
    "#         tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "#         tmphalf.sort()\n",
    "#         tmpcnt= len(tmphalf)\n",
    "#         tmp_median = tmphalf[0]\n",
    "    \n",
    "    tmpMAD_rdd = tmp_rdd.map(lambda line: abs(line-tmp_median) ).cache()\n",
    "    tmpMAD = tmpMAD_rdd.reduce( lambda a,b: a+b)*1.0 / tmp_rdd_count\n",
    "    \n",
    "    return (tmp_rdd_count, tmp_median, tmpMAD)\n",
    "\n",
    "def split_onOneFeature_exact_LAD(node_data, node_data_cnt, LAD_node_ini, feature_valueList, feature_id):\n",
    "    \n",
    "    left_count=0\n",
    "    right_count= node_data_cnt\n",
    "    bestSplitMetric=  LAD_node_ini \n",
    "    \n",
    "    #------check the number of feature values------------------\n",
    "    if len(feature_valueList) <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    #-----------------------\n",
    "\n",
    "    #this is a bit-record variable\n",
    "    leftSplit_valueSet =0 \n",
    "    leftSplit_count=0 \n",
    "    rightSplit_count=0\n",
    "    \n",
    "    bestLeft=(-1,-1)\n",
    "    bestRight=(-1,-1)\n",
    "    \n",
    "    #sort feature values according to median \n",
    "    sorted_value_map=[]\n",
    "    tmpvalues=[]\n",
    "    \n",
    "    for i in feature_valueList:\n",
    "#         dataToFeatureValue_rdd=node_data.filter(lambda l:l[1][feature_id+1]==currentVal).map(lambda l:l[1][0]).cache()\n",
    "#         dataToFeatureValue_rdd_count = dataToFeatureValue_rdd.count()\n",
    "\n",
    "#-----------------------------------        \n",
    "        tmp_rdd=node_data.filter(lambda l:l[1][feature_id+1] == i ).map(lambda l:l[1][0]).cache()  \n",
    "        tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "        if tmp_rdd_count==0:\n",
    "            continue\n",
    "        else:\n",
    "            sorted_rdd = tmp_rdd.sortBy(lambda line: line).zipWithIndex().map(lambda line: (line[1], line[0]) )\n",
    "            if(tmp_rdd_count%2 ==0):\n",
    "                tmp1=sorted_rdd.lookup( tmp_rdd_count/2-1  )[0]\n",
    "                tmp2=sorted_rdd.lookup( tmp_rdd_count/2  )[0]\n",
    "                tmp_median = (tmp1 + tmp1)*1.0/2.0\n",
    "            else:\n",
    "                tmp_median = sorted_rdd.lookup( tmp_rdd_count/2  )[0]\n",
    "    \n",
    "#             tmpMAD_rdd = tmp_rdd.map(lambda line: abs(line-tmp_median) ).cache()\n",
    "#             tmpMAD = tmpMAD_rdd.reduce( lambda a,b: a+b)\n",
    "       \n",
    "        tmp_mad=(tmp_rdd_count, tmp_median, 0)\n",
    "        \n",
    "#         del tmpvalues[:]\n",
    "#         tmpvalues.append(i)\n",
    "#         tmp_mad = median_LAD( node_data, i, tmpvalues )\n",
    "        \n",
    "# ------------------------------------\n",
    "        sorted_value_map.append( (tmp_mad[1],i) )\n",
    "        \n",
    "    sorted_value_map.sort()\n",
    "    len_values= len(sorted_value_map)\n",
    "\n",
    "    # scan the split positions    \n",
    "    left_value_set=[]\n",
    "    right_value_set= copy.deepcopy(feature_valueList)\n",
    "    \n",
    "    #--------- initialize the value set ----------------------\n",
    "    \n",
    "    # check the number of feature values\n",
    "    if len_values <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    \n",
    "    current_feature_value = sorted_value_map[0][1] \n",
    "\n",
    "    left_value_set.append(current_feature_value)\n",
    "    right_value_set.remove(current_feature_value)\n",
    "        \n",
    "    left = median_LAD( node_data, feature_id, left_value_set )\n",
    "    right= median_LAD( node_data, feature_id, right_value_set )\n",
    "        \n",
    "    leftSplit_count = left[0]\n",
    "    leftMedian= left[1]\n",
    "    leftMetric= left[2]\n",
    "        \n",
    "    rightSplit_count= right[0]\n",
    "    rightMedian= right[1]\n",
    "    rightMetric= right[2]  \n",
    "        \n",
    "    current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "    bestSplitMetric=current_splitMetric     \n",
    "    bestLeft=left\n",
    "    bestRight=right\n",
    "    #bit variable to record which value is chosen for the left child\n",
    "    leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "    \n",
    "    #--------------------------------------------------------------\n",
    "    \n",
    "    for k in range(1,len_values-1):\n",
    "        \n",
    "        current_feature_value = sorted_value_map[k][1] \n",
    "        \n",
    "        left_value_set.append(current_feature_value)\n",
    "        right_value_set.remove(current_feature_value)\n",
    "        \n",
    "        left = median_LAD( node_data, feature_id, left_value_set )\n",
    "        right= median_LAD( node_data, feature_id, right_value_set )\n",
    "        \n",
    "        leftSplit_count = left[0]\n",
    "        leftMedian= left[1]\n",
    "        leftMetric= left[2]\n",
    "        \n",
    "        rightSplit_count= right[0]\n",
    "        rightMedian= right[1]\n",
    "        rightMetric= right[2]  \n",
    "        \n",
    "        current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "        if current_splitMetric < bestSplitMetric:\n",
    "            bestSplitMetric=current_splitMetric     \n",
    "            bestLeft=left\n",
    "            bestRight=right\n",
    "            #bit variable to record which value is chosen for the left child\n",
    "            leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "\n",
    "    return [ leftSplit_valueSet, bestLeft[1], bestRight[1], bestSplitMetric  ] \n",
    "#          value_set, left median, right median, weighted mad\n",
    "\n",
    "def find_bestSplit_exact( dataToNode, current_NumNodes, nodes_tree, nodes_tree_test, feature_valueList, loss_func):\n",
    "    \n",
    "    tmpnode_cnt=[]\n",
    "    presplit=[]\n",
    "        \n",
    "#------------- grow the unbalanced tree------------------------    \n",
    "    cur_node_num= len(nodes_tree)\n",
    "# -------------------------------------------------------------\n",
    "        \n",
    "    for i in range(0, current_NumNodes):\n",
    "    \n",
    "    #------------- grow the unbalanced tree ---------------------------------- \n",
    "        if cur_node_num >= 3:\n",
    "            parent_node = int(math.ceil(  (i + cur_node_num -2.0)*1.0/ 2 ))\n",
    "            parent_node_esti =nodes_tree_test[2* parent_node +1 ] \n",
    "            tmp_split_feature= nodes_tree[parent_node][0]\n",
    "            \n",
    "            if tmp_split_feature == -1:\n",
    "                nodes_tree.append(  (-1,  -1  )  )\n",
    "                nodes_tree_test.append( parent_node_esti  )\n",
    "                nodes_tree_test.append( parent_node_esti )\n",
    "                continue\n",
    "    #-------------------------------------------------------------------------\n",
    "        # prepare data for this node\n",
    "        current_node=i\n",
    "        current_node_data = dataToNode.filter( lambda line: line[0] == current_node ).cache()\n",
    "        current_node_data_cnt = current_node_data.count()\n",
    "        \n",
    "        # split initialization  \n",
    "\n",
    "        if loss_func == 'lad':\n",
    "                tmp_metric = median_LAD_node( current_node_data)\n",
    "                best_splitMetric_sofar= tmp_metric[2]\n",
    "       \n",
    "        \n",
    "        best_split=(-1,-1,-1,-1)\n",
    "        best_split_feature=-1\n",
    "        best_split_featureValueSet=-1\n",
    "                \n",
    "        #debug\n",
    "        tmpnode_cnt.append( current_node_data.count()  )\n",
    "        presplit.append( best_splitMetric_sofar/100000 )\n",
    "        \n",
    "        for j in range(0,numFeatures):    \n",
    "                                    \n",
    "#             if loss_func == 'tmse':\n",
    "                \n",
    "#                 current_node_data= tmp_metric[3]\n",
    "#                 current_node_data_cnt= tmp_metric[0]\n",
    "                \n",
    "#                 cur_split=split_onOneFeature_exact_trimmedMSE(current_node_data,current_node_data_cnt,\\\n",
    "                                           best_splitMetric_sofar,feature_valueList[j][1],j)\n",
    "            if loss_func == 'lad':\n",
    "                 cur_split=split_onOneFeature_exact_LAD(current_node_data,current_node_data_cnt,\\\n",
    "                                           best_splitMetric_sofar,feature_valueList[j][1],j)\n",
    "#             elif loss_func == 'mad':\n",
    "#                  cur_split=split_onOneFeature_exact_MAD(current_node_data,current_node_data_cnt,\\\n",
    "#                                            best_splitMetric_sofar,feature_valueList[j][1],j)\n",
    "#             elif loss_func == 'mse':\n",
    "#                  cur_split=split_onOneFeature_exact_MSE(current_node_data,current_node_data_cnt,\\\n",
    "#                                            best_splitMetric_sofar,feature_valueList[j][1],j)\n",
    "                \n",
    "            if  cur_split[0]!=-1  and cur_split[3] < best_splitMetric_sofar:\n",
    "                best_split=cur_split\n",
    "                best_splitMetric_sofar=cur_split[3]\n",
    "                best_split_feature = j\n",
    "                best_split_featureValueSet= cur_split[0]                  \n",
    "                \n",
    "#------------- grow the unbalanced tree ---------------------------------- \n",
    "        if best_split_feature == -1:\n",
    "            nodes_tree.append(  (-1,  -1  )  )\n",
    "            nodes_tree_test.append( tmp_metric[1]  )\n",
    "            nodes_tree_test.append( tmp_metric[1] )\n",
    "            continue\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "        # tree for spliting nodes        \n",
    "        nodes_tree.append(  (best_split_feature,  best_split_featureValueSet  )  )\n",
    "        \n",
    "        # tree for predicting   \n",
    "        nodes_tree_test.append(best_split[1])\n",
    "        nodes_tree_test.append(best_split[2])\n",
    "        \n",
    "#-----debug---------\n",
    "#     print tmpnode_cnt\n",
    "#     print presplit\n",
    "    \n",
    "#     return nodes_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994511 332190\n"
     ]
    }
   ],
   "source": [
    "# training and testing process: data preparation\n",
    "\n",
    "# with outliers\n",
    "# tmpdta = dta.takeSample(False, 1000000, 12243)\n",
    "# dta_train_all = sc.parallelize(tmpdta).cache().sample(False, .1, 12345)\n",
    "# dta_test_all = sc.parallelize(tmpdta).cache().sample(False, .1, 43243)\n",
    "\n",
    "\n",
    "dta_train_all = dta.cache().sample(False, .3, 12345)\n",
    "dta_test_all = dta.cache().sample(False, .1, 43243)\n",
    "\n",
    "print dta_train_all.count(), dta_test_all.count() \n",
    "# configurate extraction\n",
    "# print 'number of feature-value combinations:',len(dta_train_all.map(lambda line:(line[1],line[2],line[3])).distinct().collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "994511\n",
      "312746\n"
     ]
    }
   ],
   "source": [
    "# training and testing process: clearned or noisy data ?\n",
    "\n",
    "# 98974\n",
    "# 99987\n",
    "\n",
    "dta_train = dta_train_all\n",
    "# .filter(lambda line: line[0]<1000 )\n",
    "dta_test = dta_test_all.filter(lambda line: line[0]<1000 )\n",
    "\n",
    "print dta_train.count()\n",
    "print dta_test.count()\n",
    "# print 'number of feature-value combinations:',len(dta_train.map(lambda line:(line[1],line[2],line[3])).distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training and testing process: main process\n",
    "\n",
    "#parameters\n",
    "maxdepth=8\n",
    "numFeatures=7\n",
    "trimm_ratio = 0.00\n",
    "# tmse (trimmed mse), lad, ma d, mse\n",
    "loss_func= 'lad'\n",
    "\n",
    "def tree_test_mapFunc_median(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], node_split)\n",
    "    return (leaf_nodes[ tmpnode ]  - line[0])*(leaf_nodes[ tmpnode ]  - line[0])\n",
    "\n",
    "def tree_test_mapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], node_split)\n",
    "    return (leaf_nodes[ tmpnode ]  - line[0])*(leaf_nodes[ tmpnode ]  - line[0])\n",
    "    \n",
    "def tree_test( testData_rdd ):\n",
    "    err_rdd=testData_rdd.map( lambda line: tree_test_mapFunc(line))\n",
    "    err_sum = err_rdd.reduce( lambda a,b: a+b)\n",
    "    return err_sum / testData_rdd.count()\n",
    "\n",
    "# variables for the tree structure \n",
    "node_split=[]\n",
    "node_test=[]\n",
    "\n",
    "test_err = []\n",
    "train_err = []\n",
    "run_time=[]\n",
    "\n",
    "tree_history_split=[]\n",
    "tree_history_esti=[]\n",
    "tree_history_leaf=[]\n",
    "tree_history_runtime=[]\n",
    "\n",
    "\n",
    "#ini parameter on the cluster\n",
    "currentNode_split_fromMaster = sc.broadcast([])\n",
    "\n",
    "\n",
    "#prepare feature value set\n",
    "feature_valueSet = data_featureValues_collect( dta_train )\n",
    "feature_valueList=[]\n",
    "for i in range(0, len(feature_valueSet)):\n",
    "    feature_valueList.append( ( i, list(feature_valueSet[i][1]) ) )\n",
    "    \n",
    "#     tune the starting depth\n",
    "for i in range(maxdepth,maxdepth+1):\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    del node_split[:]\n",
    "    del node_test[:]\n",
    "    currentNode_split_fromMaster = sc.broadcast([])\n",
    "    \n",
    "    for cur_depth in range(0,i):\n",
    "        \n",
    "        current_nodeNum= int( math.pow(2,  cur_depth)) \n",
    "        dataToNode = dataToNode_assignment( dta_train )\n",
    "        \n",
    "        #debug\n",
    "        print  dataToNode.map(lambda line:line[0]).distinct().count() , current_nodeNum\n",
    "        \n",
    "        find_bestSplit_exact(dataToNode,current_nodeNum, node_split, node_test, feature_valueList,loss_func)   \n",
    "#         ( dataToNode, current_NumNodes, nodes_tree, nodes_tree_test, feature_valueSet):\n",
    "        #for whole tree\n",
    "        currentNode_split_fromMaster = sc.broadcast(node_split)\n",
    "    \n",
    "    end = time.time()\n",
    "    leaf_nodes = node_test[ len(node_test)-  (int)( math.pow(2,i)): len(node_test)  ]\n",
    "    \n",
    "    tmp_test_err= 0 \n",
    "#     tree_test( dta_test )\n",
    "    tmp_train_err= 0\n",
    "#     tree_test( dta_train )\n",
    "#     test_err.append( tmp_test_err)\n",
    "#     train_err.append( tmp_train_err )\n",
    "\n",
    "    elapsed = end-start\n",
    "    run_time.append(elapsed)\n",
    "\n",
    "    tree_history_split.append(copy.deepcopy(node_split) )\n",
    "    tree_history_esti.append( copy.deepcopy(node_test)  )\n",
    "    tree_history_leaf.append( copy.deepcopy(leaf_nodes)  )\n",
    "    tree_history_runtime.append( copy.deepcopy(run_time)  )\n",
    "        \n",
    "    print \"error at tree height\", i,\":\",  tmp_test_err, tmp_train_err\n",
    "    print \"running time at tree height\", i,\":\",  elapsed\n",
    "    print \"number of leaf nodes at tree height\", i,\":\",  len(leaf_nodes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp_split=tree_history_split\n",
    "tmp_esti= tree_history_esti\n",
    "tmp_leaf= tree_history_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print dta_test.first()\n",
    "err_rdd=dta_test.map(lambda line:search_nodeToData(line[1:numFeatures+1], node_split) ) \n",
    "print len(node_split)\n",
    "print len(node_test)\n",
    "print len(leaf_nodes)\n",
    "\n",
    "# print node_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.995640799666\n"
     ]
    }
   ],
   "source": [
    "err_rdd=dta_test.map(lambda line:(line[0],leaf_nodes[search_nodeToData(line[1:numFeatures+1], node_split) ], search_nodeToData(line[1:numFeatures+1], node_split) ) )\n",
    "\n",
    "print err_rdd.map( lambda line: (line[0]-line[1])*(line[0]-line[1])  ).reduce(lambda a, b: a+b)/err_rdd.count()\n",
    "# err_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# training and testing process: extract all the feature-value combinations \n",
    "# in the training data set\n",
    "\n",
    "total_featureVal_set=[]\n",
    "for i in range(0, numFeatures):\n",
    "    featureValues = dta_train.map(lambda line: line[1+i]).distinct().collect()\n",
    "    total_featureVal_set.append( featureValues)\n",
    "\n",
    "print 'done'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "15\n",
      "1\n",
      "30\n",
      "1\n",
      "16\n",
      "[[4014.9761939048767]]\n"
     ]
    }
   ],
   "source": [
    "#---------------- LAD -------------------  data backup\n",
    "lad_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "lad_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "lad_tree_history_leaf =  copy.deepcopy( tree_history_leaf)\n",
    "lad_tree_history_runtime =  copy.deepcopy( tree_history_runtime)\n",
    "\n",
    "print len(lad_tree_history_split)\n",
    "print len(lad_tree_history_split[0])\n",
    "print len(lad_tree_history_esti)\n",
    "print len(lad_tree_history_esti[0])\n",
    "\n",
    "print len(lad_tree_history_leaf)\n",
    "print len(lad_tree_history_leaf[0])\n",
    "\n",
    "print lad_tree_history_runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LAD result statistic: node infor.\n",
    "\n",
    "# lad_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "# lad_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "\n",
    "print 'test error:',test_err\n",
    "print 'training error:',train_err\n",
    "print len(lad_tree_history_split)\n",
    "\n",
    "recog_conf=bfs_tree(lad_tree_history_split[ 0 ], total_featureVal_set, maxdepth-1,  lad_tree_history_esti[0], dta_train )  \n",
    "\n",
    "print len(recog_conf[0])\n",
    "print recog_conf[1]\n",
    "print recog_conf[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error at depth 1 : 19766.1169192\n",
      "test error at depth 2 : 18967.6044969\n",
      "test error at depth 3 : 48333.2289398\n",
      "test error at depth 4 : 19889.3832151\n"
     ]
    }
   ],
   "source": [
    "# LAD result statistic : test error\n",
    "\n",
    "pre=0\n",
    "tree_esti = lad_tree_history_esti[0]\n",
    "tree_split =  lad_tree_history_split[ 0 ]\n",
    "\n",
    "for i in range(1,maxdepth+1):\n",
    "            \n",
    "    end_bound= pre+ (int)( math.pow(2,i)) \n",
    "    test_leaf_nodes = tree_esti[ pre: end_bound  ]\n",
    "    pre= end_bound        \n",
    "    test_tree_split= tree_split[ 0:(int)( math.pow(2,i)-1)  ]\n",
    "    \n",
    "#     print len(test_tree_split)\n",
    "#     print len(test_leaf_nodes)\n",
    "\n",
    "    test_error_tree( i, dta_test  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data inspection for the nodes of the trained tree\n",
    "\n",
    "def feature_value_extract( valueSet):\n",
    "    \n",
    "    tmpval=1\n",
    "    tmp_valSet= valueSet\n",
    "    valSet_list=[]\n",
    "    cnt=0\n",
    "    \n",
    "    while tmpval <= tmp_valSet:\n",
    "        if tmpval & tmp_valSet !=0:\n",
    "            valSet_list.append(cnt)\n",
    "        tmpval=tmpval<<1\n",
    "        cnt=cnt+1\n",
    "        \n",
    "    return valSet_list\n",
    "\n",
    "def dfs_tree(tree, current_nodeIdx, current_featureVal_set, current_depth):\n",
    "    \n",
    "    nodeNum=len(tree)   \n",
    "    if nodeNum == 0:\n",
    "        return\n",
    "    \n",
    "    split_feature= tree[current_nodeIdx][0]\n",
    "    split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "    if split_feature==-1:\n",
    "        print current_featureVal_set\n",
    "        return;\n",
    "    \n",
    "    featureValue_list = feature_value_extract( split_valueSet)\n",
    "    \n",
    "    left_featureValue_set=[]\n",
    "    right_featureValue_set=[]\n",
    "    featureNum= len(current_featureVal_set)\n",
    "    \n",
    "    for i in range(0, featureNum):\n",
    "        if i != split_feature:\n",
    "            left_featureValue_set.append(  copy.deepcopy(current_featureVal_set[i])   )\n",
    "            right_featureValue_set.append(  copy.deepcopy(current_featureVal_set[i])   )\n",
    "        else:\n",
    "            left_featureValue_set.append(  copy.deepcopy(featureValue_list )   )\n",
    "            right_featureValue_set.append(copy.deepcopy(list(set(current_featureVal_set[i] )-set(featureValue_list)))   )\n",
    "            \n",
    "        \n",
    "    traverse_tree(tree, current_nodeIdx*2+1,left_featureValue_set,current_depth+1)\n",
    "    traverse_tree(tree, current_nodeIdx*2+2,right_featureValue_set, current_depth+1)\n",
    "    \n",
    "    return \n",
    "\n",
    "def training_data_summary( train_rdd ):\n",
    "\n",
    "#     always remove outliers before\n",
    "    train_rdd_clean= train_rdd.filter(lambda line: line[0]<1000)\n",
    "    distinct_featureVals = train_rdd_clean.map(lambda line: (line[1], line[2],line[3],line[4]) ).distinct().collect()\n",
    "   \n",
    "    featureConfig_val={}\n",
    "    for i in distinct_featureVals:\n",
    "        \n",
    "        tmprdd=train_rdd_clean.filter(lambda line:(line[1], line[2],line[3],line[4]) ==i ).map(lambda line: line[0])\n",
    "        tmpval=tmprdd.reduce(lambda a,b:a+b)\n",
    "        tmpval= tmpval/tmprdd.count()*1.0\n",
    "        \n",
    "        featureConfig_val.update( {i:tmpval} )\n",
    "    \n",
    "#     print featureConfig_val\n",
    "    return featureConfig_val\n",
    "        \n",
    "\n",
    "def config_check( featureVal_set, feature_num):\n",
    "    \n",
    "    for i in range(0, feature_num):\n",
    "        if len(featureVal_set[i]) >1 :\n",
    "            return -1\n",
    "    return 1\n",
    "\n",
    "def lookup_featureConfig_val( configToVal_dict, value_set, feature_num):\n",
    "    \n",
    "    tmptuple=()\n",
    "    for i in range(0, feature_num):\n",
    "        tmptuple = tmptuple + ( value_set[i][0]  ,)\n",
    "#     print '++++++++++++', tmptuple\n",
    "    return configToVal_dict[tmptuple]\n",
    "    \n",
    "def test_error_mapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], test_tree_split)\n",
    "    return (test_leaf_nodes[ tmpnode ]  - line[0])*(test_leaf_nodes[ tmpnode ]  - line[0])\n",
    "    \n",
    "def test_error_tree( current_depth, testRDD  ):\n",
    "    \n",
    "    test_cnt= testRDD.count()\n",
    "    err_rdd=testRDD.map( lambda line: test_error_mapFunc(line))\n",
    "    err_sum = err_rdd.reduce( lambda a,b: a+b)\n",
    "    \n",
    "    print 'test error at depth',current_depth, ':', err_sum*1.0/test_cnt\n",
    "    return err_sum*1.0/test_cnt\n",
    "    \n",
    "def bfs_tree(tree, total_featureVal_set, depth, tree_esti, trainRDD ):\n",
    "    \n",
    "    nodeNum=len(tree)   \n",
    "    if nodeNum == 0:\n",
    "        return\n",
    "    \n",
    "    feature_ConfigToVal = training_data_summary( trainRDD )\n",
    "    \n",
    "    interNode_cnt=0\n",
    "    non_leaf_cnt=0\n",
    "    recog_config_cnt=0\n",
    "    \n",
    "    qu=[]\n",
    "    current_depth=0\n",
    "    qu.append((0,total_featureVal_set,0))\n",
    "    \n",
    "    \n",
    "    recog_config_cnt_depth=[]\n",
    "    recog_conf=[]\n",
    "    \n",
    "    while current_depth<= depth:\n",
    "        \n",
    "        print 'non_leaf_node count at previous depth', non_leaf_cnt\n",
    "        \n",
    "        print 'depth', current_depth,':', len(qu)\n",
    "        tmpqu=[]\n",
    "        \n",
    "        non_leaf_cnt=0\n",
    "        \n",
    "        for i in qu:\n",
    "            current_nodeIdx= i[0] \n",
    "            current_featureVal_set= i[1]\n",
    "            \n",
    "            split_feature= tree[current_nodeIdx][0]\n",
    "            split_valueSet= tree[current_nodeIdx][1]\n",
    "                 \n",
    "            if split_feature==-1: \n",
    "                \n",
    "                if config_check( current_featureVal_set, featureNum) ==1:\n",
    "                    recog_config_cnt= recog_config_cnt+1\n",
    "                    tmpval=lookup_featureConfig_val( feature_ConfigToVal, current_featureVal_set, featureNum)\n",
    "                    \n",
    "                    print current_featureVal_set, 'has the estimate:', tree_esti[ current_nodeIdx*2+1],\\\n",
    "                    'true:',tmpval\n",
    "                    #result\n",
    "                    recog_conf.append( (tree_esti[ current_nodeIdx*2+1], tmpval) )\n",
    "                    \n",
    "                else:\n",
    "                    print current_featureVal_set, 'has the estimate:', tree_esti[ current_nodeIdx*2+1],'!!! not seperated !!!'\n",
    "                \n",
    "                interNode_cnt=interNode_cnt+1\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                non_leaf_cnt=non_leaf_cnt+1\n",
    "                print current_featureVal_set,'non-leaf node'\n",
    "                \n",
    "                \n",
    "            featureValue_list = feature_value_extract( split_valueSet)\n",
    "    \n",
    "            left_featureValue_set=[]\n",
    "            right_featureValue_set=[]\n",
    "            featureNum= len(current_featureVal_set)\n",
    "    \n",
    "            for i in range(0, featureNum):\n",
    "                if i != split_feature:\n",
    "                    left_featureValue_set.append(  copy.deepcopy(current_featureVal_set[i])   )\n",
    "                    right_featureValue_set.append(  copy.deepcopy(current_featureVal_set[i])   )\n",
    "                else:\n",
    "                    left_featureValue_set.append(  copy.deepcopy(featureValue_list )   )\n",
    "                    right_featureValue_set.append(copy.deepcopy(list(set(current_featureVal_set[i] )-set(featureValue_list)))   )\n",
    "            \n",
    "            tmpqu.append(  (current_nodeIdx*2+1,left_featureValue_set, current_nodeIdx)  )\n",
    "            tmpqu.append(  (current_nodeIdx*2+2,right_featureValue_set,current_nodeIdx) )\n",
    "        \n",
    "        qu=copy.deepcopy(tmpqu)\n",
    "        current_depth= current_depth+1\n",
    "        \n",
    "        #result\n",
    "        recog_config_cnt_depth.append( recog_config_cnt )\n",
    "    \n",
    "    \n",
    "# print out leaf nodes\n",
    "    print 'bottom depth', current_depth,':', len(qu)\n",
    "    \n",
    "    for i in qu:\n",
    "        current_nodeIdx= i[0] \n",
    "        current_featureVal_set= i[1]\n",
    "        parent_nodeIdx= i[2]\n",
    "        \n",
    "        if 2*parent_nodeIdx+1 == current_nodeIdx: \n",
    "#         parent_nodeIdx == (current_nodeIdx*2+1):\n",
    "            tree_esti_val= tree_esti[ parent_nodeIdx*2]\n",
    "        else:\n",
    "            tree_esti_val= tree_esti[ parent_nodeIdx*2+1] \n",
    "        \n",
    "        if config_check( current_featureVal_set, featureNum) ==1:\n",
    "            recog_config_cnt= recog_config_cnt+1\n",
    "            tmpval=lookup_featureConfig_val( feature_ConfigToVal, current_featureVal_set, featureNum)\n",
    "            \n",
    "            #result\n",
    "            recog_conf.append( (tree_esti_val, tmpval) )\n",
    "            \n",
    "            print current_featureVal_set, 'has the estimate:', tree_esti_val,\\\n",
    "                    'true:',tmpval\n",
    "        else:\n",
    "            print current_featureVal_set, 'has the estimate:', tree_esti_val\n",
    "    \n",
    "    #result\n",
    "    recog_config_cnt_depth.append( recog_config_cnt )\n",
    "        \n",
    "    print 'inter-node number:', interNode_cnt\n",
    "    print ' *******  number of identified configurations:', recog_config_cnt\n",
    "    \n",
    "    tmpval=0\n",
    "    for i in range(0,recog_config_cnt):\n",
    "        tmpval=tmpval+ (recog_conf[i][0]-recog_conf[i][1])*(recog_conf[i][0]-recog_conf[i][1])\n",
    "    \n",
    "    return (recog_conf, recog_config_cnt_depth, tmpval*1.0/recog_config_cnt)\n",
    "\n",
    "# len( tree_history_split)\n",
    "# len(tree_history_esti)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#do not run this part\n",
    "# training and testing process\n",
    "\n",
    "# without outliers\n",
    "tmpdta = dta.filter(lambda line: line[0]<1000).takeSample(False, 100000, 12243)\n",
    "dta_train = sc.parallelize(tmpdta).cache().sample(False, .7, 12345)\n",
    "dta_test = sc.parallelize(tmpdta).cache().sample(False, .3, 43243)\n",
    "\n",
    "maxdepth=10\n",
    "numFeatures=3\n",
    "\n",
    "node_split=[]\n",
    "node_test=[]\n",
    "run_time=[]\n",
    "\n",
    "#------------ estimate phase ---------------------\n",
    "# def search_nodeToData(features, tree):\n",
    "#     nodeNum=len(tree)   \n",
    "#     if nodeNum == 0:\n",
    "#         return 0;\n",
    "    \n",
    "#     current_nodeIdx=0\n",
    "#     while current_nodeIdx< nodeNum:\n",
    "#         split_feature= tree[current_nodeIdx][0]\n",
    "#         split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "#         if split_feature==-1:\n",
    "#             return -1\n",
    "#         if (( 1<<features[split_feature]) &  split_valueSet) != 0 :\n",
    "#             current_nodeIdx=current_nodeIdx*2+1\n",
    "#         else:\n",
    "#             current_nodeIdx=current_nodeIdx*2+2        \n",
    "#     return current_nodeIdx - nodeNum\n",
    "\n",
    "def tree_test_mapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], node_split)\n",
    "    return (leaf_nodes[ tmpnode ]  - line[0])*(leaf_nodes[ tmpnode ]  - line[0])\n",
    "    \n",
    "def tree_test( testData_rdd ):\n",
    "    err_rdd=testData_rdd.map( lambda line: tree_test_mapFunc(line))\n",
    "    err_sum = err_rdd.reduce( lambda a,b: a+b)\n",
    "    return err_sum / testData_rdd.count()\n",
    "#------------------------------------------------\n",
    "    \n",
    "#ini parameter on the cluster\n",
    "currentNode_split_fromMaster = sc.broadcast([])\n",
    "test_err = []\n",
    "train_err = []\n",
    "#prepare feature value set\n",
    "feature_valueSet = data_featureValues_collect( dta_train )\n",
    "feature_valueList=[]\n",
    "for i in range(0, len(feature_valueSet)):\n",
    "    feature_valueList.append( ( i, list(feature_valueSet[i][1]) ) )\n",
    "    \n",
    "    \n",
    "for i in range(6,maxdepth):\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    del node_split[:]\n",
    "    del node_test[:]\n",
    "    currentNode_split_fromMaster = sc.broadcast([])\n",
    "    \n",
    "    for cur_depth in range(0,i):\n",
    "        \n",
    "        current_nodeNum= int( math.pow(2,  cur_depth)) \n",
    "        \n",
    "        dataToNode = dataToNode_assignment( dta_train )\n",
    "        find_bestSplit_exact(dataToNode,current_nodeNum, node_split, node_test, feature_valueList)   \n",
    "#         ( dataToNode, current_NumNodes, nodes_tree, nodes_tree_test, feature_valueSet):\n",
    "        #for whole tree\n",
    "        currentNode_split_fromMaster = sc.broadcast(node_split)\n",
    "    \n",
    "    end = time.time()\n",
    "    elapsed = end - start    \n",
    "    leaf_nodes = node_test[ len(node_test)-  (int)( math.pow(2,i)): len(node_test)  ]\n",
    "\n",
    "    \n",
    "    tmp_test_err= tree_test( dta_test )\n",
    "    tmp_train_err= tree_test( dta_train )\n",
    "    \n",
    "    test_err.append( tmp_test_err)\n",
    "    train_err.append( tmp_train_err )\n",
    "    run_time.append(elapsed)\n",
    "    \n",
    "    print \"error at tree height\", i,\":\",  tmp_test_err, tmp_train_err\n",
    "    print \"running time at tree height\", i,\":\",  elapsed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# singal-run training  process  \n",
    "dta_train = dta.sample(False, .7, 12345)\n",
    "dta_test = dta.sample(False, .3, 43243)\n",
    "dta_test = dta_test.filter(lambda line: line[0] <1000 )\n",
    "\n",
    "#parameters\n",
    "maxdepth=\n",
    "numFeatures=3\n",
    "\n",
    "node_split=[]\n",
    "node_test=[]\n",
    "#ini parameter\n",
    "currentNode_split_fromMaster = sc.broadcast([])\n",
    "\n",
    "#prepare feature value set\n",
    "feature_valueSet = data_featureValues_collect( dta_train )\n",
    "feature_valueList=[]\n",
    "for i in range(0, len(feature_valueSet)):\n",
    "    feature_valueList.append( ( i, list(feature_valueSet[i][1]) ) )\n",
    "    \n",
    "\n",
    "for i in range(0,maxdepth):\n",
    "    \n",
    "    start = time.time()\n",
    "    current_nodeNum= int( math.pow(2,  i))\n",
    "    \n",
    "    print 'current split decision:',currentNode_split_fromMaster.value\n",
    "\n",
    "    dataToNode = dataToNode_assignment( dta_train )\n",
    "    cluster_end = time.time() \n",
    "    find_bestSplit_exact(dataToNode,current_nodeNum,node_split,node_test, feature_valueList)   \n",
    "#   ( dataToNode, current_NumNodes, nodes_tree, nodes_tree_test, feature_valueSet):\n",
    "    currentNode_split_fromMaster = sc.broadcast(node_split)\n",
    "  \n",
    "    end = time.time() \n",
    "\n",
    "    print i,'-th level running time: ', cluster_end - start,'sec', end- cluster_end, 'sec'\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69015"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#select one feature-value\n",
    "feature_id=1\n",
    "feature_val=11\n",
    "\n",
    "dataToFeatureValue_rdd=node_data.filter(lambda line:line[1][feature_id+1]==feature_val).map(lambda line:line[1][0])\n",
    "\n",
    "dataToFeatureValue_rdd_count = dataToFeatureValue_rdd.count()\n",
    "        \n",
    "# med_tmpLarge= dataToFeatureValue_rdd.top( dataToFeatureValue_rdd_count/2+1  )\n",
    "med_tmp = dataToFeatureValue_rdd.top( dataToFeatureValue_rdd_count/2  )\n",
    "med_tmp.sort()\n",
    "tmpcnt= len(med_tmp)\n",
    "print med_tmp[ tmpcnt-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437547\n"
     ]
    }
   ],
   "source": [
    "#select several feature-values\n",
    "feature_values=[1,2,3]\n",
    "\n",
    "dataToFeatureValue_rdd=node_data.filter(lambda line:line[1][feature_id+1] in feature_values).map(lambda line:line[1][0])\n",
    "\n",
    "dataToFeatureValue_rdd_count = dataToFeatureValue_rdd.count()\n",
    "\n",
    "print dataToFeatureValue_rdd_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
