{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os \n",
    "import sys \n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "from datetime  import datetime \n",
    "\n",
    "import numpy as np  # learn \n",
    "import pandas as pd # learn\n",
    "from pandas import *\n",
    "from numpy import *\n",
    "\n",
    "from scipy import stats # look at scipy\n",
    "from scipy import linalg\n",
    "from scipy import *\n",
    "\n",
    " \n",
    "import matplotlib as mplt # learn matplolib \n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "import seaborn as sns \n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc={\"figure.figsize\": (14, 6)})\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import sklearn as sk\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import random\n",
    "import time\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#BT data loading\n",
    "\n",
    "# dta_RDD = sc.textFile(\"file:///home/tguo/data/tian-test/udpjitter-H1april2014_regTree.csv\")\n",
    "\n",
    "dta_RDD = sc.textFile(\"hdfs://computer61.ant-net/user/tguo/udpjitter-H1april2014_regTree_mllib.csv\")\n",
    "\n",
    "\n",
    "dta_splited = dta_RDD.map(lambda line: line.split(\",\")).map(lambda r:\n",
    "                                                                (float(r[0]),int( float(r[1])), \n",
    "                                                                 int(float(r[2])),int(float(r[3])),\n",
    "                                                                 int(float(r[4])),int(float(r[5])),\n",
    "                                                                 int(float(r[6])),int(float(r[7])) ))\n",
    "# dta_splited.first()\n",
    "\n",
    "dta= dta_splited\n",
    "dta.cache()\n",
    "print dta.first()\n",
    "print dta.count()\n",
    "\n",
    "\n",
    "# dta_filtered = dta.filter(lambda line: line[0] <2000 )\n",
    "# dta_filtered.cache()\n",
    "# dta_filtered.count()\n",
    "# print dta.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#block3: load synthetic data\n",
    "# dta_RDD = sc.textFile(\"hdfs://computer61.ant-net/user/tguo/syn-30m-1p10mag.csv\",10)\n",
    "\n",
    "dta_rdd = sc.textFile(\"file:///home/tguo/data/tian-syn/syn-10m-1p.txt\").cache()\n",
    "\n",
    "print dta_rdd.first()\n",
    "\n",
    "# data type conversion and organize data of the form (dependent variable value, \n",
    "# feature-values) \n",
    "dta_splited = dta_rdd.map(lambda line: line.split(\",\")).map(lambda r:\n",
    "                                                        ( float(r[5]),\n",
    "                                                        int(float(r[0])),int( float(r[1])), \n",
    "                                                        int(float(r[2])),int(float(r[3])),\n",
    "                                                        int(float(r[4])),\n",
    "                                                        int(float(r[5])),\n",
    "                                                        int(float(r[6])),int(float(r[7])),\n",
    "                                                        int(float(r[8]))\n",
    "                                                         ,int(float(r[9])),\n",
    "                                                        ) )\n",
    "\n",
    "\n",
    "dta= dta_splited\n",
    "print dta.first()\n",
    "print dta.count()\n",
    "print dta.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  load flight data\n",
    "\n",
    "dta_rdd = sc.textFile(\"hdfs://computer61.ant-net/user/tguo/airline_data.csv\")\n",
    "\n",
    "dta = dta_rdd.map(lambda line: line.split(\",\")).map(lambda r:\n",
    "                                                                (float(r[0]),\n",
    "                                                                 int( float(r[1])), \n",
    "                                                                 int(float(r[2])),\n",
    "                                                                 int(float(r[3])),\n",
    "                                                                 int(float(r[4])),\n",
    "                                                                 int(float(r[5])),\n",
    "                                                                 int(float(r[6])),\n",
    "                                                                 int(float(r[7])), \n",
    "                                                                 int(float(r[8])))).cache()\n",
    "\n",
    "\n",
    "print dta.first()\n",
    "print dta.count()\n",
    "\n",
    "tmprdd =  dta.map(lambda line:line[0])\n",
    "print tmprdd.max(), tmprdd.min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_workers=5\n",
    "maxdepth=10\n",
    "numFeatures=10\n",
    "trimm_ratio = 0.00\n",
    "bin_num=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# basic functions in one-pass method  [on the cluster side]\n",
    "\n",
    "# assign each data instance to the node\n",
    "def search_nodeToData(features, tree):\n",
    "    \n",
    "    nodeNum=len(tree)   \n",
    "    if nodeNum == 0:\n",
    "        return 0;\n",
    "    \n",
    "    current_nodeIdx=0\n",
    "    while current_nodeIdx< nodeNum:\n",
    "        split_feature= tree[current_nodeIdx][0]\n",
    "        split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "        # unbalanced tree grow\n",
    "        if split_feature==-1:\n",
    "            current_nodeIdx=current_nodeIdx*2+1+ random.randint(0, 1)\n",
    "            continue\n",
    "            \n",
    "        if (( 1<<features[split_feature]) &  split_valueSet) != 0 :\n",
    "            current_nodeIdx=current_nodeIdx*2+1\n",
    "        else:\n",
    "            current_nodeIdx=current_nodeIdx*2+2\n",
    "            \n",
    "    return current_nodeIdx - nodeNum\n",
    "\n",
    "def update_hist(hist, newY):\n",
    "\n",
    "    numbins=len(hist)\n",
    "    \n",
    "    if newY in hist.keys():\n",
    "        hist[newY][2]= hist[newY][2] +1 \n",
    "    else:\n",
    "        hist.update({newY: [newY,newY,1]})\n",
    "        if numbins+1 > bin_num:\n",
    "            hist.update({newY: [newY,newY,1]}) \n",
    "            #data in each bin: left bound, right bound, count\n",
    "        \n",
    "            sorted_binIds=hist.keys()\n",
    "            # binId is the lower bound of the value range of the bin\n",
    "            sorted_binIds.sort()\n",
    "        \n",
    "            mindis= sorted_binIds[1]- sorted_binIds[0]\n",
    "            mindis_bin_left=sorted_binIds[0]\n",
    "            mindis_bin_right=sorted_binIds[1]\n",
    "            \n",
    "            for i in range(1,numbins+1):\n",
    "                tmp= sorted_binIds[i]- sorted_binIds[i-1]\n",
    "                if tmp < mindis:\n",
    "                    mindis=tmp\n",
    "                    mindis_bin_left= sorted_binIds[i-1]\n",
    "                    mindis_bin_right= sorted_binIds[i]       \n",
    "            \n",
    "#           print hist_local[mindis_bin_right][1],hist_local[mindis_bin_left][1]\n",
    "            hist[mindis_bin_left][1] = max(hist[mindis_bin_right][1],hist[mindis_bin_left][1] )                        \n",
    "            hist[mindis_bin_left][2] = hist[mindis_bin_left][2] + \\\n",
    "            hist[mindis_bin_right][2]\n",
    "            del hist[mindis_bin_right]\n",
    "            \n",
    "    return hist\n",
    "\n",
    "def merge_hist(hist1, hist2):\n",
    "    tmp=[]\n",
    "    concat_hist=hist1\n",
    "    \n",
    "    if len(hist1)==0:\n",
    "        return hist2\n",
    "    if len(hist2)==0:\n",
    "        return hist1\n",
    "\n",
    "    for i in concat_hist.keys():\n",
    "        if i in hist2.keys():\n",
    "            concat_hist.update( {i: [i,max( concat_hist[i][1], hist2[i][1]),concat_hist[i][2]+hist2[i][2]]})\n",
    "    \n",
    "    for i in hist2.keys():\n",
    "        if i not in concat_hist.keys():\n",
    "            concat_hist.update( {i:hist2[i]})\n",
    "\n",
    "    cnt_total= len(concat_hist)\n",
    "    if cnt_total <= bin_num:\n",
    "        return concat_hist\n",
    "    else:\n",
    "        bins=concat_hist.keys()\n",
    "        bins.sort()\n",
    "        \n",
    "        disl=[]\n",
    "        disr=[]\n",
    "        tmpdis=0\n",
    "        tmpleft=0\n",
    "        tmpright=0\n",
    "        \n",
    "        for i in range(0,cnt_total-1):\n",
    "            tmpleft= bins[i]\n",
    "            tmpright= bins[i+1]\n",
    "            tmpdis= tmpright-tmpleft \n",
    "            disl.append((tmpdis,tmpleft))\n",
    "            disr.append((tmpdis,tmpright))\n",
    "        disl.sort()\n",
    "        disr.sort()\n",
    "        \n",
    "        bin_num_toRemove=cnt_total - bin_num\n",
    "        disIdx=[]\n",
    "        cur=0\n",
    "        \n",
    "        for i in range(0,bin_num_toRemove):\n",
    "            while disl[cur][1] not in concat_hist.keys()  :\n",
    "                cur=cur+1\n",
    "            \n",
    "            tmphist1= concat_hist[disl[cur][1] ]\n",
    "            tmphist2= concat_hist[disr[cur][1] ]\n",
    "            tmpval1=tmphist1[1]\n",
    "            tmpval2=tmphist2[1]\n",
    "            tmphist1[1]= max(tmpval1, tmpval2)\n",
    "            tmphist1[2]=  tmphist1[2] + tmphist2[2]\n",
    "            del concat_hist[disr[cur][1]]\n",
    "            \n",
    "            cur=cur+1\n",
    "            \n",
    "    return concat_hist\n",
    "\n",
    "def partition_combiner_hist(list_dvAndfeatures):\n",
    "    \n",
    "    nodes_dict={}\n",
    "    tmpcnt=0\n",
    "    \n",
    "#     #debug\n",
    "#     for line in dvFeatures:\n",
    "#         tmp_var= line[0]\n",
    "    \n",
    "    \n",
    "    for dvAndfeatures in list_dvAndfeatures:\n",
    "        \n",
    "        Y= dvAndfeatures[0]\n",
    "        \n",
    "        node= search_nodeToData(dvAndfeatures[1:numFeatures+1], \\\n",
    "                                currentNode_split_fromMaster.value)\n",
    "        \n",
    "        if node in nodes_dict.keys():\n",
    "            \n",
    "            # new added: s um of y in a node\n",
    "            nodes_dict[node]['sumY'] = nodes_dict[node]['sumY']+Y \n",
    "#             nodes_dict[node]['sumSquY'] = nodes_dict[node]['sumSquY']+Y*Y \n",
    "            nodes_dict[node]['count'] = nodes_dict[node]['count']+1 \n",
    "            nodes_dict[node]['hist']= update_hist(nodes_dict[node]['hist'], Y)\n",
    "            \n",
    "            for i in range(0,numFeatures):    \n",
    "                feature_val= dvAndfeatures[i+1]\n",
    "                \n",
    "                if feature_val not in nodes_dict[node][i]:\n",
    "                    nodes_dict[node][i].update( { feature_val: {}   }  )\n",
    "    \n",
    "                    nodes_dict[node][i][feature_val].update( {'count_inFeatureValue':1} )\n",
    "                    nodes_dict[node][i][feature_val].update( {'hist_inFeatureValue': {} } )\n",
    "                    nodes_dict[node][i][feature_val]['hist_inFeatureValue'].update({Y:[Y,Y,1]})\n",
    "                    \n",
    "                else:\n",
    "                    nodes_dict[node][i][feature_val]['count_inFeatureValue'] = \\\n",
    "                    nodes_dict[node][i][feature_val]['count_inFeatureValue']+1 \n",
    "                    nodes_dict[node][i][feature_val]['hist_inFeatureValue']=\\\n",
    "                    update_hist(nodes_dict[node][i][feature_val]['hist_inFeatureValue'], Y)\n",
    "                    \n",
    "            \n",
    "        else:\n",
    "            nodes_dict.update( {node: {}})  \n",
    "            \n",
    "            # new added: sum of y in a node\n",
    "            nodes_dict[node].update( { 'sumY': Y} )\n",
    "            nodes_dict[node].update( { 'count': 1} )\n",
    "            nodes_dict[node].update( { 'hist': {}} )\n",
    "            nodes_dict[node]['hist'].update({Y:[Y,Y,1]})            \n",
    "        \n",
    "            for i in range(0,numFeatures):\n",
    "                feature_val= dvAndfeatures[i+1]\n",
    "                \n",
    "                nodes_dict[node].update( { i: {}} )    \n",
    "                nodes_dict[node][i].update( { feature_val: {}   }  )\n",
    "             \n",
    "                nodes_dict[node][i][feature_val].update( {'count_inFeatureValue':1} )\n",
    "                nodes_dict[node][i][feature_val].update( {'hist_inFeatureValue': {} } )\n",
    "                nodes_dict[node][i][feature_val]['hist_inFeatureValue'].update({Y:[Y,Y,1]})    \n",
    "                        \n",
    "    return zip(nodes_dict.keys(), nodes_dict.values())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def partition_combiner_hist():\n",
    "    \n",
    "\n",
    "def partition_combiner_hist_opt(list_dvAndfeatures):\n",
    "    \n",
    "    nodes_dict={}\n",
    "    tmpcnt=0\n",
    "\n",
    "#     local_dvFeatures=[]        \n",
    "#     for dvAndfeatures in list_dvAndfeatures:\n",
    "#         local_dvFeatures.append(dvAndfeatures)\n",
    "#     local_dvFeatures.sort()\n",
    "    \n",
    "    \n",
    "    \n",
    "    for dvAndfeatures in list_dvAndfeatures:\n",
    "        \n",
    "        Y= dvAndfeatures[0]\n",
    "        \n",
    "        node= search_nodeToData(dvAndfeatures[1:numFeatures+1], \\\n",
    "                                currentNode_split_fromMaster.value)\n",
    "        \n",
    "        if node in nodes_dict.keys():\n",
    "            \n",
    "            # new added: s um of y in a node\n",
    "            nodes_dict[node]['sumY'] = nodes_dict[node]['sumY']+Y \n",
    "#             nodes_dict[node]['sumSquY'] = nodes_dict[node]['sumSquY']+Y*Y \n",
    "            nodes_dict[node]['count'] = nodes_dict[node]['count']+1 \n",
    "            nodes_dict[node]['hist']= update_hist(nodes_dict[node]['hist'], Y)\n",
    "            \n",
    "            for i in range(0,numFeatures):    \n",
    "                feature_val= dvAndfeatures[i+1]\n",
    "                \n",
    "                if feature_val not in nodes_dict[node][i]:\n",
    "                    nodes_dict[node][i].update( { feature_val: {}   }  )\n",
    "    \n",
    "                    nodes_dict[node][i][feature_val].update( {'count_inFeatureValue':1} )\n",
    "                    nodes_dict[node][i][feature_val].update( {'hist_inFeatureValue': {} } )\n",
    "                    nodes_dict[node][i][feature_val]['hist_inFeatureValue'].update({Y:[Y,Y,1]})\n",
    "                    \n",
    "                else:\n",
    "                    nodes_dict[node][i][feature_val]['count_inFeatureValue'] = \\\n",
    "                    nodes_dict[node][i][feature_val]['count_inFeatureValue']+1 \n",
    "                    nodes_dict[node][i][feature_val]['hist_inFeatureValue']=\\\n",
    "                    update_hist(nodes_dict[node][i][feature_val]['hist_inFeatureValue'], Y)\n",
    "                    \n",
    "            \n",
    "        else:\n",
    "            nodes_dict.update( {node: {}})  \n",
    "            \n",
    "            # new added: sum of y in a node\n",
    "            nodes_dict[node].update( { 'sumY': Y} )\n",
    "            nodes_dict[node].update( { 'count': 1} )\n",
    "            nodes_dict[node].update( { 'hist': {}} )\n",
    "            nodes_dict[node]['hist'].update({Y:[Y,Y,1]})            \n",
    "        \n",
    "            for i in range(0,numFeatures):\n",
    "                feature_val= dvAndfeatures[i+1]\n",
    "                \n",
    "                nodes_dict[node].update( { i: {}} )    \n",
    "                nodes_dict[node][i].update( { feature_val: {}   }  )\n",
    "             \n",
    "                nodes_dict[node][i][feature_val].update( {'count_inFeatureValue':1} )\n",
    "                nodes_dict[node][i][feature_val].update( {'hist_inFeatureValue': {} } )\n",
    "                nodes_dict[node][i][feature_val]['hist_inFeatureValue'].update({Y:[Y,Y,1]})    \n",
    "                        \n",
    "    return zip(nodes_dict.keys(), nodes_dict.values())\n",
    "\n",
    "def merge_parttion_combiner_hist(nodeToFeatureToValue_1,nodeToFeatureToValue_2 ):\n",
    "# optimization: calculate median and amd for feature-value    \n",
    "    \n",
    "    # new added: sum of Y in a node\n",
    "    nodeToFeatureToValue_1['sumY']= nodeToFeatureToValue_1['sumY']+\\\n",
    "    nodeToFeatureToValue_2['sumY']\n",
    "    \n",
    "    nodeToFeatureToValue_1['count']= nodeToFeatureToValue_1['count']+\\\n",
    "    nodeToFeatureToValue_2['count'] \n",
    "    \n",
    "    nodeToFeatureToValue_1['hist']=merge_hist(nodeToFeatureToValue_1['hist'],nodeToFeatureToValue_2['hist'])\n",
    "    \n",
    "    for i in range(0, numFeatures): #feature\n",
    "        for j in nodeToFeatureToValue_1[i].keys(): #feature value\n",
    "            \n",
    "            feature_val=j\n",
    "            \n",
    "            if feature_val in nodeToFeatureToValue_2[i].keys():               \n",
    "                nodeToFeatureToValue_1[i][j]['hist_inFeatureValue'] = \\\n",
    "                merge_hist(nodeToFeatureToValue_1[i][j]['hist_inFeatureValue'], \n",
    "                           nodeToFeatureToValue_2[i][j]['hist_inFeatureValue'])     \n",
    "                nodeToFeatureToValue_1[i][j]['count_inFeatureValue'] = nodeToFeatureToValue_1[i][j]['count_inFeatureValue']+nodeToFeatureToValue_2[i][j]['count_inFeatureValue']\n",
    "                \n",
    "    for i in range(0, numFeatures):\n",
    "        for j in nodeToFeatureToValue_2[i].keys():\n",
    "            \n",
    "            feature_val=j\n",
    "            \n",
    "            if feature_val not in nodeToFeatureToValue_1[i].keys():\n",
    "                nodeToFeatureToValue_1[i].update({feature_val: {} })\n",
    "                nodeToFeatureToValue_1[i][feature_val]= nodeToFeatureToValue_2[i][feature_val].copy()\n",
    "                              \n",
    "    return  nodeToFeatureToValue_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split decision  [on the local side]\n",
    "\n",
    "#MAD: median absolute median in a histogram   \n",
    "def MAD_hist(hist, cnt): \n",
    "\n",
    "    sorted_his= sorted(hist.items(), key= lambda line: line[0])\n",
    "    num_bins= len(hist)\n",
    "    \n",
    "    cntByBounds=0\n",
    "    curCnt=0 \n",
    "    midCnt= cnt/2.0\n",
    "    \n",
    "    mid_bin_idx=0\n",
    "    flag=0\n",
    "    \n",
    "    meanAbsDevi=0\n",
    "    median=0\n",
    "    \n",
    "    cntSum_beforeMid=0\n",
    "    \n",
    "    for i in range(0, num_bins): \n",
    "        \n",
    "        if flag==0:\n",
    "            tmp_curCnt=curCnt+ sorted_his[i][1][2]\n",
    "            tmp_cntByBounds =cntByBounds - \\\n",
    "        (sorted_his[i][1][0]+sorted_his[i][1][1])/2.0*sorted_his[i][1][2]\n",
    "        \n",
    "            if tmp_curCnt >= midCnt:\n",
    "                flag=1       \n",
    "                \n",
    "                mid_bin_idx= i\n",
    "                \n",
    "                cntSum_beforeMid = curCnt\n",
    "            else:    \n",
    "                curCnt=tmp_curCnt\n",
    "                cntByBounds=tmp_cntByBounds\n",
    "        else:\n",
    "            curCnt= curCnt - sorted_his[i][1][2]\n",
    "            cntByBounds= cntByBounds + \\\n",
    "            (sorted_his[i][1][0]+sorted_his[i][1][1])/2.0*sorted_his[i][1][2]\n",
    "    \n",
    "    if cnt == 1:\n",
    "        for tmpkey in hist.keys():\n",
    "            return (hist[tmpkey][0],0)\n",
    "    elif cnt ==0:\n",
    "        return (0,0)\n",
    "    else:        \n",
    "\n",
    "        sample_inMedBin = midCnt - cntSum_beforeMid     \n",
    "        \n",
    "        if sorted_his[mid_bin_idx][1][2] ==1:\n",
    "            median = sorted_his[mid_bin_idx][1][0]\n",
    "        elif sorted_his[mid_bin_idx][1][2] ==2:\n",
    "            median= (sorted_his[mid_bin_idx][1][0] +sorted_his[mid_bin_idx][1][1])*1.0/2.0\n",
    "        else:\n",
    "            median= sorted_his[mid_bin_idx][1][0]*1.0 + \\\n",
    "        (1.0*sorted_his[mid_bin_idx][1][1]-1.0*sorted_his[mid_bin_idx][1][0])/(sorted_his[mid_bin_idx][1][2]+1)*sample_inMedBin\n",
    "        \n",
    "        meanAbsDevi =  cntByBounds + curCnt*median \n",
    "        \n",
    "        \n",
    "        \n",
    "        if sorted_his[mid_bin_idx][1][2] ==1:\n",
    "            return (median, meanAbsDevi*1.0/cnt)\n",
    "        \n",
    "        sample_val= sorted_his[ mid_bin_idx][1][0]*1.0 \n",
    "        sample_interval=1.0*(sorted_his[mid_bin_idx][1][1]-sorted_his[mid_bin_idx][1][0])/(sorted_his[mid_bin_idx][1][2]\\\n",
    "                                                                          -1.0)\n",
    "#         print meanAbsDevi     \n",
    "    \n",
    "        for i in range(0, sorted_his[mid_bin_idx][1][2]):\n",
    "            sample_val= sample_val + sample_interval*i\n",
    "            meanAbsDevi=meanAbsDevi+ abs( sample_val-median)\n",
    "            \n",
    "#             print abs( sample_val-median)\n",
    "                        \n",
    "        #debug\n",
    "#         if meanAbsDevi <0:\n",
    "#             tmpsum=0\n",
    "#             for tmpkey in hist:\n",
    "#                 tmpsum= tmpsum+ hist[tmpkey][2]\n",
    "#             if tmpsum!=cnt:\n",
    "#                 print 'inside MAD calculation, number of data check:',tmpsum,cnt             \n",
    "#             print 'inside MAD calculation:', tmpMAD, cnt\n",
    "            \n",
    "        \n",
    "        return (median, meanAbsDevi*1.0/cnt)\n",
    "    \n",
    "    \n",
    "\n",
    "def local_merge_hist(hist1, hist2):\n",
    "    \n",
    "    local_hist= copy.deepcopy(hist1)\n",
    "    \n",
    "    for i in hist2.keys():\n",
    "        if i in local_hist.keys():\n",
    "            local_hist[i][2] = local_hist[i][2] +hist2[i][2]\n",
    "            local_hist[i][1]= max(local_hist[i][1], hist2[i][1])\n",
    "        else:\n",
    "            local_hist.update( {i: hist2[i]}  )\n",
    "    return local_hist\n",
    "    \n",
    "\n",
    "def split_onOneFeature_hist( values_hist,node_data_cnt, node_hist):\n",
    "    \n",
    "    left_count=0\n",
    "    right_count= node_data_cnt\n",
    "    \n",
    "#     if len(values_hist)<=1:\n",
    "#         return [-1,-1,-1,-1]\n",
    "\n",
    "    #this is a bit-record variable\n",
    "    leftSplit_valueSet = 0 \n",
    "    leftSplit_count = 0\n",
    "    leftSplit_hist= {}\n",
    "    rightSplit_hist={}\n",
    "        \n",
    "    bestLeft=(-1,-1)\n",
    "    bestRight=(-1,-1)  \n",
    "    bestSplitMetric =0 \n",
    "    \n",
    "    \n",
    "    #sort feature values according to median \n",
    "    sorted_value_map=[]\n",
    "    \n",
    "    for i in values_hist.keys():\n",
    "        currentVal=i\n",
    "        currentVal_hist_count= values_hist[i]['count_inFeatureValue']\n",
    "        currentVal_hist= values_hist[i]['hist_inFeatureValue']\n",
    "        \n",
    "        #debug\n",
    "        if len(currentVal_hist) > bin_num:\n",
    "            print '!!! histogrm bin number error !!!'\n",
    "        \n",
    "        sorted_value_map.append((currentVal,MAD_hist(currentVal_hist,currentVal_hist_count)))\n",
    "            \n",
    "    sorted_value_map=sorted(sorted_value_map, key= lambda val: val[1][0] )\n",
    "    values_cnt= len(sorted_value_map)\n",
    "    \n",
    "    #--------- initialize the value set ----------------------\n",
    "    \n",
    "    # check the number of feature values\n",
    "    if values_cnt <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    \n",
    "    current_feature_value = sorted_value_map[0][0] \n",
    "    currentVal_count= values_hist[current_feature_value]['count_inFeatureValue']\n",
    "    currentVal_hist= values_hist[current_feature_value]['hist_inFeatureValue']\n",
    "    \n",
    "    #debug\n",
    "#     if len(currentVal_hist) > bin_num:\n",
    "#         print '!!! histogrm bin number error !!!'\n",
    "\n",
    "    leftSplit_count = leftSplit_count+currentVal_count     \n",
    "    leftSplit_hist=local_merge_hist( leftSplit_hist, currentVal_hist)\n",
    "    \n",
    "    # histograms for the right values\n",
    "    rightSplit_hist={}\n",
    "    tmp_value=0\n",
    "    for j in range(0, values_cnt-1 ):\n",
    "        tmp_value = sorted_value_map[values_cnt-1-j][0]\n",
    "        rightSplit_hist= local_merge_hist( rightSplit_hist, values_hist[tmp_value]['hist_inFeatureValue'])      \n",
    "            \n",
    "    left = MAD_hist( leftSplit_hist, leftSplit_count)\n",
    "    right= MAD_hist( rightSplit_hist, (node_data_cnt - leftSplit_count)) \n",
    "        \n",
    "    #debug\n",
    "#     tmpcnt=0\n",
    "#     for i in leftSplit_hist.keys():\n",
    "#         tmpcnt= tmpcnt+ leftSplit_hist[i][2]\n",
    "#     if tmpcnt!= leftSplit_count:\n",
    "#         print '++++ problem in left split histogram', tmpcnt, leftSplit_count\n",
    "#     tmpcnt=0\n",
    "#     for i in rightSplit_hist.keys():\n",
    "#         tmpcnt= tmpcnt+ rightSplit_hist[i][2]\n",
    "#     if tmpcnt!= (node_data_cnt - leftSplit_count):\n",
    "#         print '???? problem in right split histogram', tmpcnt, (node_data_cnt - leftSplit_count)\n",
    "    \n",
    "    \n",
    "    leftMedian= left[0]\n",
    "    leftMetric= left[1]\n",
    "    rightMedian= right[0]\n",
    "    rightMetric= right[1]\n",
    "        \n",
    "# #         debug\n",
    "#     if leftMedian<0 or rightMedian<0 or leftMetric <=0 or rightMetric <=0:\n",
    "#         print '$$$$ problem in MAD calculation',leftMedian,rightMedian,leftMetric,rightMetric,\\\n",
    "#         leftSplit_count,(node_data_cnt - leftSplit_count),\\\n",
    "#         current_feature_value\n",
    "            \n",
    "    current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "    bestSplitMetric=current_splitMetric     \n",
    "    bestLeft=left\n",
    "    bestRight=right\n",
    "    #bit variable to record which value is chosen for the left child\n",
    "    leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "    \n",
    "\n",
    "    \n",
    "    split_pos=0\n",
    "    \n",
    "    \n",
    "    #---------------------------------------------------------------\n",
    "\n",
    "    # scan the sorted feature values\n",
    "    for k in range(1,values_cnt-1):\n",
    "\n",
    "        current_feature_value=sorted_value_map[k][0]\n",
    "        currentVal_count= values_hist[current_feature_value]['count_inFeatureValue']\n",
    "        currentVal_hist= values_hist[current_feature_value]['hist_inFeatureValue']\n",
    "        \n",
    "        \n",
    "        #debug\n",
    "#         if len(currentVal_hist) > bin_num:\n",
    "#             print '!!! histogrm bin number error !!!'\n",
    "        \n",
    "\n",
    "#         histograms for the left values\n",
    "        leftSplit_count = leftSplit_count+currentVal_count     \n",
    "        leftSplit_hist=local_merge_hist( leftSplit_hist, currentVal_hist)\n",
    "    \n",
    "#         histograms for the right values\n",
    "        rightSplit_hist={}\n",
    "        tmp_value=0\n",
    "        for j in range(0, values_cnt-1-k ):\n",
    "            tmp_value = sorted_value_map[values_cnt-1-j][0]\n",
    "            rightSplit_hist= local_merge_hist( rightSplit_hist, values_hist[tmp_value]['hist_inFeatureValue'])      \n",
    "            \n",
    "        left = MAD_hist( leftSplit_hist, leftSplit_count)\n",
    "        right= MAD_hist( rightSplit_hist, (node_data_cnt - leftSplit_count)) \n",
    "        \n",
    "        #debug\n",
    "#         tmpcnt=0\n",
    "#         for i in leftSplit_hist.keys():\n",
    "#             tmpcnt= tmpcnt+ leftSplit_hist[i][2]\n",
    "#         if tmpcnt!= leftSplit_count:\n",
    "#             print '++++ problem in left split histogram', tmpcnt, leftSplit_count\n",
    "#         tmpcnt=0\n",
    "#         for i in rightSplit_hist.keys():\n",
    "#             tmpcnt= tmpcnt+ rightSplit_hist[i][2]\n",
    "#         if tmpcnt!= (node_data_cnt - leftSplit_count):\n",
    "#             print '???? problem in right split histogram', tmpcnt, (node_data_cnt - leftSplit_count)\n",
    "                 \n",
    "        leftMedian= left[0]\n",
    "        leftMetric= left[1]\n",
    "        rightMedian= right[0]\n",
    "        rightMetric= right[1]\n",
    "        \n",
    "#         #debug\n",
    "#         if leftMedian<0 or rightMedian<0 or leftMetric <=0 or rightMetric <=0:\n",
    "#             print '$$$$ problem in MAD calculation',leftMedian,rightMedian,leftMetric,rightMetric,\\\n",
    "#             leftSplit_count,(node_data_cnt - leftSplit_count),\\\n",
    "#             current_feature_value\n",
    "            \n",
    "        current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "        if current_splitMetric < bestSplitMetric:\n",
    "            \n",
    "            bestSplitMetric=current_splitMetric\n",
    "            bestLeft=left\n",
    "            bestRight=right\n",
    "            split_pos = k\n",
    "            \n",
    "    for k in range(1, split_pos+1):\n",
    "        leftSplit_valueSet=leftSplit_valueSet+ (1<<sorted_value_map[k][0])        \n",
    "    \n",
    "    return [leftSplit_valueSet, bestLeft[0], bestRight[0], bestSplitMetric ]\n",
    "\n",
    "\n",
    "def find_bestSplit_hist(local_aggre_nodes, current_NumNodes, nodes_tree, \\\n",
    "                        nodes_tree_test):\n",
    "        \n",
    "    #debug \n",
    "#     tmpnode_cnt=[]\n",
    "#     presplit=[]\n",
    "    \n",
    "    local_node_idx=0\n",
    "    \n",
    "#------------- grow the unbalanced tree------------------------    \n",
    "    cur_node_num= len(nodes_tree)\n",
    "\n",
    "    for i in range(0, current_NumNodes):\n",
    "\n",
    "        #------------- grow the unbalanced tree ---------------------------------- \n",
    "        if cur_node_num >= 3:\n",
    "            parent_node = int(math.ceil(  (i + cur_node_num -2.0)*1.0/ 2 ))\n",
    "            parent_node_esti =nodes_tree_test[2* parent_node +1 ] \n",
    "            tmp_split_feature= nodes_tree[parent_node][0]\n",
    "            \n",
    "            if tmp_split_feature == -1:\n",
    "                nodes_tree.append(  (-1,  -1  )  )\n",
    "                nodes_tree_test.append( parent_node_esti  )\n",
    "                nodes_tree_test.append( parent_node_esti )\n",
    "                continue\n",
    "            else:\n",
    "                while i!= local_aggre_nodes[local_node_idx][0]:\n",
    "                    local_node_idx=local_node_idx+1\n",
    "                    \n",
    "#                 print '!!!  possible problem !!!!'\n",
    "        #-------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "#         if local_aggre_nodes[ current_idx_nodes ][0] != i:\n",
    "#             nodes_layer.append( (-1,-1) )\n",
    "#             nodes_tree.append( (-1,-1) )\n",
    "#             continue\n",
    " \n",
    "        node_count= local_aggre_nodes[local_node_idx][1]['count'] \n",
    "        node_hist=  local_aggre_nodes[local_node_idx][1]['hist']           \n",
    "        \n",
    "#         (median, meanAbsDevi)\n",
    "        \n",
    "#      ????\n",
    "        tmp_metric= MAD_hist( node_hist, node_count)\n",
    "        best_split_sofar= tmp_metric[1]\n",
    "        best_split=(-1,-1,-1,-1)\n",
    "        \n",
    "        #debug\n",
    "#         tmpnode_cnt.append(node_count)\n",
    "#         presplit.append( best_splitMetric_feature[1]/100000 )\n",
    "         \n",
    "        best_split_feature=-1\n",
    "        best_split_featureValueSet=-1\n",
    "\n",
    "        for j in range(0,numFeatures):\n",
    "                     \n",
    "            statisticToValues = local_aggre_nodes[local_node_idx][1][j]\n",
    "\n",
    "            split=split_onOneFeature_hist(statisticToValues,node_count, node_hist)\n",
    "            \n",
    "            #debug\n",
    "#             print split[1],split[2]\n",
    "#             [leftSplit_valueSet, bestLeft[0],bestRight[0], bestSplitMetric ]\n",
    "\n",
    "            #debug\n",
    "#             if len(nodes_tree) ==0:\n",
    "#                 print 'bug check: ', split\n",
    "    \n",
    "            if  split[0]!=-1 and  split[3] < best_split_sofar:\n",
    "                best_split=split\n",
    "                best_split_sofar=split[3]\n",
    "                best_split_feature = j\n",
    "                best_split_featureValueSet= split[0]\n",
    "         \n",
    "        \n",
    "        local_node_idx=local_node_idx+1\n",
    "        \n",
    "        #------------- grow the unbalanced tree ---------------------------------- \n",
    "        if best_split_feature == -1:\n",
    "            nodes_tree.append(  (-1,  -1  )  )\n",
    "            nodes_tree_test.append( tmp_metric[0]  )\n",
    "            nodes_tree_test.append( tmp_metric[0] )\n",
    "            #debug\n",
    "#             print 'chosen:', tmp_metric[0],tmp_metric[0]\n",
    "            continue\n",
    "        #-------------------------------------------------------------------------        \n",
    "   \n",
    "        #debug \n",
    "#         if len(nodes_tree) ==217:\n",
    "#             print '--- debug---', best_split_feature,  best_split_featureValueSet,\\\n",
    "#             (local_aggre_nodes[i][1][0].keys()),(local_aggre_nodes[i][1][1].keys()),\\\n",
    "#             (local_aggre_nodes[i][1][2].keys())\n",
    "            \n",
    "#         if len(nodes_tree) == 435:\n",
    "#             print '--- debug---', best_split_feature,  best_split_featureValueSet,\\\n",
    "#             (local_aggre_nodes[i][1][0].keys()),(local_aggre_nodes[i][1][1].keys()),\\\n",
    "#             (local_aggre_nodes[i][1][2].keys())\n",
    "            \n",
    "#         if len(nodes_tree) == 872:\n",
    "#             print '--- debug---', best_split_feature,  best_split_featureValueSet,\\\n",
    "#             (local_aggre_nodes[i][1][0].keys()),(local_aggre_nodes[i][1][1].keys()),\\\n",
    "#             (local_aggre_nodes[i][1][2].keys())\n",
    "\n",
    "        #debug\n",
    "#         if current_NumNodes == 512:\n",
    "#             print '----- debug ----', len(nodes_tree), best_split_feature,best_split_featureValueSet,(local_aggre_nodes[i][1][0].keys()),\\\n",
    "#             (local_aggre_nodes[i][1][1].keys()),(local_aggre_nodes[i][1][2].keys())\n",
    "#             if len(local_aggre_nodes[i][1][0])==1 and len(local_aggre_nodes[i][1][1])==1 and len(local_aggre_nodes[i][1][2])==1:\n",
    "#                 print '-------------',local_aggre_nodes[i][1][0].keys(),local_aggre_nodes[i][1][1].keys(),local_aggre_nodes[i][1][2].keys(),\\\n",
    "#                 'split:', best_split_feature,  best_split_featureValueSet\n",
    "\n",
    "        #debug\n",
    "#         print 'chosen:', best_split[1],best_split[2]\n",
    "\n",
    "        # split on each node\n",
    "        nodes_tree.append(  (best_split_feature,  best_split_featureValueSet  )      )\n",
    "        \n",
    "        # tree for predicting\n",
    "        nodes_tree_test.append(best_split[1])\n",
    "        nodes_tree_test.append(best_split[2])\n",
    "    \n",
    "#     \n",
    "#     if local_node_idx != len(local_aggre_nodes):\n",
    "#         print '!!!! debug !!!!: not all local nodes are processed ', local_node_idx,len(local_aggre_nodes)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#statistic_infor_check for distributed information collection\n",
    "\n",
    "def statistic_infor_check(local_aggre_nodes):\n",
    "    \n",
    "    numNode = len(local_aggre_nodes)\n",
    "#     print numNode\n",
    "    tmpsum=0\n",
    "    for i in range(0, numNode):\n",
    "        tmpsum=tmpsum+local_aggre_nodes[i][1]['count']\n",
    "        \n",
    "#     print tmpsum\n",
    "    if tmpsum != 98974:\n",
    "        print 'problem in node count'\n",
    "\n",
    "    tmpsum=0\n",
    "    for i in range(0, numNode):\n",
    "        #check node hist\n",
    "        node_count = local_aggre_nodes[i][1]['count']\n",
    "        tmpsum=0\n",
    "        for j in local_aggre_nodes[i][1]['hist'].keys():\n",
    "            tmpsum= tmpsum+local_aggre_nodes[i][1]['hist'][j][2]\n",
    "        if tmpsum!= node_count:\n",
    "            print '$$$$ problem in the histogram of node'\n",
    "            \n",
    "        #check feature values\n",
    "        for j in range(0, numFeatures):\n",
    "            tmpsum=0\n",
    "            for k in local_aggre_nodes[i][1][j].keys():\n",
    "                value_hist= local_aggre_nodes[i][1][j][k]['hist_inFeatureValue']\n",
    "                value_cnt = local_aggre_nodes[i][1][j][k]['count_inFeatureValue']\n",
    "                tmpsum= tmpsum+value_cnt\n",
    "            \n",
    "#                 print len(value_hist)\n",
    "                if len(value_hist) > bin_num:\n",
    "                    print '???? bin num wrong', len(value_hist)\n",
    "            \n",
    "                #check feature-value hist\n",
    "                tmpsum1=0\n",
    "                for m in value_hist.keys():\n",
    "                    tmpsum1= tmpsum1 + value_hist[m][2]\n",
    "                if tmpsum1 != value_cnt:\n",
    "                    print '++++ problem in feature-value histogram', tmpsum1, value_cnt\n",
    "            \n",
    "            if tmpsum != node_count:\n",
    "                print '---- problem !!!! in feature:', tmpsum, node_count    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# def tree_test_mapFunc_median(line):\n",
    "#     tmpnode = search_nodeToData(line[1:numFeatures+1], node_split)\n",
    "#     return (leaf_nodes[ tmpnode ]  - line[0])*(leaf_nodes[ tmpnode ]  - line[0])\n",
    "\n",
    "# def tree_test_mapFunc(line):\n",
    "#     tmpnode = search_nodeToData(line[1:numFeatures+1], node_split)\n",
    "#     return (leaf_nodes[ tmpnode ]  - line[0])*(leaf_nodes[ tmpnode ]  - line[0])\n",
    "    \n",
    "# def tree_test( testData_rdd ):\n",
    "#     err_rdd=testData_rdd.map( lambda line: tree_test_mapFunc(line))\n",
    "#     err_sum = err_rdd.reduce( lambda a,b: a+b)\n",
    "#     return err_sum / testData_rdd.count()\n",
    "\n",
    "\n",
    "dta_train= dta_train.repartition(number_workers)\n",
    "print dta_train.getNumPartitions()\n",
    "print dta_train.first()\n",
    "\n",
    "# variables for the tree structure \n",
    "node_split=[]\n",
    "node_test=[]\n",
    "\n",
    "test_err = []\n",
    "train_err = []\n",
    "run_time=[]\n",
    "\n",
    "ltime=[]\n",
    "ctime=[]\n",
    "\n",
    "tree_history_split=[]\n",
    "tree_history_esti=[]\n",
    "tree_history_leaf=[]\n",
    "tree_history_runtime=[]\n",
    "\n",
    "#ini parameter on the cluster\n",
    "currentNode_split_fromMaster = sc.broadcast([])\n",
    "\n",
    "#prepare feature value set\n",
    "# feature_valueSet = data_featureValues_collect( dta_train )\n",
    "# feature_valueList=[]\n",
    "# for i in range(0, len(feature_valueSet)):\n",
    "#     feature_valueList.append( ( i, list(feature_valueSet[i][1]) ) )\n",
    "\n",
    "#     tune the starting depth\n",
    "for i in range(maxdepth,maxdepth+1):\n",
    "    \n",
    "    del node_split[:]\n",
    "    del node_test[:]\n",
    "    currentNode_split_fromMaster = sc.broadcast([])\n",
    "    \n",
    "    for cur_depth in range(0,i):\n",
    "        \n",
    "        start_cluster = time.time() \n",
    "        \n",
    "        current_NumNodes= int( math.pow(2,  cur_depth)) \n",
    "        statis_partitions = dta_train.mapPartitions( partition_combiner_hist )\n",
    "        \n",
    "        aggre_nodes =statis_partitions.reduceByKey(lambda statis_partition_1, statis_partition_2: \n",
    "                              merge_parttion_combiner_hist(statis_partition_1,statis_partition_2 ))\n",
    "        \n",
    "        local_aggre_nodes= aggre_nodes.collect()\n",
    "        \n",
    "        #debug\n",
    "#         statistic_infor_check(local_aggre_nodes)\n",
    "\n",
    "        #debug\n",
    "#         print len(local_aggre_nodes) , current_NumNodes\n",
    "        \n",
    "        end_cluster = time.time()\n",
    "        \n",
    "        find_bestSplit_hist( sorted(local_aggre_nodes,key= lambda val:val[0])\\\n",
    "                                    ,current_NumNodes, node_split, node_test)\n",
    "        end_local= time.time()\n",
    "        \n",
    "        #debug\n",
    "#         statistic_infor_check(local_aggre_nodes)\n",
    "    \n",
    "        ctime.append(  end_cluster- start_cluster )\n",
    "        ltime.append(  end_local - end_cluster )\n",
    "    \n",
    "        currentNode_split_fromMaster = sc.broadcast(node_split)\n",
    "        print \"time consumption ratio\", i,\":\",  end_cluster-start_cluster, end_local-end_cluster\n",
    "    \n",
    "    \n",
    "    leaf_nodes = node_test[ len(node_test)-  (int)( math.pow(2,i)): len(node_test)  ]\n",
    "    \n",
    "    tmp_test_err= 0 \n",
    "    tmp_train_err= 0\n",
    "\n",
    "    tree_history_split.append(copy.deepcopy(node_split) )\n",
    "    tree_history_esti.append( copy.deepcopy(node_test)  )\n",
    "    tree_history_leaf.append( copy.deepcopy(leaf_nodes)  )\n",
    "        \n",
    "    \n",
    "    print ctime\n",
    "    print ltime\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# training and testing process: extract all the feature-value combinations \n",
    "# in the training data set\n",
    "\n",
    "total_featureVal_set=[]\n",
    "for i in range(0, numFeatures):\n",
    "    featureValues = dta_train.map(lambda line: line[1+i]).distinct().collect()\n",
    "    total_featureVal_set.append( featureValues)\n",
    "\n",
    "print total_featureVal_set    \n",
    "    \n",
    "print 'done'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LAD result statistic : test error\n",
    "\n",
    "#---------------- LAD -------------------  data backup\n",
    "lad_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "lad_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "lad_tree_history_leaf =  copy.deepcopy( tree_history_leaf)\n",
    "lad_tree_history_runtime =  copy.deepcopy( tree_history_runtime)\n",
    "\n",
    "# print len(lad_tree_history_split)\n",
    "# print len(lad_tree_history_split[0])\n",
    "# print len(lad_tree_history_esti)\n",
    "# print len(lad_tree_history_esti[0])\n",
    "\n",
    "# print len(lad_tree_history_leaf)\n",
    "# print len(lad_tree_history_leaf[0])\n",
    "\n",
    "\n",
    "pre=0\n",
    "tree_esti = lad_tree_history_esti[0]\n",
    "tree_split =  lad_tree_history_split[ 0 ]\n",
    "err_depth=[]\n",
    "\n",
    "for i in range(1,maxdepth+1):\n",
    "            \n",
    "    end_bound= pre+ (int)( math.pow(2,i)) \n",
    "    test_leaf_nodes = tree_esti[ pre: end_bound  ]\n",
    "    pre= end_bound        \n",
    "    test_tree_split= tree_split[ 0:(int)( math.pow(2,i)-1)  ]\n",
    "    \n",
    "#     print len(test_tree_split)\n",
    "#     print len(test_leaf_nodes)\n",
    "\n",
    "    err_depth.append(test_error_tree( i, dta_test)  ) \n",
    "    \n",
    "\n",
    "print err_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data inspection for the nodes of the trained tree\n",
    "\n",
    "def feature_value_extract( valueSet):\n",
    "    \n",
    "    tmpval=1\n",
    "    tmp_valSet= valueSet\n",
    "    valSet_list=[]\n",
    "    cnt=0\n",
    "    \n",
    "    while tmpval <= tmp_valSet:\n",
    "        if tmpval & tmp_valSet !=0:\n",
    "            valSet_list.append(cnt)\n",
    "        tmpval=tmpval<<1\n",
    "        cnt=cnt+1\n",
    "        \n",
    "    return valSet_list\n",
    "\n",
    "#????\n",
    "def training_data_summary( train_rdd ):\n",
    "\n",
    "#     always remove outliers before\n",
    "    train_rdd_clean= train_rdd.filter(lambda line: line[0]<1000)\n",
    "    distinct_featureVals = train_rdd_clean.map(lambda line: (line[1], line[2],line[3],line[4] ) ).distinct().collect()\n",
    "   \n",
    "    featureConfig_val={}\n",
    "    for i in distinct_featureVals:\n",
    "        \n",
    "        tmprdd=train_rdd_clean.filter(lambda line:(line[1], line[2],line[3],line[4])  ==i ).map(lambda line: line[0])\n",
    "        tmpval=tmprdd.reduce(lambda a,b:a+b)\n",
    "        tmpval= tmpval/tmprdd.count()*1.0\n",
    "        \n",
    "        featureConfig_val.update( {i:tmpval} )\n",
    "    \n",
    "#     print featureConfig_val\n",
    "    return featureConfig_val\n",
    "        \n",
    "\n",
    "def config_check( featureVal_set, feature_num):\n",
    "    \n",
    "    \n",
    "    for i in range(0, feature_num):\n",
    "        if len(featureVal_set[i]) >1 :\n",
    "            return -1\n",
    "    return 1\n",
    "\n",
    "def lookup_featureConfig_val( configToVal_dict, value_set, feature_num):\n",
    "    \n",
    "    tmptuple=()\n",
    "    for i in range(0, feature_num):\n",
    "        tmptuple = tmptuple + ( value_set[i][0]  ,)\n",
    "#     print '++++++++++++', tmptuple\n",
    "    return configToVal_dict[tmptuple]\n",
    "\n",
    "\n",
    "\n",
    "def test_error_indiviMapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], test_tree_split)\n",
    "    return (test_leaf_nodes[ tmpnode ], line[0])\n",
    "    \n",
    "def test_error_mapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], test_tree_split)\n",
    "    \n",
    "#     return ( test_leaf_nodes[ tmpnode ],  line[0]   )\n",
    "    return (test_leaf_nodes[ tmpnode ]  - line[0])*(test_leaf_nodes[ tmpnode ]  - line[0])\n",
    "    \n",
    "def test_error_tree( current_depth, testRDD  ):\n",
    "    \n",
    "    test_cnt= testRDD.count()\n",
    "    err_rdd=testRDD.map( lambda line: test_error_mapFunc(line))\n",
    "    err_sum = err_rdd.reduce( lambda a,b: a+b)\n",
    "    \n",
    "    print 'test error at depth',current_depth, ':', sqrt(err_sum*1.0/test_cnt)/(ymax-ymin)*1.0\n",
    "    return sqrt(err_sum*1.0/test_cnt)/(ymax-ymin)*1.0\n",
    "    \n",
    "def bfs_tree(tree, total_featureVal_set, depth, tree_esti, trainRDD ):\n",
    "    \n",
    "    nodeNum=len(tree)   \n",
    "    if nodeNum == 0:\n",
    "        return\n",
    "    \n",
    "    feature_ConfigToVal = training_data_summary( trainRDD )\n",
    "    \n",
    "    interNode_cnt=0\n",
    "    non_leaf_cnt=0\n",
    "    recog_config_cnt=0\n",
    "    \n",
    "    qu=[]\n",
    "    current_depth=0\n",
    "    qu.append((0,total_featureVal_set,0))\n",
    "    \n",
    "    \n",
    "    recog_config_cnt_depth=[]\n",
    "    recog_conf=[]\n",
    "    \n",
    "    #debug\n",
    "    test_queue=[]\n",
    "    \n",
    "    while current_depth<= depth:\n",
    "        \n",
    "        print 'non_leaf_node   count at previous depth', non_leaf_cnt\n",
    "        \n",
    "        print 'depth', current_depth,':', len(qu)\n",
    "        tmpqu=[]\n",
    "        \n",
    "        non_leaf_cnt=0\n",
    "        \n",
    "        for i in qu:\n",
    "            current_nodeIdx= i[0] \n",
    "            current_featureVal_set= i[1]\n",
    "            \n",
    "            split_feature= tree[current_nodeIdx][0]\n",
    "            split_valueSet= tree[current_nodeIdx][1]\n",
    "                 \n",
    "            if split_feature == -1: \n",
    "                \n",
    "                if config_check( current_featureVal_set, numFeatures) ==1:\n",
    "                    recog_config_cnt= recog_config_cnt+1\n",
    "                    \n",
    "                    tmpval=lookup_featureConfig_val( feature_ConfigToVal, current_featureVal_set, featureNum)\n",
    "                    \n",
    "                    print current_nodeIdx,current_featureVal_set, 'has the estimate:',\\\n",
    "                    tree_esti[ current_nodeIdx*2+1],'true:',tmpval\n",
    "                    #result\n",
    "                    recog_conf.append( (tree_esti[ current_nodeIdx*2+1], tmpval) )\n",
    "                    \n",
    "                else:\n",
    "                    print current_nodeIdx,current_featureVal_set, 'has the estimate:',\\\n",
    "                    tree_esti[ current_nodeIdx*2+1],'!!! not seperated !!!'\n",
    "                    \n",
    "#                     print '!!! debug !!!', current_nodeIdx,tree[current_nodeIdx], current_featureVal_set\n",
    "                \n",
    "                interNode_cnt=interNode_cnt+1\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                non_leaf_cnt=non_leaf_cnt+1\n",
    "                \n",
    "                featureValue_list = feature_value_extract( split_valueSet)\n",
    "                \n",
    "                print current_nodeIdx, current_featureVal_set,'non-leaf node,', split_feature, featureValue_list\n",
    "                \n",
    "                \n",
    "            #debug\n",
    "#             if current_depth == 9:\n",
    "#                 print '+++ debug ++++',current_nodeIdx,tree[current_nodeIdx], current_featureVal_set, \\\n",
    "#                 featureValue_list,split_feature\n",
    "    \n",
    "            left_featureValue_set=[]\n",
    "            right_featureValue_set=[]\n",
    "            featureNum= len(current_featureVal_set)\n",
    "    \n",
    "            for i in range(0, numFeatures):\n",
    "                if i != split_feature:\n",
    "                    left_featureValue_set.append(  copy.deepcopy(current_featureVal_set[i])   )\n",
    "                    right_featureValue_set.append(  copy.deepcopy(current_featureVal_set[i])   )\n",
    "                else:\n",
    "                    left_featureValue_set.append(  copy.deepcopy(featureValue_list )   )\n",
    "                    right_featureValue_set.append(copy.deepcopy(list(set(current_featureVal_set[i] )-set(featureValue_list)))   )\n",
    "            \n",
    "            \n",
    "#             #debug\n",
    "#             if current_depth==8:\n",
    "#                 if left_featureValue_set== [[2], [11, 13], [103]] or \\\n",
    "#                 right_featureValue_set==right_featureValue_set:\n",
    "#                     print '????? debug ????', current_nodeIdx,left_featureValue_set,right_featureValue_set\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            tmpqu.append(  (current_nodeIdx*2+1,left_featureValue_set, current_nodeIdx)  )\n",
    "            tmpqu.append(  (current_nodeIdx*2+2,right_featureValue_set,current_nodeIdx) )\n",
    "        \n",
    "        qu=copy.deepcopy(tmpqu)\n",
    "        current_depth= current_depth+1\n",
    "        \n",
    "        #result\n",
    "        recog_config_cnt_depth.append( recog_config_cnt )\n",
    "    \n",
    "    \n",
    "# print out leaf nodes\n",
    "\n",
    "    print 'non_leaf_node count at previous depth', non_leaf_cnt\n",
    "    print 'bottom depth', current_depth,':', len(qu)\n",
    "    \n",
    "    for i in qu:\n",
    "        current_nodeIdx= i[0] \n",
    "        current_featureVal_set= i[1]\n",
    "        parent_nodeIdx= i[2]\n",
    "        \n",
    "#         debug \n",
    "#         print '----debug----', current_featureVal_set\n",
    "        \n",
    "        if 2*parent_nodeIdx+1 == current_nodeIdx: \n",
    "#         parent_nodeIdx == (current_nodeIdx*2+1):\n",
    "            tree_esti_val= tree_esti[ parent_nodeIdx*2]\n",
    "        else:\n",
    "            tree_esti_val= tree_esti[ parent_nodeIdx*2+1] \n",
    "        \n",
    "        if config_check( current_featureVal_set, featureNum) ==1:\n",
    "            recog_config_cnt= recog_config_cnt+1\n",
    "            tmpval=lookup_featureConfig_val( feature_ConfigToVal, current_featureVal_set, featureNum)\n",
    "            \n",
    "            #result\n",
    "            recog_conf.append( (tree_esti_val, tmpval) )\n",
    "            \n",
    "            print current_nodeIdx,current_featureVal_set, 'has the estimate:', tree_esti_val,\\\n",
    "                    'true:',tmpval\n",
    "        else:\n",
    "            print current_nodeIdx,current_featureVal_set, 'has the estimate:', tree_esti_val\n",
    "    \n",
    "    #result\n",
    "    recog_config_cnt_depth.append( recog_config_cnt )\n",
    "        \n",
    "    print 'inter-node number:', interNode_cnt\n",
    "    print ' *******  number of identified configurations:', recog_config_cnt\n",
    "    \n",
    "    tmpval=0\n",
    "    for i in range(0,recog_config_cnt):\n",
    "        tmpval=tmpval+ (recog_conf[i][0]-recog_conf[i][1])*(recog_conf[i][0]-recog_conf[i][1])\n",
    "    \n",
    "    return (recog_conf, recog_config_cnt_depth, tmpval*1.0/recog_config_cnt)\n",
    "\n",
    "# len( tree_history_split)\n",
    "# len(tree_history_esti)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write leaf-nodes results to txt file\n",
    "text_file = open(\"regTree_binNumFixed.txt\", 'a')\n",
    "text_file.write('\\n \\nleaf nodes at depth ')\n",
    "text_file.write(\"%f: \\n\" % maxdepth)\n",
    "for item in leaf_nodes:\n",
    "    text_file.write(\"%f  \" % item)\n",
    "\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # training and testing data preparation\n",
    "# # sample from the original data RDD variable: dta\n",
    "\n",
    "# # outlier threshold to filter outliers in the testing dataset\n",
    "# outlier_threshold = 40000\n",
    "\n",
    "# print dta.count()\n",
    "\n",
    "\n",
    "# dta_train_all = dta.cache().sample(False, .12, 12345)\n",
    "# dta_test_all = dta.cache().sample(False, .1, 43243)\n",
    "\n",
    "# print dta_train_all.count(), dta_train_all.getNumPartitions()\n",
    "# print dta_test_all.count(),  dta_test_all.getNumPartitions()\n",
    "\n",
    "# # training data set can be set to have outerliers or not\n",
    "# dta_train = dta_train_all\n",
    "# # .filter(lambda line: line[0]<outlier_threshold )\n",
    "\n",
    "# # tmprdd= dta_train.filter(lambda line: line[0]<outlier_threshold )\n",
    "# tmprdd = dta_train.filter(lambda line: line[0]<outlier_threshold )\n",
    "# ymax = tmprdd.map(lambda line: line[0]).max()\n",
    "# ymin = tmprdd.map(lambda line: line[0]).min()\n",
    "# print ymax, ymin\n",
    "\n",
    "# # print dta_train.map(lambda line: line[0]).take(10)\n",
    "# # print dta_train.map(lambda line: line[0]).min()\n",
    "\n",
    "# # filter outliers for testing data\n",
    "# dta_test = dta_test_all.filter(lambda line: line[0]< outlier_threshold )\n",
    "# # ymax = dta_test.map(lambda line: line[0]).max()\n",
    "# # ymin = dta_test.map(lambda line: line[0]).min()\n",
    "# # print ymax, ymin\n",
    "\n",
    "# print 'number of training data instance:',dta_train.count()\n",
    "# print 'number of testing data instance:',dta_test.count()\n",
    "# print dta_train.first()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
