{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os \n",
    "import sys \n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "from datetime  import datetime \n",
    "\n",
    "import numpy as np  # learn \n",
    "import pandas as pd # learn\n",
    "from pandas import *\n",
    "from numpy import *\n",
    "\n",
    "from scipy import stats # look at scipy\n",
    "from scipy import linalg\n",
    "from scipy import *\n",
    "\n",
    " \n",
    "import matplotlib as mplt # learn matplolib \n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "import seaborn as sns \n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc={\"figure.figsize\": (14, 6)})\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import sklearn as sk\n",
    "import itertools\n",
    "\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import heapq\n",
    "from SyntheticDataGenerator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  generate synthetic dataset.\n",
    "\n",
    "# the text file for the produced dataset is stored locally\n",
    "# the meaning of the file name 'syndata_f5v5_6zeros_1percen.txt':\n",
    "# 1percet: the percentage of outerliers in the dataset\n",
    "filename = '/home/tguo/data/tian-syn/syn-10m-1p.txt'\n",
    "\n",
    "# three main parameters: rows: number of data instance, cols: number of features\n",
    "# featureValues: number of feature values for each feature \n",
    "# rows \n",
    "# cols\n",
    "# featureValues\n",
    "\n",
    "# call class SyntheticDataGenerator to generate synthetic data \n",
    "data = SyntheticDataGenerator(rows, cols,featureValues+1,0.01) \n",
    "data.writeData(filename)\n",
    "\n",
    "\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load synthetic data and reset feature-value for categorical features for the next training and testing\n",
    "\n",
    "dta_RDD = sc.textFile(\"file:///home/tguo/data/tian-syn/syn-30m.txt\")\n",
    "dta_RDD.cache()\n",
    "# number of features in the dataset\n",
    "num_cols= 10\n",
    "\n",
    "\n",
    "# function called by RDD.map to organize data instance in the form of\n",
    "#  ( dependent variable, feature1, feature2, ....)\n",
    "def cons_valueFeatures( line):\n",
    "    val_feature=[float(line[num_cols]) ]\n",
    "    for i in range(0, num_cols):\n",
    "        val_feature.append(  int(line[i]))\n",
    "    return tuple(val_feature)\n",
    "    \n",
    "dta_splited = dta_RDD.map(lambda line: line.split(\",\")).map(lambda r: cons_valueFeatures(r)) \n",
    "dta= dta_splited\n",
    "dta.cache()\n",
    "\n",
    "print 'tuple (dependent variable, features) :',dta.take(1)\n",
    "print dta.count()\n",
    "\n",
    "\n",
    "# re-set the index of categorical feature values for efficient best-split finding\n",
    "# for a categorical feature with n different values, its feature value should be\n",
    "# from 0 to n-1. (the decision tree in MLlib makes this assumption on dataset as well )\n",
    "\n",
    "# collect the raw different values of each feature\n",
    "feature_dist=[]\n",
    "for i in range(0, num_cols):\n",
    "    tmp1 = dta.map(lambda line: line[i+1]).distinct().collect()\n",
    "    print tmp1\n",
    "    feature_dist.append(tmp1)\n",
    "\n",
    "# build a map to map raw feature values to a consecutive range    \n",
    "fea_cnt= num_cols\n",
    "fea_map=[]\n",
    "for i in range(0, fea_cnt):\n",
    "    tmpcnt = len(feature_dist[i])\n",
    "    val_map= dict( zip(feature_dist[i], range(0,tmpcnt)) )\n",
    "    fea_map.append(val_map)\n",
    "    \n",
    "# reset the feature values\n",
    "def reset_index( line ):\n",
    "    tmp=[ line[0] ]\n",
    "    for i in range(0, num_cols):\n",
    "        tmp.append( fea_map[i][ line[i+1] ] )\n",
    "    return tuple(tmp)\n",
    "dta = dta.map( lambda line: reset_index(line)  )\n",
    "\n",
    "print 'feature value re-indexed:',dta.first()\n",
    "print dta.count()\n",
    "\n",
    "# check the number of different feature values for reset values.\n",
    "print 'check:'\n",
    "for i in range(0, num_cols):\n",
    "    tmp1 = dta.map(lambda line: line[i+1]).distinct().collect()\n",
    "#     print len(tmp1)\n",
    "    \n",
    "    \n",
    "#  save the synthetic data to hdfs\n",
    "# dta = dta.map(lambda line:                str(line[0])+ \",\" + str(line[1])+ \n",
    "#                               \",\" + str(line[2])+ \",\" + str(line[3])+ \n",
    "#                               \",\" + str(line[4])+ \",\" + str(line[5])+\n",
    "#                               \",\" + str(line[6])+ \",\" + str(line[7])+ \n",
    "#                               \",\" + str(line[8])+ \",\" + str(line[9])+ \n",
    "#                                                             \",\"+ str(line[10])).saveAsTextFile(\"hdfs://computer61.ant-net/user/tguo/syn30m-1p10mag.csv\")\n",
    "# print dta.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load BT data\n",
    "# dta_RDD = sc.textFile(\"hdfs://computer61.ant-net/user/tguo/udpjitter-H1april2014_regTree_mllib.csv\")\n",
    "\n",
    "\n",
    "dta_rdd = sc.textFile(\"hdfs://computer61.ant-net/user/tguo/bt_data.csv\").cache()\n",
    "# tmpfirst = dta_rdd\n",
    "# dta_rdd = dta_rdd.filter( lambda line: line!=tmfirst )\n",
    "\n",
    "# \"file:///home/tguo/data/tian-syn/syn-30m.txt\"\n",
    "\n",
    "# data type conversion and organize data of the form (dependent variable value, feature-values) \n",
    "dta_splited = dta_rdd.map(lambda line: line.split(\",\")).map(lambda r:\n",
    "                                                                (float(r[0]),\n",
    "                                                                 int( float(r[1])), \n",
    "                                                                 int(float(r[2])),\n",
    "                                                                 int(float(r[3])),\n",
    "                                                                 int(float(r[4])),int(float(r[5])),\n",
    "                                                                 int(float(r[6])),\n",
    "                                                                 int(float(r[7])),\n",
    "                                                                 int(float(r[8])),int(float(r[9])),\n",
    "                                                                 int(float(r[10]))))\n",
    "\n",
    "dta= dta_splited\n",
    "dta.cache()\n",
    "print dta.first()\n",
    "print dta.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  load flight data\n",
    "\n",
    "dta_rdd = sc.textFile(\"hdfs://computer61.ant-net/user/tguo/airline_data.csv\")\n",
    "\n",
    "dta = dta_rdd.map(lambda line: line.split(\",\")).map(lambda r:\n",
    "                                                                (float(r[0]),\n",
    "                                                                 int( float(r[1])), \n",
    "                                                                 int(float(r[2])),\n",
    "                                                                 int(float(r[3])),\n",
    "                                                                 int(float(r[4])),\n",
    "                                                                 int(float(r[5])),\n",
    "                                                                 int(float(r[6])),\n",
    "                                                                 int(float(r[7])), \n",
    "                                                                 int(float(r[8])))).cache()\n",
    "\n",
    "\n",
    "print dta.first()\n",
    "print dta.count()\n",
    "\n",
    "tmprdd =  dta.map(lambda line:line[0])\n",
    "print tmprdd.max(), tmprdd.min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load synthetic data\n",
    "# dta_RDD = sc.textFile(\"hdfs://computer61.ant-net/user/tguo/syn-30m-1p10mag.csv\",10)\n",
    "# syn-10m-0p.txt\n",
    "\n",
    "dta_rdd = sc.textFile(\"file:///home/tguo/data/tian-syn/syn-10m-1p-6z.txt\").cache()\n",
    "print dta_rdd.first()\n",
    "\n",
    "# data type conversion and organize data of the form (dependent variable value, \n",
    "# feature-values) \n",
    "dta_splited = dta_rdd.map(lambda line: line.split(\",\")).map(lambda r:\n",
    "                                                        ( float(r[5]),\n",
    "                                                        int(float(r[0])), int( float(r[1])), \n",
    "                                                        int(float(r[2])),int(float(r[3])),\n",
    "                                                        int(float(r[4]))\n",
    "                                                         ,int(float(r[5])),\n",
    "                                                        int(float(r[6])),int(float(r[7])),\n",
    "                                                        int(float(r[8]))\n",
    "                                                         ,int(float(r[9])),\n",
    "                                                        ) )\n",
    "\n",
    "\n",
    "dta= dta_splited\n",
    "print dta.first()\n",
    "print dta.count()\n",
    "print dta.getNumPartitions()\n",
    "\n",
    "# cate_map={0: 6, 1: 6, 2: 6, 3: 6, 4: 6}\n",
    "\n",
    "tmprdd= dta.map(lambda line: line[0]).cache()\n",
    "print tmprdd.max(), tmprdd.min()\n",
    "\n",
    "dta_rdd= dta\n",
    "\n",
    "cate_map = {0: 6, 1: 6, 2: 6, 3: 6, 4: 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_scale= 20\n",
    "feature_num= 10\n",
    "numFeatures = 10\n",
    "number_workers= 5\n",
    "maxdepth= 10\n",
    "bin_num= 1000\n",
    "trim_ratio = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# adjust the outlier scale\n",
    "\n",
    "def outlier_scale(line):\n",
    "    if line[0] >= 10000:\n",
    "        return  (out_scale * np.random.normal(100, 1, 1)[0], )+ line[1:feature_num+1] \n",
    "    return line\n",
    "\n",
    "dta_rdd = dta_rdd.map(lambda line: outlier_scale(line) ).cache()\n",
    "print dta_rdd.getNumPartitions()\n",
    "print dta_rdd.first()\n",
    "\n",
    "tmprdd= dta_rdd.map(lambda line: line[0]).cache()\n",
    "print tmprdd.max(), tmprdd.min()\n",
    "\n",
    "dta = dta_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cluster side operations\n",
    "# build histograms for each feature-value in each tree node of the current layer\n",
    "\n",
    "# locate the node of the regression tree to which a data instance belongs to\n",
    "# input: \n",
    "#   features: feature values of a data instance, <list>\n",
    "#   tree: is the tree structure we have trained so far, <list>\n",
    "# output: \n",
    "#   tree node id of this data instance, <integer>\n",
    "def search_nodeToData(features, tree):\n",
    "    \n",
    "    nodeNum=len(tree)   \n",
    "    if nodeNum == 0:\n",
    "        return 0\n",
    "    \n",
    "    current_nodeIdx=0\n",
    "    while current_nodeIdx< nodeNum:\n",
    "        \n",
    "#       extract the split feature and feature-value set of the current tree node\n",
    "        split_feature= tree[current_nodeIdx][0]\n",
    "        split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "#       if this tree node is not splitted, randomly continue the search path\n",
    "        if split_feature == -1:\n",
    "            current_nodeIdx=current_nodeIdx*2+1+ random.randint(0, 1)\n",
    "            continue\n",
    "#       if the feature values of this data instance is on the feature-value set of the left child\n",
    "        if (( 1<<features[split_feature]) &  split_valueSet) != 0 :\n",
    "            current_nodeIdx=current_nodeIdx*2+1\n",
    "        else:\n",
    "            current_nodeIdx=current_nodeIdx*2+2\n",
    "            \n",
    "    return current_nodeIdx - nodeNum\n",
    "\n",
    "# binary search to find the bin of the histogram which a dependent value should be insert\n",
    "# input: \n",
    "#   hist_list: histogram w.r.t a feature value, <list>\n",
    "#              the bins in histogram hist_list is maintained in the increasing order\n",
    "#   newY: the dependent variable value of a data instance to update the histogram,<float>\n",
    "# output: \n",
    "#   the bin id in the histogram to update, <integer> \n",
    "def update_hist_list_lookup( hist_list, newY ):\n",
    "    \n",
    "    cnt= len(hist_list)\n",
    "    l=0\n",
    "    r= len(hist_list)-1\n",
    "    \n",
    "    while l<r-1:\n",
    "        mid= l+ (r-l)/2\n",
    "        \n",
    "        if hist_list[mid][0]> newY:\n",
    "            r=mid\n",
    "        elif newY > hist_list[mid][1]:\n",
    "            l=mid\n",
    "        elif  hist_list[mid][0]  <= newY and newY<= hist_list[mid][1]:\n",
    "            return [1,mid]\n",
    "    \n",
    "    if hist_list[l][0]  <= newY and newY<= hist_list[l][1]:\n",
    "            return [1,l]\n",
    "    \n",
    "    if l+1<cnt and hist_list[l+1][0]  <= newY and newY<= hist_list[l+1][1]:\n",
    "            return [1,l+1]\n",
    "\n",
    "    if newY < hist_list[l][0]:\n",
    "        return [0,l]\n",
    "    \n",
    "    if l+1<cnt and newY< hist_list[ l+1 ][0]:\n",
    "        return [0,l+1]\n",
    "    elif l+1<cnt and newY> hist_list[ l+1 ][1]:\n",
    "        return [0,l+2]\n",
    "    \n",
    "    return [0,l+1]\n",
    "\n",
    "# update the histogram w.r.t. a feature value in a tree node with a data instance \n",
    "# newY: dependent variable value\n",
    "# it calls function update_hist_list_lookup( hist_list, newY ) first to find the tree node\n",
    "\n",
    "# a histogram is maintained to ensure the number of bins is no more than the fixed value bin_num(in the block below)\n",
    "# a bin in the histogram is constructed by a list [] with three elements:\n",
    "#   value at index 0: left boundary of the bin\n",
    "#   value at index 1: right boundary of the bin\n",
    "#   value at index 2: number of data instatnces in the bin\n",
    "\n",
    "# input:\n",
    "#   hist_list: a list of bins in a histogram, <list>\n",
    "#   newY: the new independent variable value to insert, <float>\n",
    "# output:\n",
    "#   no direct output, update the histogram in input\n",
    "def update_hist_list( hist_list, newY ):\n",
    "    \n",
    "    cnt= len(hist_list)\n",
    "    \n",
    "#   condition: \n",
    "#     cnt==0: if histogrm has no bins, just insert the new value\n",
    "    if cnt==0:        \n",
    "        hist_list.append([newY, newY,1,newY])\n",
    "    else:\n",
    "#       call function update_hist_list_lookup() to find the position to insert newY \n",
    "        pos=update_hist_list_lookup( hist_list, newY )\n",
    "        tmpidx=pos[1]\n",
    "        \n",
    "#       condition:\n",
    "#         pos[0]==1: if newY can be inserted to an existing bin\n",
    "#         otherwise: initialize a new bin for newY in the histogram\n",
    "        if pos[0]==1:\n",
    "            hist_list[tmpidx][2]= hist_list[tmpidx][2]+1\n",
    "            hist_list[tmpidx][3]= hist_list[tmpidx][3]+newY\n",
    "        else:\n",
    "#           initialize a new bin for newY\n",
    "            hist_list.insert(tmpidx, [ newY,newY,1, newY])\n",
    "            \n",
    "#   initizlize some variables used for bin elimination\n",
    "    tmpdis=-1\n",
    "#   indexes of two bins that could be merged\n",
    "    merge_l=0\n",
    "    merge_r=0\n",
    "    cnt=len(hist_list)\n",
    "#   initialize the minimum distance for finding the cloest bin\n",
    "    minDis =  hist_list[cnt-1][1] - hist_list[0][0]\n",
    "    \n",
    "#   if current number of bins exceeds the bin number bound, \n",
    "#   scan the bins to find the pair of bin wih minimum distance and merge them\n",
    "    if cnt> bin_num:\n",
    "        \n",
    "        for i in range(0,cnt-1):\n",
    "            tmpdis= hist_list[i+1][0]-hist_list[i][1]\n",
    "            if tmpdis<minDis:\n",
    "                merge_l=i\n",
    "                merge_r=i+1\n",
    "                minDis=tmpdis\n",
    "        \n",
    "        hist_list[merge_l][1]= max(hist_list[merge_l][1],  hist_list[merge_r][1])\n",
    "        hist_list[merge_l][2]= hist_list[merge_l][2] + hist_list[merge_r][2]\n",
    "        hist_list[merge_l][3]= hist_list[merge_l][3] + hist_list[merge_r][3]\n",
    "        hist_list.pop(merge_r)\n",
    "\n",
    "#   this function is called by mapPartition to process a data partition\n",
    "#   it builds histgorams for each feature-value of a tree node using the data locating\n",
    "#   at this node\n",
    "#   input: data partition is a list of tuples, i.e. list_dvAndfeatures\n",
    "\n",
    "#  the data structure maintained for each tree node is a hierachical structure:\n",
    "#  node - feature - value - histogram\n",
    "\n",
    "# input: the list of tuples in a data partition, <list>\n",
    "# output: an iterator, <composite list and dictionary>\n",
    "def partition_combiner_hist(list_dvAndfeatures):\n",
    "    \n",
    "#   data structure for maintaing the hisgorams for the local data partition\n",
    "    nodes_dict={}\n",
    "    \n",
    "#   iterate the data instance in data partition list_dvAndfeatures \n",
    "    for dvAndfeatures in list_dvAndfeatures:\n",
    "        \n",
    "#       Y is the dependent variable value of current data instance dvAndfeatures   \n",
    "        Y= dvAndfeatures[0]\n",
    "        \n",
    "#       find the tree node data instance dvAndfeatures belongs to, \n",
    "#       variable node is the output node id\n",
    "        node= search_nodeToData(dvAndfeatures[1:numFeatures+1],currentNode_split_fromMaster.value)\n",
    "        \n",
    "        if node in nodes_dict:\n",
    "            \n",
    "            \n",
    "#           for each tree node, i.e.,nodes_dict[node], it has three atrributes:\n",
    "#           ['sumY']: sume of Y of the data in this node\n",
    "#           ['count']: count of data instances in this node\n",
    "#           ['hist_list']: the histogram over the data of this node\n",
    "#           such attributes are used for debuging and the potential functionality extension in the future\n",
    "\n",
    "            nodes_dict[node]['sumY'] = nodes_dict[node]['sumY']+Y \n",
    "            nodes_dict[node]['count'] = nodes_dict[node]['count']+1     \n",
    "            update_hist_list( nodes_dict[node]['hist_list'], Y)\n",
    "\n",
    "#           iterate each feature-value of this data instacne to update the \n",
    "#           corresponidng histograms in nodes_dict \n",
    "            for i in range(0,numFeatures):    \n",
    "                feature_val= dvAndfeatures[i+1]\n",
    "                \n",
    "                if feature_val not in nodes_dict[node][i]:\n",
    "                    \n",
    "                    nodes_dict[node][i].update( { feature_val: {}   }  )\n",
    "                    nodes_dict[node][i][feature_val].update( {'count_inFeatureValue':1} )                \n",
    "                    nodes_dict[node][i][feature_val].update( {'hist_val_list': [] } )\n",
    "                    update_hist_list( nodes_dict[node][i][feature_val]['hist_val_list'], Y)\n",
    "                    \n",
    "                else:\n",
    "#           the associate data struture for each feature-value has two attributes:\n",
    "#           1. ['count_inFeatureValue']: recode the number of data instacne having feature_val in feature i\n",
    "#           2. ['hist_val_list']: is the histogram built on the data of this feature-value\n",
    "                    nodes_dict[node][i][feature_val]['count_inFeatureValue'] = \\\n",
    "                    nodes_dict[node][i][feature_val]['count_inFeatureValue']+1 \n",
    "                    update_hist_list( nodes_dict[node][i][feature_val]['hist_val_list'], Y)\n",
    "        else:\n",
    "            nodes_dict.update( {node: {}})  \n",
    "            nodes_dict[node].update( { 'sumY': Y} )\n",
    "            nodes_dict[node].update( { 'count': 1} )\n",
    "            nodes_dict[node].update( { 'hist_list': []   } )  \n",
    "            update_hist_list( nodes_dict[node]['hist_list'], Y)\n",
    "    \n",
    "            for i in range(0,numFeatures):\n",
    "                feature_val= dvAndfeatures[i+1]\n",
    "                \n",
    "                nodes_dict[node].update( { i: {}} )    \n",
    "                nodes_dict[node][i].update( { feature_val: {}   }  )\n",
    "                nodes_dict[node][i][feature_val].update( {'count_inFeatureValue':1} )                \n",
    "                nodes_dict[node][i][feature_val].update( {'hist_val_list': [] } )\n",
    "                \n",
    "                update_hist_list( nodes_dict[node][i][feature_val]['hist_val_list'], Y)\n",
    "          \n",
    "    return zip(nodes_dict.keys(), nodes_dict.values())\n",
    "\n",
    "\n",
    "# merge two histograms from two data partitions but w.r.t. the same feature-value of a tree node \n",
    "# this function performs in a way similar to merge_sort to ensure the order of merged bins\n",
    "# input:\n",
    "#   the bins of two histograms, <two dimensional list>\n",
    "# output:\n",
    "#   a merged histogram, <two dimensional list>\n",
    "def merge_hist_list(hist1, hist2):\n",
    "    \n",
    "#   variable storing the merged histogram\n",
    "    tmphist=[]\n",
    "#   two pointers of the list hist1 and hist2\n",
    "    p1=0\n",
    "    p2=0\n",
    "#   the number of bins in each histogram\n",
    "    c1=len(hist1)\n",
    "    c2=len(hist2)\n",
    "    cnt=0\n",
    "        \n",
    "#   initialize the merged histogram using the bin with less left boundary\n",
    "    if p1<c1 and p2<c2 and hist1[p1][0] < hist2[p2][0]:\n",
    "        tmphist.append( [0,0,0,0] )\n",
    "        tmphist[cnt][0]= hist1[p1][0]\n",
    "        tmphist[cnt][1]= hist1[p1][1]\n",
    "        tmphist[cnt][2]= hist1[p1][2]\n",
    "        tmphist[cnt][3]= hist1[p1][3]\n",
    "        \n",
    "        p1=p1+1\n",
    "        cnt=cnt+1\n",
    "    elif p1<c1 and p2<c2 and hist1[p1][0] >= hist2[p2][0] :\n",
    "        tmphist.append( [0,0,0,0] )\n",
    "        tmphist[cnt][0]= hist2[p2][0]\n",
    "        tmphist[cnt][1]= hist2[p2][1]\n",
    "        tmphist[cnt][2]= hist2[p2][2]\n",
    "        tmphist[cnt][3]= hist2[p2][3]\n",
    "        \n",
    "        p2=p2+1\n",
    "        cnt=cnt+1\n",
    "    \n",
    "#   alternatively iterate over hist1 and hist2 to merge bins into the merged histgoram tmphist\n",
    "    while p1<c1 and p2<c2:\n",
    "        \n",
    "        while p2<c2 and hist2[p2][1] <= tmphist[cnt-1][1]:\n",
    "            tmphist[cnt-1][2]= tmphist[cnt-1][2] + hist2[p2][2]\n",
    "            tmphist[cnt-1][3]= tmphist[cnt-1][3] + hist2[p2][3]\n",
    "            p2=p2+1\n",
    "\n",
    "        while p1<c1 and hist1[p1][1] <= tmphist[cnt-1][1]:\n",
    "            tmphist[cnt-1][2]= tmphist[cnt-1][2] + hist1[p1][2]\n",
    "            tmphist[cnt-1][3]= tmphist[cnt-1][3] + hist1[p1][3]\n",
    "            p1=p1+1\n",
    "        \n",
    "        if p1<c1 and p2<c2 and hist1[p1][0] < hist2[p2][0]:\n",
    "            tmphist.append( [0,0,0,0] )\n",
    "            tmphist[cnt][0]= hist1[p1][0]\n",
    "            tmphist[cnt][1]= hist1[p1][1]\n",
    "            tmphist[cnt][2]= hist1[p1][2]\n",
    "            tmphist[cnt][3]= hist1[p1][3]\n",
    "            p1=p1+1\n",
    "            cnt=cnt+1  \n",
    "                \n",
    "        elif p1<c1 and p2<c2 and hist1[p1][0] >= hist2[p2][0]:\n",
    "            tmphist.append( [0,0,0,0] )\n",
    "            tmphist[cnt][0]= hist2[p2][0]\n",
    "            tmphist[cnt][1]= hist2[p2][1]\n",
    "            tmphist[cnt][2]= hist2[p2][2]\n",
    "            tmphist[cnt][3]= hist2[p2][3]\n",
    "            p2=p2+1\n",
    "            cnt=cnt+1\n",
    "    \n",
    "    while p2<c2 and hist2[p2][1] <= tmphist[cnt-1][1]:\n",
    "            tmphist[cnt-1][2]= tmphist[cnt-1][2] + hist2[p2][2]\n",
    "            tmphist[cnt-1][3]= tmphist[cnt-1][3] + hist2[p2][3]\n",
    "            p2=p2+1\n",
    "    while p1<c1 and hist1[p1][1] <= tmphist[cnt-1][1]:\n",
    "            tmphist[cnt-1][2]= tmphist[cnt-1][2] + hist1[p1][2]\n",
    "            tmphist[cnt-1][3]= tmphist[cnt-1][3] + hist1[p1][3]\n",
    "            p1=p1+1\n",
    "    \n",
    "#   append the left bins in either histogram to the merged histogram\n",
    "    while p1<c1:\n",
    "        tmphist.append( [0,0,0,0] )\n",
    "        tmphist[cnt][0]= hist1[p1][0]\n",
    "        tmphist[cnt][1]= hist1[p1][1]\n",
    "        tmphist[cnt][2]= hist1[p1][2]\n",
    "        tmphist[cnt][3]= hist1[p1][3]\n",
    "        cnt=cnt+1\n",
    "        p1=p1+1\n",
    "    while p2<c2:\n",
    "        tmphist.append( [0,0,0,0] )\n",
    "        tmphist[cnt][0]= hist2[p2][0]\n",
    "        tmphist[cnt][1]= hist2[p2][1]\n",
    "        tmphist[cnt][2]= hist2[p2][2]\n",
    "        tmphist[cnt][3]= hist2[p2][3]\n",
    "        cnt=cnt+1\n",
    "        p2=p2+1\n",
    "\n",
    "# If merged histogram has the number of bins less than bin_num, directly return.\n",
    "# Otherwise, iterate over the bins to continusly find the pairs of bin with minimum distance and merge them\n",
    "# until the bin_num constraint is addressed. \n",
    "    cnt= len(tmphist)\n",
    "    if cnt< bin_num:\n",
    "        return tmphist\n",
    "    \n",
    "    tmpdis=[]  \n",
    "    for i in range(0, cnt-1):\n",
    "        tmpdis.append( ( abs( tmphist[i+1][0]-tmphist[i][1]), i, i+1 ) )        \n",
    "    \n",
    "    heapq.heapify(tmpdis)\n",
    "    cnt_remove= cnt- bin_num\n",
    "    bin_remove=[]\n",
    "    \n",
    "    root_bin=range(0, cnt)\n",
    "\n",
    "    for i in range(0, cnt_remove):\n",
    "        \n",
    "        tmptuple= heapq.heappop(tmpdis)\n",
    "        \n",
    "        merge_l= tmptuple[1]\n",
    "        merge_r= tmptuple[2]\n",
    "\n",
    "#       here use union-find algorithm to speed up the bin merge and removal process\n",
    "        tmproot= merge_l \n",
    "        while root_bin[ tmproot] != tmproot:\n",
    "            tmproot= root_bin[tmproot]\n",
    "        tmphist[tmproot][1]= max(tmphist[tmproot][1], tmphist[merge_r][1])\n",
    "        tmphist[tmproot][2]= tmphist[tmproot][2] + tmphist[merge_r][2]\n",
    "        tmphist[tmproot][3]= tmphist[tmproot][3] + tmphist[merge_r][3]\n",
    "        root_bin[merge_r]= tmproot    \n",
    "        \n",
    "        bin_remove.append( merge_r )\n",
    "    \n",
    "#   remove the right-side bin in a pair of merged bins\n",
    "    bin_remove.sort()\n",
    "    tmpdiff=0\n",
    "    for i in bin_remove:\n",
    "        tmphist.pop(i-tmpdiff)\n",
    "        tmpdiff=tmpdiff+1\n",
    "    return tmphist         \n",
    "\n",
    "#  this function is called by reduceByKey to merge histograms from two data partitions w.r.t. the same tree node\n",
    "#  this function calls merge_hist_list() above to merge a certain pair of histograms\n",
    "\n",
    "#  input: \n",
    "#    two node histogram structure collected by above function partition_combiner_hist(),i.e. its output,\n",
    "#    <composite list and dictionary>\n",
    "#  output:\n",
    "#    unified histograms for a tree node, <composite list and dictionary>\n",
    "def merge_parttion_combiner_hist(nodeToFeatureToValue_1,nodeToFeatureToValue_2 ):\n",
    "    \n",
    "#   merge the node attributes \n",
    "    nodeToFeatureToValue_1['sumY']= nodeToFeatureToValue_1['sumY']+\\\n",
    "    nodeToFeatureToValue_2['sumY']\n",
    "    nodeToFeatureToValue_1['count']= nodeToFeatureToValue_1['count']+\\\n",
    "    nodeToFeatureToValue_2['count'] \n",
    "    nodeToFeatureToValue_1['hist_list']=merge_hist_list(nodeToFeatureToValue_1['hist_list'],\\\n",
    "                                                   nodeToFeatureToValue_2['hist_list'])\n",
    "\n",
    "#   iterate the features to merge relevant histograms\n",
    "    for i in range(0, numFeatures): #feature\n",
    "        for j in nodeToFeatureToValue_1[i].keys(): #feature value\n",
    "            \n",
    "            feature_val=j\n",
    "            \n",
    "            if feature_val in nodeToFeatureToValue_2[i].keys():                  \n",
    "                \n",
    "                nodeToFeatureToValue_1[i][j]['hist_val_list'] = \\\n",
    "                merge_hist_list(nodeToFeatureToValue_1[i][j]['hist_val_list'], \n",
    "                           nodeToFeatureToValue_2[i][j]['hist_val_list'])     \n",
    "                \n",
    "                nodeToFeatureToValue_1[i][j]['count_inFeatureValue']=nodeToFeatureToValue_1[i][j]['count_inFeatureValue']+\\\n",
    "                nodeToFeatureToValue_2[i][j]['count_inFeatureValue']\n",
    "                \n",
    "    for i in range(0, numFeatures):\n",
    "        for j in nodeToFeatureToValue_2[i].keys():\n",
    "\n",
    "            feature_val=j\n",
    "            \n",
    "            if feature_val not in nodeToFeatureToValue_1[i].keys():\n",
    "                nodeToFeatureToValue_1[i].update({feature_val: {} })\n",
    "                nodeToFeatureToValue_1[i][feature_val]= copy.deepcopy(nodeToFeatureToValue_2[i][feature_val])\n",
    "                              \n",
    "    return  nodeToFeatureToValue_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split decision on the local side.\n",
    "\n",
    "# def approx_partial_sum(lcnt, bucket):\n",
    "#     tmpbuc= bucket\n",
    "    \n",
    "#     if tmpbuc[2]==1:\n",
    "#         return tmpbuc[3]\n",
    "    \n",
    "#     return 1.0*lcnt*tmpbuc[0]+ lcnt*(lcnt-1.0)*\\\n",
    "# ( tmpbuc[3]-tmpbuc[2]*tmpbuc[0])/ tmpbuc[2] / (tmpbuc[2]-1.0) \n",
    "\n",
    "def approx_partial_sum(lcnt, bucket):\n",
    "    tmpbuc= bucket\n",
    "    \n",
    "    if tmpbuc[2]==1:\n",
    "        return tmpbuc[3]\n",
    "    \n",
    "    if lcnt == tmpbuc[2]:\n",
    "        return tmpbuc[3]\n",
    "    if lcnt==1:\n",
    "        return tmpbuc[0]\n",
    "    if tmpbuc[2] == 2:\n",
    "        return tmpbuc[lcnt-1]\n",
    "    \n",
    "    delta=(tmpbuc[3]-(tmpbuc[2]-1.0)*tmpbuc[0]-tmpbuc[1])*1.0/((tmpbuc[2]-2.0)*\\\n",
    "                                                               (tmpbuc[2]-1.0)/2.0)\n",
    "    \n",
    "    return tmpbuc[0]+ lcnt*(1.0+lcnt)*delta/2.0+ lcnt*tmpbuc[0]\n",
    "\n",
    "def median_esti(lcnt, bucket):\n",
    "    tmpbuc= bucket\n",
    "    \n",
    "    if tmpbuc[2]==1:\n",
    "        return tmpbuc[0]\n",
    "    \n",
    "    if lcnt == tmpbuc[2]:\n",
    "        return tmpbuc[1]\n",
    "    \n",
    "    if lcnt==1:\n",
    "        return tmpbuc[0]\n",
    "#     if tmpbuc[2] == 2:\n",
    "#         return tmpbuc[lcnt-1]\n",
    "    \n",
    "    delta=(tmpbuc[3]-(tmpbuc[2]-1.0)*tmpbuc[0]-tmpbuc[1])*1.0/((tmpbuc[2]-2.0)*\\\n",
    "                                                               (tmpbuc[2]-1.0)/2.0)\n",
    "    \n",
    "    return tmpbuc[0]+ (lcnt-1.0)*delta\n",
    "\n",
    "def TLAD_histogram(hist, cnt):\n",
    "\n",
    "    num_bins= len(hist)\n",
    "    \n",
    "    bin_sum=0\n",
    "    tmp_bin_sum=0\n",
    "    curCnt=0 \n",
    "    \n",
    "    if cnt%2 ==0:\n",
    "        midCnt= cnt/2.0+1\n",
    "    else:\n",
    "        midCnt= ceil(cnt/2.0)\n",
    "    leftCnt= int(cnt*trim_ratio)\n",
    "    rightCnt= cnt - int( cnt*trim_ratio)\n",
    "    trim_ratio\n",
    "    mid_bin_idx=0\n",
    "    left_bin_idx=0\n",
    "    right_bin_idx=0\n",
    "\n",
    "    midFlag=0\n",
    "    leftFlag=0\n",
    "    rightFlag=0\n",
    "    \n",
    "    lad=0\n",
    "    median=0\n",
    "    \n",
    "    left_rcnt = 0\n",
    "    right_rcnt = 0\n",
    "    mid_rcnt = 0\n",
    "    \n",
    "    for i in range(0, num_bins): \n",
    "        \n",
    "        if curCnt+hist[i][2]>= midCnt:\n",
    "            if midFlag==0:\n",
    "                mid_bin_idx= i\n",
    "                mid_rcnt = midCnt- curCnt\n",
    "                midFlag=1\n",
    "            else:\n",
    "                if rightFlag !=1:\n",
    "                    bin_sum = bin_sum + hist[i][3]\n",
    "        elif leftFlag==1:\n",
    "            bin_sum = bin_sum - hist[i][3]\n",
    "            \n",
    "        \n",
    "        if curCnt+hist[i][2]>= leftCnt:\n",
    "            if leftFlag==0:\n",
    "                left_bin_idx= i\n",
    "                left_rcnt = leftCnt - curCnt\n",
    "                leftFlag=1, trim_ratio\n",
    "            \n",
    "        if curCnt+hist[i][2]>= rightCnt:\n",
    "            if rightFlag==0:\n",
    "                right_bin_idx= i\n",
    "                right_rcnt = rightCnt-curCnt\n",
    "                rightFlag=1\n",
    "            \n",
    "        curCnt= curCnt+hist[i][2]\n",
    "    \n",
    "    if cnt == 1:\n",
    "        return (hist[0][0],0)\n",
    "    elif cnt ==0:\n",
    "        return (0,0)\n",
    "    else:        \n",
    "        sample_inMedBin =  mid_rcnt    \n",
    "        \n",
    "        if hist[mid_bin_idx][2] ==1:\n",
    "            median = hist[mid_bin_idx][0]\n",
    "        elif hist[mid_bin_idx][2] ==2:\n",
    "            median= (hist[mid_bin_idx][0] + hist[mid_bin_idx][1])*1.0/2.0\n",
    "        else:\n",
    "            median= hist[mid_bin_idx][0]*1.0 + \\\n",
    "        (1.0*hist[mid_bin_idx][1]-1.0*hist[mid_bin_idx][0])/(hist[mid_bin_idx][2]-1)*(sample_inMedBin-1)\n",
    "        \n",
    "        \n",
    "        if cnt%2 ==0 :\n",
    "            if sample_inMedBin == 1:\n",
    "                median = (hist[mid_bin_idx-1][1] + median)/2.0\n",
    "            else:\n",
    "                median_offset = hist[mid_bin_idx][0]*1.0 + \\\n",
    "        (1.0*hist[mid_bin_idx][1]-1.0*hist[mid_bin_idx][0])/(hist[mid_bin_idx][2]-1)*(sample_inMedBin-2)\n",
    "                median = (median + median_offset)/2.0\n",
    "    \n",
    "        \n",
    "        tlad =  bin_sum + hist[mid_bin_idx][3] - 2.0*approx_partial_sum(mid_rcnt, hist[mid_bin_idx])\\\n",
    "                - (hist[left_bin_idx][3] - approx_partial_sum(left_rcnt, hist[left_bin_idx]) ) + \\\n",
    "                + approx_partial_sum(right_rcnt, hist[right_bin_idx]) \n",
    "        \n",
    "        median = median_esti(mid_rcnt, hist[mid_bin_idx])\n",
    "        \n",
    "        return (median, tlad*1.0/(cnt-2*cnt*trim_ratio))\n",
    "\n",
    "def LAD_histogram(hist, cnt):\n",
    "    num_bins= len(hist)\n",
    "    \n",
    "    bin_sum=0\n",
    "    tmp_bin_sum=0\n",
    "    curCnt=0 \n",
    "    \n",
    "    if cnt%2 ==0 :\n",
    "        midCnt= cnt/2.0+1\n",
    "    else:\n",
    "        midCnt= ceil(cnt/2.0)\n",
    "    \n",
    "    mid_bin_idx=0\n",
    "    flag=0\n",
    "    \n",
    "    lad=0\n",
    "    median=0\n",
    "    \n",
    "    cntSum_beforeMid=0\n",
    "    \n",
    "    for i in range(0, num_bins): \n",
    "        \n",
    "        if curCnt+hist[i][2]>= midCnt:\n",
    "\n",
    "            if flag==0:\n",
    "                mid_bin_idx= i\n",
    "                cntSum_beforeMid = curCnt\n",
    "                flag=1\n",
    "            else:\n",
    "                bin_sum = bin_sum + hist[i][3]\n",
    "        else:\n",
    "            bin_sum = bin_sum - hist[i][3]\n",
    "            \n",
    "        curCnt= curCnt+hist[i][2]\n",
    "    \n",
    "    if cnt == 1:\n",
    "        return (hist[0][0],0)\n",
    "    elif cnt ==0:\n",
    "        return (0,0)\n",
    "    else:        \n",
    "        sample_inMedBin =  midCnt - cntSum_beforeMid     \n",
    "        \n",
    "        if hist[mid_bin_idx][2] ==1:\n",
    "            median = hist[mid_bin_idx][0]\n",
    "        elif hist[mid_bin_idx][2] ==2:\n",
    "            median= (hist[mid_bin_idx][0] + hist[mid_bin_idx][1])*1.0/2.0\n",
    "        else:\n",
    "            median= hist[mid_bin_idx][0]*1.0 + \\\n",
    "        (1.0*hist[mid_bin_idx][1]-1.0*hist[mid_bin_idx][0])/(hist[mid_bin_idx][2]-1)*(sample_inMedBin-1)\n",
    "        \n",
    "        \n",
    "        if cnt%2 ==0 :\n",
    "            if sample_inMedBin == 1:\n",
    "                median = (hist[mid_bin_idx-1][1] + median)/2.0\n",
    "            else:\n",
    "                median_offset = hist[mid_bin_idx][0]*1.0 + \\\n",
    "        (1.0*hist[mid_bin_idx][1]-1.0*hist[mid_bin_idx][0])/(hist[mid_bin_idx][2]-1)*(sample_inMedBin-2)\n",
    "                median = (median + median_offset)/2.0\n",
    "    \n",
    "        \n",
    "#         print bin_sum,mid_bin_idx, approx_partial_sum(sample_inMedBin, hist[mid_bin_idx])\n",
    "        \n",
    "        lad =  bin_sum + hist[mid_bin_idx][3]\\\n",
    "        - 2.0*approx_partial_sum(sample_inMedBin, hist[mid_bin_idx])  \n",
    "        \n",
    "        median = median_esti(sample_inMedBin, hist[mid_bin_idx])\n",
    "        \n",
    "        return (median, lad*1.0/cnt)\n",
    "\n",
    "\n",
    "# during split selection phase, call this function to merge the histograms belonging to the feature-values\n",
    "# on the left or right side of the split. \n",
    "# the returned merged histogram by this function will be used for median/mean and loss function calculations.\n",
    "\n",
    "# input: \n",
    "#   hist1: a list of bins,<list>\n",
    "#   hist2: a list of bins,<list>\n",
    "# output:\n",
    "#   a merged hist, <list>\n",
    "def local_merge_hist_list(hist1, hist2):\n",
    "\n",
    "#   tmphist: stores the bins of merged histogram, <list>  \n",
    "    tmphist=[]\n",
    "    \n",
    "#   p1,p1: pointer for accessing the bins of hist1 and hist2,<integer>\n",
    "    p1=0\n",
    "    p2=0\n",
    "    \n",
    "#   c1, c2: number of bins in each histogram, <integer>   \n",
    "    c1=len(hist1)\n",
    "    c2=len(hist2)\n",
    "#   cnt: number of bins in the merged histogram, <integer>\n",
    "    cnt=0\n",
    " \n",
    "    last_choose=0\n",
    "    \n",
    "#   merging processing applies the idea of merge-sort and run in linear time\n",
    "#   recall that each bin in a histogram is represented by a list of three element \n",
    "#   [left boundary, right boundary, number of data instances ]\n",
    "\n",
    "#   insert the bin with left-most boundary from his1 and hist2 into the tmphist[] initially\n",
    "    if p1<c1 and p2<c2 and hist1[p1][0] < hist2[p2][0]:\n",
    "        tmphist.append( [0,0,0,0] )\n",
    "        tmphist[cnt][0]= hist1[p1][0]\n",
    "        tmphist[cnt][1]= hist1[p1][1]\n",
    "        tmphist[cnt][2]= hist1[p1][2]\n",
    "        tmphist[cnt][3]= hist1[p1][3]\n",
    "\n",
    "        p1=p1+1\n",
    "        cnt=cnt+1\n",
    "        last_choose=1\n",
    "    elif p1<c1 and p2<c2 and hist1[p1][0] >= hist2[p2][0] :\n",
    "        tmphist.append( [0,0,0,0] )\n",
    "        tmphist[cnt][0]= hist2[p2][0]\n",
    "        tmphist[cnt][1]= hist2[p2][1]\n",
    "        tmphist[cnt][2]= hist2[p2][2]\n",
    "        tmphist[cnt][3]= hist2[p2][3]\n",
    "        p2=p2+1\n",
    "        cnt=cnt+1\n",
    "        last_choose=2\n",
    "    \n",
    "#   merge-sort process  \n",
    "    while p1<c1 and p2<c2:\n",
    "        \n",
    "#       if a bin in hist1 (or hist2) is included in the last bin of the merged histogram, \n",
    "#       this bin is directd integrated into that bin of the merged histogram\n",
    "        while p2<c2 and hist2[p2][1] <= tmphist[cnt-1][1]:\n",
    "            tmphist[cnt-1][2]= tmphist[cnt-1][2] + hist2[p2][2]\n",
    "            tmphist[cnt-1][3]= tmphist[cnt-1][3] + hist2[p2][3]\n",
    "            p2=p2+1\n",
    "        while p1<c1 and hist1[p1][1] <= tmphist[cnt-1][1]:\n",
    "            tmphist[cnt-1][2]= tmphist[cnt-1][2] + hist1[p1][2]\n",
    "            tmphist[cnt-1][3]= tmphist[cnt-1][3] + hist1[p1][3]\n",
    "\n",
    "            p1=p1+1\n",
    "        \n",
    "#       choose a bin from hist1 or hist2 to append as a new bin into the merged histogram\n",
    "        if p1<c1 and p2<c2 and hist1[p1][0] < hist2[p2][0]:\n",
    "            tmphist.append( [0,0,0,0] )\n",
    "            tmphist[cnt][0]= hist1[p1][0]\n",
    "            tmphist[cnt][1]= hist1[p1][1]\n",
    "            tmphist[cnt][2]= hist1[p1][2]\n",
    "            tmphist[cnt][3]= hist1[p1][3]\n",
    "\n",
    "            p1=p1+1\n",
    "            cnt=cnt+1  \n",
    "            last_choose =1\n",
    "        elif p1<c1 and p2<c2 and hist1[p1][0] >= hist2[p2][0]:\n",
    "            tmphist.append( [0,0,0,0] )\n",
    "            tmphist[cnt][0]= hist2[p2][0]\n",
    "            tmphist[cnt][1]= hist2[p2][1]\n",
    "            tmphist[cnt][2]= hist2[p2][2]\n",
    "            tmphist[cnt][3]= hist2[p2][3]\n",
    "\n",
    "            p2=p2+1\n",
    "            cnt=cnt+1\n",
    "            last_choose =2\n",
    "\n",
    "#       if a bin in hist1 (or hist2) is included in the last bin of the merged histogram, \n",
    "#       this bin is directd integrated into that bin of the merged histogram\n",
    "    while p2<c2 and cnt>0 and hist2[p2][1] <= tmphist[cnt-1][1]:\n",
    "        tmphist[cnt-1][2]= tmphist[cnt-1][2] + hist2[p2][2]\n",
    "        tmphist[cnt-1][3]= tmphist[cnt-1][3] + hist2[p2][3]\n",
    "\n",
    "        p2=p2+1\n",
    "    while p1<c1 and cnt>0 and hist1[p1][1] <= tmphist[cnt-1][1]:\n",
    "        tmphist[cnt-1][2]= tmphist[cnt-1][2] + hist1[p1][2]\n",
    "        tmphist[cnt-1][3]= tmphist[cnt-1][3] + hist1[p1][3]\n",
    "\n",
    "        p1=p1+1\n",
    "    \n",
    "#   insert the left bins in hist1 and hist2 into the merged histogram\n",
    "    while p1<c1:\n",
    "        tmphist.append( [0,0,0,0] )\n",
    "        tmphist[cnt][0]= hist1[p1][0]\n",
    "        tmphist[cnt][1]= hist1[p1][1]\n",
    "        tmphist[cnt][2]= hist1[p1][2]\n",
    "        tmphist[cnt][3]= hist1[p1][3]\n",
    "        cnt=cnt+1\n",
    "        p1=p1+1\n",
    "        \n",
    "    while p2<c2:\n",
    "        tmphist.append( [0,0,0,0] )\n",
    "        tmphist[cnt][0]= hist2[p2][0]\n",
    "        tmphist[cnt][1]= hist2[p2][1]\n",
    "        tmphist[cnt][2]= hist2[p2][2]\n",
    "        tmphist[cnt][3]= hist2[p2][3]\n",
    "        cnt=cnt+1\n",
    "        p2=p2+1\n",
    "    \n",
    "    return tmphist\n",
    "\n",
    "# find the best split on a certain feature of a tree node\n",
    "# input:\n",
    "#   values_hist: the histograms corresponding to each value in this feature, <dictionary>\n",
    "#   node_data_cnt: the number of data instances in this tree node, i.e.,the number of data on this feature,<integer>\n",
    "#   node_hist: the histogram of this node\n",
    "# output: \n",
    "#   the best split value on this feature and t            \n",
    "#         if flag==0:\n",
    "#             tmp_curCnt=curCnt+ hist[i][2]\n",
    "#             tmp_bin_sum = bin_sum - hist[i][3]\n",
    "        \n",
    "#             if tmp_curCnt merge_hist_list>= midCnt:\n",
    "#                 flag=1       \n",
    "#                 mid_bin_idx= i\n",
    "#                 cntSum_beforeMid = curCnt\n",
    "#             else:    \n",
    "#                 curCnt=tmp_curCnt\n",
    "#                 bin_sum=tmp_bin_sum\n",
    "#         else:\n",
    "#             bin_sum= bin_sum+ hist[i][3]\n",
    "# he featur-value set for the left split, < tuple>\n",
    "#   [ leftSplit_valueSet, mean/median estimate for the data in the left split, \n",
    "#     mean/median estimate for the data in the right split, the value of the loss function on the best split ]\n",
    "def split_onOneFeature_hist( values_hist,node_data_cnt, node_hist):\n",
    "    \n",
    "#   initialize the number of data instances on the left split   \n",
    "    leftSplit_count = 0\n",
    "#   initialize the feature-value set for the left split  \n",
    "    leftSplit_valueSet = 0 \n",
    "\n",
    "#   initialize two empty historams that will be used to summarize the data on the left and right splits    \n",
    "    leftSpl1it_hist= []\n",
    "    rightSplit_hist=[]\n",
    "    \n",
    "#   two tuples for recording the median estiamte and loss function values on the left and right splits\n",
    "#   during the following split choosing process\n",
    "    bestLeft=(-1,-1)\n",
    "    bestRight=(-1,-1)\n",
    "    \n",
    "#   initialize the loss function value as zero \n",
    "    bestSplitMetric =0 \n",
    "    \n",
    "# ------- sort feature values according to median ------------------ \n",
    "    sorted_value_map=[]\n",
    "    \n",
    "#   for each feature-value, call loss function calculation function, i.e., LAD_hist_list(), \n",
    "#   to compute the median for the data with this feature-value\n",
    "    for i in values_hist:\n",
    "        \n",
    "#        <integer> \n",
    "        currentVal=i\n",
    "#       the number of data instances wit feature-value currentVal, <integer>\n",
    "        currentVal_hist_count= values_hist[i]['count_inFeatureValue']\n",
    "#       histogram of current feature-value, <list>\n",
    "        currentVal_hist= values_hist[i]['hist_val_list']\n",
    "    \n",
    "#       debug\n",
    "\n",
    "\n",
    "\n",
    "#       call function LAD_hist_list() to compute the median using the histogram of current feature-value\n",
    "#       and insert a tuple ( feature-value, median of data in this featurevalue ) into list sorted_value_map[]\n",
    "        sorted_value_map.append((currentVal,TLAD_histogram(currentVal_hist,currentVal_hist_count)))\n",
    "\n",
    "#   sort the list according to medians, <list>         \n",
    "    sorted_value_map=sorted(sorted_value_map, key= lambda val: val[1][0] )\n",
    "#   number of available values in this feature,<integer>\n",
    "    values_cnt= len(sorted_value_map)\n",
    "    \n",
    "    \n",
    "#   check the number of available feature values\n",
    "#   if it is less than 2, this feature cannot be further splited and return directly \n",
    "    if values_cnt <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    \n",
    "#--------- initialize the featur-value-set on the left split with the first feature-value ------------------\n",
    "#   insert the first feature-value in sorted value list sorted_value_map[] \n",
    "#   to the left split for initilization.\n",
    "# \n",
    "#   record the median estimate and weighted loss under this initial split\n",
    "    \n",
    "    current_feature_value = sorted_value_map[0][0] \n",
    "    currentVal_count= values_hist[current_feature_value]['count_inFeatureValue']\n",
    "    currentVal_hist= values_hist[current_feature_value]['hist_val_list']\n",
    "    \n",
    "    leftSplit_count = leftSplit_count+currentVal_count     \n",
    "    \n",
    "#   call function local_merge_hist_list() \n",
    "#   to merge the histogram of current feature-value into the histogram of the left split \n",
    "    leftSplit_hist={}\n",
    "    leftSplit_hist=local_merge_hist_list( leftSplit_hist, currentVal_hist)\n",
    "    \n",
    "#   call function local_merge_hist_list() \n",
    "\n",
    "#   merge the histograms of left feature-values into the histogram of the right split \n",
    "    rightSplit_hist={}\n",
    "    tmp_value=0\n",
    "    for j in range(0, values_cnt-1 ):\n",
    "        tmp_value = sorted_value_map[values_cnt-1-j][0]\n",
    "        rightSplit_hist= local_merge_hist_list( rightSplit_hist, values_hist[tmp_value]['hist_val_list'])      \n",
    "    \n",
    "#   call function LAD_hist_list to comupte the median and LAD of initial left and right split  \n",
    "    left = TLAD_histogram( leftSplit_hist, leftSplit_count)\n",
    "    right= TLAD_histogram( rightSplit_hist, (node_data_cnt - leftSplit_count))    \n",
    "    \n",
    "#   extract the median and LAD of each split     \n",
    "    leftMedian= left[0]\n",
    "    leftMetric= left[1]\n",
    "    rightMedian= right[0]\n",
    "    rightMetric= right[1]\n",
    "        \n",
    "#   compute the weighted loss: weight= number of data instances in a split/ total numbero of data instance       \n",
    "    current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "#   initiliaize the variables for recording the best split\n",
    "    bestSplitMetric=current_splitMetric     \n",
    "    bestLeft=left\n",
    "    bestRight=right\n",
    "    #bit variable to record which value is chosen for the left child\n",
    "    leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "    \n",
    "#   record the split position in the sorted value set\n",
    "    split_pos=0\n",
    "    \n",
    "#-------------begin to iterate over the sorted feature values to find the best split on this feature ------\n",
    "\n",
    "#   iterate over the remaining feature-values to find the best split on this feature\n",
    "    for k in range(1,values_cnt-1):\n",
    "        \n",
    "#       <integer>\n",
    "        current_feature_value = sorted_value_map[k][0]\n",
    "#       the number of data instances with current feature-value, <integer>\n",
    "        currentVal_count = values_hist[current_feature_value]['count_inFeatureValue']\n",
    "#       the histogram of current feature value, <list>\n",
    "        currentVal_hist = values_hist[current_feature_value]['hist_val_list']\n",
    "        \n",
    "#       histograms for the left values\n",
    "        leftSplit_count = leftSplit_count+currentVal_count     \n",
    "        leftSplit_hist= local_merge_hist_list( leftSplit_hist, currentVal_hist)\n",
    "    \n",
    "#       histograms for the right values\n",
    "        rightSplit_hist={}\n",
    "        tmp_value=0\n",
    "        for j in range(0, values_cnt-1-k ):\n",
    "            tmp_value = sorted_value_map[values_cnt-1-j][0]\n",
    "            rightSplit_hist= local_merge_hist_list( rightSplit_hist, values_hist[tmp_value]['hist_val_list'])      \n",
    "            \n",
    "        left = TLAD_histogram( leftSplit_hist, leftSplit_count)\n",
    "        right= TLAD_histogram( rightSplit_hist, (node_data_cnt - leftSplit_count)) \n",
    "        \n",
    "#   extract the median and LAD of left and right splits           \n",
    "        leftMedian= left[0]\n",
    "        leftMetric= left[1]\n",
    "        rightMedian= right[0]\n",
    "        rightMetric= right[1]\n",
    "        \n",
    "#   compute the weighted loss: weight= number of data instances in a split/ total numbero of data instance                       \n",
    "        current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "#   compare the current loss with the best loss sofar \n",
    "        if current_splitMetric < bestSplitMetric:\n",
    "            \n",
    "#   if this is a better one, update the loss \n",
    "            bestSplitMetric=current_splitMetric\n",
    "            bestLeft=left\n",
    "            bestRight=right\n",
    "            split_pos=k\n",
    "#   print \"finisn feature-value\",k\n",
    "            \n",
    "# build split value set\n",
    "    for k in range(1, split_pos+1):\n",
    "        leftSplit_valueSet=leftSplit_valueSet+ (1<<sorted_value_map[k][0])        \n",
    "    \n",
    "#   return the result of best split in the form of \n",
    "# [ feature-value set in the left split, median estimation for the left split, median estimation for the right split,\n",
    "#   weighted loss value], <list>\n",
    "    return [leftSplit_valueSet, bestLeft[0], bestRight[0], bestSplitMetric ]\n",
    "\n",
    "\n",
    "\n",
    "# iterate over each tree node to find the best-split for each one\n",
    "# input:\n",
    "#   local_aggre_nodes: the histograms for each tree node collected from cluster-side, <composite structure of list\n",
    "#                      and dictionary>\n",
    "#   current_NumNodes: the number of tree nodes in the current layer,<integer>\n",
    "#   nodes_tree: record the split feature and feature-values of each tree node, it will be updated during the\n",
    "#               execution of this function, <list>\n",
    "#   node_tree_test: record the estimation value for each tree node, it will be updated during the\n",
    "#                   execution of this function, <list>\n",
    "# output:\n",
    "#   the best split and estimation on each tree node in the current layer are appended to \n",
    "#   list variable nodes_tree and nodes_tree_test\n",
    "def find_bestSplit_hist(local_aggre_nodes, current_NumNodes, nodes_tree, \\\n",
    "                        nodes_tree_test):\n",
    "    \n",
    "#   index to iterate over the local histograms of tree nodes in variable local_aggre_nodes \n",
    "    local_node_idx=0\n",
    "#   the number of tree nodes already splitted and stored in node_tree\n",
    "    cur_node_num= len(nodes_tree)\n",
    "    \n",
    "#   iterate over the number of tree nodes in the current layer\n",
    "    for i in range(0, current_NumNodes):\n",
    "\n",
    "#------------- grow the unbalanced tree ---------------------------------- \n",
    "#  first, it judges whether a tree node needs to be splitted\n",
    "#  the condition is:\n",
    "#    if a tree node's parent node stops growing, then this node just inherits the estimation value from its parent \n",
    "#    node and does not need to split\n",
    "     \n",
    "#  above condition only needs to be checked for the nodes below tree depth 2\n",
    "        if cur_node_num >= 3:\n",
    "#           derive the parent_node index \n",
    "            parent_node = int(math.ceil(  (i + cur_node_num -2.0)*1.0/ 2 ))\n",
    "#           get the median estimation in the parent node and the best split feature in the parent node if any \n",
    "            parent_node_esti =nodes_tree_test[2* parent_node +1 ] \n",
    "            tmp_split_feature= nodes_tree[parent_node][0]\n",
    "        \n",
    "#           if a tree node stops growing, i.e., it is not splitted.\n",
    "#           its best-split feature is set to -1 before\n",
    "#           therefore, here tmp_split_feature is checked to see whether it is splitted or not\n",
    "            if tmp_split_feature == -1:\n",
    "#               the parent tree node is not spliited, this node does not need to grow either\n",
    "                nodes_tree.append(  (-1,  -1  )  )\n",
    "                nodes_tree_test.append( parent_node_esti  )\n",
    "                nodes_tree_test.append( parent_node_esti )\n",
    "                continue\n",
    "            else:\n",
    "#               this node needs to find best-split, then find the histograms belonging to it in the local\n",
    "#               histogram variable local_aggre_nodes\n",
    "                while i!= local_aggre_nodes[local_node_idx][0]:\n",
    "                    local_node_idx=local_node_idx+1\n",
    "                    \n",
    "#-------------search for the best split for the current node-------------   \n",
    "        \n",
    "#       the number of data instances in this tree node\n",
    "        node_count= local_aggre_nodes[local_node_idx][1]['count']\n",
    "#       a global histogram over the data of this tree node\n",
    "        node_hist=  local_aggre_nodes[local_node_idx][1]['hist_list']           \n",
    "        \n",
    "#       call function LAD_hist_list() to compute the median and LAD for the whole data of this tree node\n",
    "#       this LAD is set as the initial loss value for the next best-split searching process\n",
    "#       recall that the output of LAD_hist_list() is a tuple [median estimation, LAD]\n",
    "        tmp_metric= TLAD_histogram( node_hist, node_count)\n",
    "        best_split_sofar= tmp_metric[1]\n",
    "        best_split=(-1,-1,-1,-1)\n",
    "        \n",
    "\n",
    "#       initial values for best split feature and feature-value set         \n",
    "        best_split_feature=-1\n",
    "        best_split_featureValueSet=-1\n",
    "\n",
    "#       iterater over each feature to search for the best-split\n",
    "        for j in range(0,numFeatures):\n",
    "            \n",
    "#             print \"processing feature\",j\n",
    "                     \n",
    "#           extract the histograms corresponding to the current feature j \n",
    "#           from local histograms of the current tree node,   \n",
    "            statisticToValues = local_aggre_nodes[local_node_idx][1][j]\n",
    "          \n",
    "#           call function split_onOneFeature_hist to find the best split on feature j\n",
    "#           the ouput variable split is a tuple including \n",
    "#            [ leftSplit_valueSet, mean/median estimate for the data in the left split, \n",
    "#     mean/median estimate for the data in the right split, the value of the loss function on the best split ]\n",
    "            split=split_onOneFeature_hist(statisticToValues,node_count, node_hist)\n",
    "            \n",
    "#           compare the split on feature j to the best split found so far\n",
    "#           condition: \n",
    "#             split[0]!=-1: checks whether this tree node can be splitted on feature j\n",
    "#                           if it is not, skip the comparison, since the current tree node cannot be splitted \n",
    "#                           on feature j\n",
    "            if  split[0]!=-1 and  split[3] < best_split_sofar:\n",
    "                best_split=split\n",
    "                best_split_sofar=split[3]\n",
    "                best_split_feature = j\n",
    "                best_split_featureValueSet= split[0]\n",
    "         \n",
    "        \n",
    "        local_node_idx=local_node_idx+1\n",
    "        \n",
    "#------------- grow the unbalanced tree ---------------------------------- \n",
    "        if best_split_feature == -1:\n",
    "            nodes_tree.append(  (-1,  -1  )  )\n",
    "            nodes_tree_test.append( tmp_metric[0]  )\n",
    "            nodes_tree_test.append( tmp_metric[0] )\n",
    "            continue\n",
    "#-------------------------------------------------------------------------        \n",
    "\n",
    "#       append the found best-split for the tree node to the tree strucutre variable \n",
    "        nodes_tree.append(  (best_split_feature,  best_split_featureValueSet  )      )\n",
    "        \n",
    "#       append the median esitmation on the left and right children of the tree node to \n",
    "#       tree estimation variable \n",
    "        nodes_tree_test.append(best_split[1])\n",
    "        nodes_tree_test.append(best_split[2])\n",
    "    \n",
    "#     if local_node_idx != len(local_aggre_nodes):\n",
    "#         print '!!!! debug !!!!: not all local nodes are processed ', local_node_idx,len(local_aggre_nodes)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  derive the exact median predicion on each leaf node\n",
    "def map_instance_toNode(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], node_split )\n",
    "    return (line[0], tmpnode)\n",
    "\n",
    "def exact_median_leafNode(num_leaf):\n",
    "    \n",
    "    median_leaf=[]\n",
    "    tmp_dta_rdd = dta_train.map(lambda line: map_instance_toNode(line) )\n",
    "    \n",
    "    for i in range(0, num_leaf):\n",
    "        \n",
    "        leaf_dta = tmp_dta_rdd.filter(lambda line: line[1] == i).map(lambda line: line[0]).cache()\n",
    "        tmpcnt= leaf_dta.count()\n",
    "        order_leaf = leaf_dta.sortBy(lambda line: line).zipWithIndex().map(lambda line: (line[1], line[0]) )\n",
    "        \n",
    "#         sort_leaf_dta = leaf_dta.sortBy(lambda line: line)\n",
    "#         order_rdd= sc.parallelize(range(0,tmpcnt))\n",
    "#         order_leaf =  order_rdd.zip(sort_leaf_dta)\n",
    "        \n",
    "#         print tmpcnt\n",
    "        \n",
    "        if tmpcnt==0:\n",
    "            median_leaf.append(0)\n",
    "            continue\n",
    "        \n",
    "        if tmpcnt%2 == 0:\n",
    "            tmp1= order_leaf.lookup( int(tmpcnt/2) )[0]\n",
    "            tmp2= order_leaf.lookup( int(tmpcnt/2)-1 )[0]\n",
    "            median_leaf.append( (tmp1+tmp2)/2.0 )\n",
    "        else:\n",
    "            median_leaf.append(  order_leaf.lookup( int(tmpcnt/2) )[0] )\n",
    "        \n",
    "    return median_leaf\n",
    "\n",
    "\n",
    "\n",
    "#  extract exact prediction on leaf nodes layer by layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#statistic_infor_check for distributed information collection\n",
    "\n",
    "def statistic_infor_check(local_aggre_nodes):\n",
    "    \n",
    "    numNode = len(local_aggre_nodes)\n",
    "#     print numNode\n",
    "    tmpsum=0\n",
    "    for i in range(0, numNode):\n",
    "        tmpsum=tmpsum+local_aggre_nodes[i][1]['count']\n",
    "        \n",
    "#     print tmpsum\n",
    "    if tmpsum != dta_train.count():\n",
    "        print 'problem in node count'\n",
    "\n",
    "    tmpsum=0\n",
    "    for i in range(0, numNode):\n",
    "        #check node hist\n",
    "        node_count = local_aggre_nodes[i][1]['count']\n",
    "        tmpsum=0\n",
    "        for j in local_aggre_nodes[i][1]['hist_list']:\n",
    "            tmpsum= tmpsum+j[2]\n",
    "        if tmpsum!= node_count:\n",
    "            print '$$$$ problem in the histogram of node'\n",
    "            \n",
    "        #check feature values\n",
    "        for j in range(0, numFeatures):\n",
    "            tmpsum=0\n",
    "            for k in local_aggre_nodes[i][1][j].keys():\n",
    "                value_hist= local_aggre_nodes[i][1][j][k]['hist_val_list']\n",
    "                value_cnt = local_aggre_nodes[i][1][j][k]['count_inFeatureValue']\n",
    "                tmpsum= tmpsum+value_cnt\n",
    "            \n",
    "#                 print len(value_hist)\n",
    "                if len(value_hist) > bin_num:\n",
    "                    print '???? bin num wrong', len(value_hist)\n",
    "            \n",
    "                #check feature-value hist\n",
    "                tmpsum1=0\n",
    "                for m in value_hist:\n",
    "                    tmpsum1= tmpsum1 + m[2]\n",
    "                if tmpsum1 != value_cnt:\n",
    "                    print '++++ problem in feature-value histogram', tmpsum1, value_cnt\n",
    "            \n",
    "            if tmpsum != node_count:\n",
    "                print '---- problem !!!! in feature:', tmpsum, node_count    \n",
    "\n",
    "\n",
    "def consistancy_check(local_node1, local_node2):\n",
    "    numNode1 = len(local_node1)\n",
    "    numNode2 = len(local_node2)\n",
    "    \n",
    "    if numNode1 != numNode2:\n",
    "        print \"total num of nodes wrong!\"\n",
    "    \n",
    "    for i in range(0, numNode1):\n",
    "        \n",
    "        node1= local_node1[i][1]\n",
    "        node2= local_node2[i][1]\n",
    "        \n",
    "        for j in range(0, numFeatures):\n",
    "            for k in node1[j].keys():\n",
    "                if node1[j][k]['count_inFeatureValue']!=node2[j][k]['count_inFeatureValue']:\n",
    "                    print \"feature value count wrong:\",\"featurn:\",j, \"  value:\", k\n",
    "                \n",
    "                \n",
    "        \n",
    "        \n",
    "# pre_local_node=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#parameters set-up\n",
    "# max depth of regression tree to train\n",
    "# maxdepth= 9\n",
    "# # number of features in the dataset\n",
    "# # limit on the number of bin\n",
    "# bin_num=500\n",
    "# trim_ratio = 0.1\n",
    "\n",
    "\n",
    "dta_train= dta_train.repartition(number_workers)\n",
    "print dta_train.getNumPartitions()\n",
    "print dta_train.first()\n",
    "\n",
    "\n",
    "# variables for the tree structure \n",
    "node_split=[]\n",
    "node_test=[]\n",
    "\n",
    "# variables to record training performance information \n",
    "crun_time=[]\n",
    "lrun_time=[]\n",
    "commu_cost=[]\n",
    "\n",
    "tree_history_split=[]\n",
    "tree_history_esti=[]\n",
    "tree_history_runtime=[]\n",
    "\n",
    "\n",
    "# record the median on leaf nodes\n",
    "leaf_pred=[]\n",
    "\n",
    "# initilize an empty regression tree on the cluster side\n",
    "currentNode_split_fromMaster = sc.broadcast([])\n",
    "\n",
    "# starting time counter for overall running time   \n",
    "# start = time.time() \n",
    "        \n",
    "for cur_depth in range(0,maxdepth):\n",
    "    \n",
    "#   time counter for the running time of training over one layer of tree nodes\n",
    "    start_cluster = time.time() \n",
    "#   calculate the number of tree nodes that require to split on current layer cur_depth      \n",
    "    current_NumNodes= int( math.pow(2,  cur_depth)) \n",
    "    \n",
    "#   training data RDD dta_train calls mapPartitions \n",
    "#   then mapPartition calls function partition_combiner_hist in block4 to collect data summarization \n",
    "#   for the tree nodes in the current layer\n",
    "#   output variable: RDD statis_partitions containing histograms on different data partitions \n",
    "    statis_partitions = dta_train.mapPartitions( partition_combiner_hist )\n",
    "        \n",
    "#   RDD statis_partitions calls function reduceByKey \n",
    "#   then reduceByKey calls function merge_parttion_combiner_hist in block4 to merge the histograms\n",
    "#   output variable: RDD aggre_nodes containing the unified histograms for each node in the current layer\n",
    "    aggre_nodes =statis_partitions.reduceByKey(lambda statis_partition_1, statis_partition_2: \n",
    "                              merge_parttion_combiner_hist(statis_partition_1,statis_partition_2 ))\n",
    "        \n",
    "#   collect the histograms to local side, < composite structure of list and dictionary >      \n",
    "    local_aggre_nodes= aggre_nodes.collect()\n",
    "    print 'data summarization of leaf nodes:', len(local_aggre_nodes)\n",
    "#     print len(local_aggre_nodes) , current_NumNodes\n",
    "    \n",
    "#   consistency check\n",
    "#     pre_local_node = local_aggre_nodes\n",
    "#     consistancy_check(pre_local_node, local_aggre_nodes)\n",
    "\n",
    "    \n",
    "#   end time counter\n",
    "    end_cluster = time.time()\n",
    "    \n",
    "#   communication cost \n",
    "    commu_cost.append( sys.getsizeof(local_aggre_nodes) )\n",
    "#     print sys.getsizeof(local_aggre_nodes)*1.0,current_NumNodes,number_workers\n",
    "\n",
    "    \n",
    "#   data summarization check\n",
    "#     statistic_infor_check(local_aggre_nodes)\n",
    "#     print \"check done\"\n",
    "    #block1: generate synthetic dataset.\n",
    "\n",
    "\n",
    "#   function find_bestSplit_hist in block5 to find best-split for each node in the current layer\n",
    "#   \n",
    "#   input variable: \n",
    "#   local_aggre_nodes: the collected histograms for the tree nodes in the current layer\n",
    "#   current_NumNodes: the number of tree nodes to split in this layer\n",
    "#   \n",
    "#   the next two variables are used for storing the split results:\n",
    "#   node_split: records the split-feature and split-feature-value for each tree node. This variable is broadcasted\n",
    "#               to the cluster side for next-loop data summarization. \n",
    "#   node_test: records the estimate of dependent varible on each tree node, which is used for prediction.\n",
    "\n",
    "    find_bestSplit_hist( sorted(local_aggre_nodes,key= lambda val:val[0]),current_NumNodes, node_split, node_test)\n",
    "    end_local= time.time()\n",
    "    \n",
    "    \n",
    "#   broadcast variable node_split to the cluster for next-loop training    \n",
    "    currentNode_split_fromMaster = sc.broadcast(node_split)\n",
    "#     print node_split\n",
    "    \n",
    "#   monitor the running time on both cluster and local sides \n",
    "    print \"running time on cluster and local sides at height\",cur_depth,\":\",end_cluster-start_cluster,\\\n",
    "    end_local-end_cluster\n",
    "    \n",
    "#     end = time.time()\n",
    "#     elapsed = end-start\n",
    "    crun_time.append( end_cluster- start_cluster )\n",
    "    lrun_time.append( end_local  - end_cluster   )\n",
    "\n",
    "#   extract the estimations on leaf nodes from the regression tree trained so far.\n",
    "#   for the algorithm inspection and debugging\n",
    "#     leaf_nodes = node_test[ len(node_test)-  (int)( math.pow(2,cur_depth+1)): len(node_test)  ]\n",
    "    \n",
    "    \n",
    "#   record the  for the regression tree trained so far  \n",
    "    tree_history_split.append(copy.deepcopy(node_split) )\n",
    "#     tmp = exact_median_leafNode(current_NumNodes*2)\n",
    "    tmp=node_test[ len(node_test)-  (int)( math.pow(2,cur_depth+1)): len(node_test)  ]\n",
    "    tree_history_esti.append( tmp   )\n",
    "    print 'prediction size:', len(tmp)\n",
    "\n",
    "\n",
    "tmpval= commu_cost[0]\n",
    "commu_cost =[ i - tmpval for i in commu_cost]\n",
    "\n",
    "for i in range(0,maxdepth):\n",
    "    commu_cost[i] = commu_cost[i] *1.0/ number_workers / ( math.pow(2,  i)) \n",
    "\n",
    "print crun_time\n",
    "print lrun_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# you can see the testing error is decreasing as the training depth increases \n",
    "# \n",
    "# starting from using two children nodes of root node as leaf nodes of depth-1 tree \n",
    "# to predict values\n",
    "\n",
    "\n",
    "#  this fuction is called by RDD.map() to perform value prediction and error computation\n",
    "#  it calls function search_nodeToData() above to locate the tree node the give testing instance belongs to\n",
    "#  input:\n",
    "#    line: input data instance, [value of Y, feature-values..],<tuple>\n",
    "#  output: \n",
    "#    error square, <float>\n",
    "\n",
    "lad_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "lad_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "lad_tree_history_runtime =  copy.deepcopy( tree_history_runtime)\n",
    "\n",
    "\n",
    "def test_error_mapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], test_tree_split)    \n",
    "    return (test_leaf_nodes[ tmpnode ]  - line[0])*(test_leaf_nodes[ tmpnode ]  - line[0])\n",
    "\n",
    "def test_mapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], test_tree_split)    \n",
    "    return (test_leaf_nodes[ tmpnode ], line[0])\n",
    "\n",
    "# this function test the regression tree up to depth current_depth using the testing data RDD\n",
    "# input:\n",
    "#   current_depth: the depth of regression tree to test,<integer>\n",
    "#   testRDD: RDD containting test data, <RDD>\n",
    "# output:\n",
    "#   MSE of the testing data set, <float>\n",
    "def test_error_tree( current_depth, testRDD  ):\n",
    "\n",
    "    test_cnt= testRDD.count()\n",
    "    err_rdd=testRDD.map( lambda line: test_error_mapFunc(line))\n",
    "    err_sum = err_rdd.reduce( lambda a,b: a+b)\n",
    "    \n",
    "    print 'test error at depth',current_depth, ':', sqrt(err_sum*1.0/test_cnt)/(ymax-ymin)\n",
    "    return sqrt(err_sum*1.0/test_cnt)/(ymax-ymin), sqrt(err_sum*1.0/test_cnt)\n",
    "\n",
    "\n",
    "# variable for marking the position of tree nodes on a certain depth of the regression tree \n",
    "pre=0\n",
    "print ymax, ymin\n",
    "# estimations on the tree nodes,<list>\n",
    "tree_esti = lad_tree_history_esti[0]\n",
    "# split feature and feature-values of tree nodes \n",
    "tree_split =  lad_tree_history_split[ 0 ]\n",
    "# store error result at different depths\n",
    "err_depth=[]\n",
    "\n",
    "# iterate over the depth of regression tree\n",
    "for i in range(0,maxdepth):\n",
    "            \n",
    "#   extract the value estimate on the nodes of the current depth from variable tree_esti \n",
    "    test_leaf_nodes = lad_tree_history_esti[ i  ]\n",
    "#   extract the split feature and feature-value-set for the tree of current depth from variable tree_split   \n",
    "    test_tree_split= lad_tree_history_split[ i  ]\n",
    "    \n",
    "#   function test_error_tree( i, dta_test) is defined in block11\n",
    "#   calculate the test error using the regression tree of current depth\n",
    "    err_depth.append(test_error_tree( i, dta_test)  )\n",
    "    \n",
    "print err_depth\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# block8: backup the training results for the following analysis and prediction\n",
    "\n",
    "# variables descrbing regression tree stucture: tree_history_split\n",
    "# estimate values on all tree nodes: tree_history_esti\n",
    "# estimate values on leaf nodes: tree_history_leaf\n",
    "# training time: tree_history_runtime\n",
    "lad_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "lad_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "lad_tree_history_runtime =  copy.deepcopy( tree_history_runtime)\n",
    "\n",
    "# print out counts for quick review\n",
    "print \"tree splits:\"\n",
    "for i in range(0, len(lad_tree_history_split)):\n",
    "    print len(lad_tree_history_split[i])\n",
    "\n",
    "print \"tree predictions:\"\n",
    "for i in range(0, len(lad_tree_history_esti) ):\n",
    "    print len(lad_tree_history_esti[i])\n",
    "\n",
    "# print tree_history_runtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# offline prediction deriviation and evaluation\n",
    "\n",
    "#  derive the exact median predicion on each leaf node\n",
    "def instance_toNode(line, splits):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], splits )\n",
    "    return (line[0], tmpnode)\n",
    "\n",
    "def exact_predition_leafNode(num_leaf,  splits):\n",
    "    \n",
    "    median_leaf=[]\n",
    "    tmp_dta_rdd = dta_train.map(lambda line: instance_toNode(line,splits) )\n",
    "    \n",
    "    for i in range(0, num_leaf):\n",
    "        \n",
    "        leaf_dta = tmp_dta_rdd.filter(lambda line: line[1] == i).map(lambda line: line[0]).cache()\n",
    "        tmpcnt= leaf_dta.count()\n",
    "        order_leaf = leaf_dta.sortBy(lambda line: line).zipWithIndex().map(lambda line: (line[1], line[0]) )\n",
    "        \n",
    "        if tmpcnt==0:\n",
    "            median_leaf.append(0)\n",
    "            continue\n",
    "        \n",
    "        if tmpcnt%2 == 0:\n",
    "            tmp1= order_leaf.lookup( int(tmpcnt/2) )[0]\n",
    "            tmp2= order_leaf.lookup( int(tmpcnt/2)-1 )[0]\n",
    "            median_leaf.append( (tmp1+tmp2)/2.0 )\n",
    "        else:\n",
    "            median_leaf.append(  order_leaf.lookup( int(tmpcnt/2) )[0] )\n",
    "        \n",
    "    return median_leaf\n",
    "\n",
    "\n",
    "def test_error_mapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], test_tree_split)    \n",
    "    return (test_leaf_nodes[ tmpnode ]  - line[0])*(test_leaf_nodes[ tmpnode ]  - line[0])\n",
    "\n",
    "def test_mapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], test_tree_split)    \n",
    "    return (test_leaf_nodes[ tmpnode ], line[0])\n",
    "\n",
    "# this function test the regression tree up to depth current_depth using the testing data RDD\n",
    "# input:\n",
    "#   current_depth: the depth of regression tree to test,<integer>\n",
    "#   testRDD: RDD containting test data, <RDD>\n",
    "# output:\n",
    "#   MSE of the testing data set, <float>\n",
    "def test_error_tree( current_depth, testRDD  ):\n",
    "\n",
    "    test_cnt= testRDD.count()\n",
    "    err_rdd=testRDD.map( lambda line: test_error_mapFunc(line))\n",
    "    err_sum = err_rdd.reduce( lambda a,b: a+b)\n",
    "    \n",
    "    print 'test error at depth',current_depth, ':', sqrt(err_sum*1.0/test_cnt)/(ymax-ymin)\n",
    "    return sqrt(err_sum*1.0/test_cnt)/(ymax-ymin), sqrt(err_sum*1.0/test_cnt)\n",
    "\n",
    "\n",
    "# variable for marking the position of tree nodes on a certain depth of the regression tree \n",
    "pre=0\n",
    "print ymax, ymin\n",
    "# estimations on the tree nodes,<list>\n",
    "# tree_esti = lad_tree_history_esti[0]\n",
    "# split feature and feature-values of tree nodes \n",
    "# tree_split =  lad_tree_history_split[ 0 ]\n",
    "# store error result at different depths\n",
    "err_depth=[]\n",
    "\n",
    "# iterate over the depth of regression tree\n",
    "for i in range(0,maxdepth,2):\n",
    "    \n",
    "#   extract the split feature and feature-value-set for the tree of current depth from variable tree_split   \n",
    "    test_tree_split= lad_tree_history_split[ i ]\n",
    "    \n",
    "#   extract the value estimate on the nodes of the current depth from variable tree_esti \n",
    "    num_leaf = int( math.pow(2,  i+1)) \n",
    "    test_leaf_nodes = exact_predition_leafNode(num_leaf,  test_tree_split)\n",
    "    print i, len(test_leaf_nodes)    \n",
    "    \n",
    "#   function test_error_tree( i, dta_test) is defined in block11\n",
    "#   calculate the test error using the regression tree of current depth\n",
    "    err_depth.append(test_error_tree( i, dta_test)  ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#block11: functions for code debugging and inspection of tree structure \n",
    "# \n",
    "\n",
    "def feature_value_extract( valueSet):\n",
    "    \n",
    "    tmpval=1\n",
    "    tmp_valSet= valueSet\n",
    "    valSet_list=[]\n",
    "    cnt=0\n",
    "    \n",
    "    while tmpval <= tmp_valSet:\n",
    "        if tmpval & tmp_valSet !=0:\n",
    "            valSet_list.append(cnt)\n",
    "        tmpval=tmpval<<1\n",
    "        cnt=cnt+1\n",
    "        \n",
    "    return valSet_list\n",
    "\n",
    "def training_data_summary( train_rdd ):\n",
    "num_leaf\n",
    "#     always remove outliers before\n",
    "    train_rdd_clean= train_rdd.filter(lambda line: line[0]<1000)\n",
    "    distinct_featureVals = train_rdd_clean.map(lambda line: (line[1], line[2],line[3],line[4] ) ).distinct().collect()\n",
    "   \n",
    "    featureConfig_val={}\n",
    "    for i in distinct_featureVals:\n",
    "        \n",
    "        tmprdd=train_rdd_clean.filter(lambda line:(line[1], line[2],line[3],line[4])  ==i ).map(lambda line: line[0])\n",
    "        tmpval=tmprdd.reduce(lambda a,b:a+b)\n",
    "        tmpval= tmpval/tmprdd.count()*1.0\n",
    "        \n",
    "        featureContestDtafig_val.update( {i:tmpval} )\n",
    "    \n",
    "#     print featureConfig_val\n",
    "    return featureConfig_val\n",
    "        \n",
    "\n",
    "def config_check( featureVal_set, feature_num):\n",
    "    \n",
    "    for i in range(0, feature_num):\n",
    "        if len(featureVal_set[i]) >1 :\n",
    "            return -1\n",
    "    return 1\n",
    "\n",
    "def lookup_featureConfig_val( configToVal_dict, value_set, feature_num):\n",
    "    \n",
    "    tmptuple=()\n",
    "    for i in range(0, feature_num):\n",
    "        tmptuple = tmptuple + ( value_set[i][0]  ,)\n",
    "#     print '++++++++++++', tmptuple\n",
    "    return configToVal_dict[tmptuple]\n",
    "\n",
    "\n",
    "\n",
    "def test_error_indiviMapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], test_tree_split)\n",
    "    return (test_leaf_nodes[ tmpnode ], line[0])\n",
    "    \n",
    "def test_error_mapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], test_tree_split)\n",
    "    \n",
    "#     return ( test_leaf_nodes[ tmpnode ],  line[0]   )\n",
    "    return (test_leaf_nodes[ tmpnode ]  - line[0])*(test_leaf_nodes[ tmpnode ]  - line[0])\n",
    "    \n",
    "\n",
    "    \n",
    "def bfs_tree(tree, total_featureVal_set, depth, tree_esti, trainRDD ):\n",
    "    \n",
    "    nodeNum=len(tree)   \n",
    "    if nodeNum == 0:\n",
    "        return\n",
    "    \n",
    "    feature_ConfigToVal = training_data_summary( trainRDD )\n",
    "    \n",
    "    interNode_cnt=0\n",
    "    non_leaf_cnt=0\n",
    "    recog_config_cnt=0\n",
    "    \n",
    "    qu=[]\n",
    "    current_depth=0\n",
    "    qu.append((0,total_featureVal_set,0))\n",
    "    \n",
    "    \n",
    "    recog_config_cnt_depth=[]\n",
    "    recog_conf=[]\n",
    "    \n",
    "    #debug\n",
    "    test_queue=[]\n",
    "    \n",
    "    while current_depth<= depth:\n",
    "        \n",
    "        print 'non_leaf_node   count at previous depth', non_leaf_cnt\n",
    "        \n",
    "        print 'depth', current_depth,':', len(qu)\n",
    "        tmpqu=[]\n",
    "        \n",
    "        non_leaf_cnt=0\n",
    "        \n",
    "        for i in qu:\n",
    "            current_nodeIdx= i[0] \n",
    "            current_featureVal_set= i[1]\n",
    "            \n",
    "            split_feature= tree[current_nodeIdx][0]\n",
    "            split_valueSet= tree[current_nodeIdx][1]\n",
    "                 \n",
    "            if split_feature == -1: \n",
    "                \n",
    "                if config_check( current_featureVal_set, numFeatures) ==1:\n",
    "                    recog_config_cnt= recog_config_cnt+1\n",
    "                    \n",
    "                    tmpval=lookup_featureConfig_val( feature_ConfigToVal, current_featureVal_set, featureNum)\n",
    "                    \n",
    "                    print current_nodeIdx,current_featureVal_set, 'has the estimate:',\\\n",
    "                    tree_esti[ current_nodeIdx*2+1],'true:',tmpval\n",
    "                    #result\n",
    "                    recog_conf.append( (tree_esti[ current_nodeIdx*2+1], tmpval) )\n",
    "                    \n",
    "                else:\n",
    "                    print current_nodeIdx,current_featureVal_set, 'has the estimate:',\\\n",
    "                    tree_esti[ current_nodeIdx*2+1],'!!! not seperated !!!'\n",
    "                    \n",
    "#                     print '!!! debug !!!', current_nodeIdx,tree[current_nodeIdx], current_featureVal_set\n",
    "                \n",
    "                interNode_cnt=interNode_cnt+1\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                non_leaf_cnt=non_leaf_cnt+1\n",
    "                \n",
    "                featureValue_list = feature_value_extract( split_valueSet)\n",
    "                \n",
    "                print current_nodeIdx, current_featureVal_set,'non-leaf node,', split_feature, featureValue_list\n",
    "                \n",
    "                \n",
    "            #debug\n",
    "#             if current_depth == 9:\n",
    "#                 print '+++ debug ++++',current_nodeIdx,tree[current_nodeIdx], current_featureVal_set, \\\n",
    "#                 featureValue_list,split_feature\n",
    "    \n",
    "            left_featureValue_set=[]\n",
    "            right_featureValue_set=[]\n",
    "            featureNum= len(current_featureVal_set)\n",
    "    \n",
    "            for i in range(0, numFeatures):\n",
    "                if i != split_feature:\n",
    "                    left_featureValue_set.append(  copy.deepcopy(current_featureVal_set[i])   )\n",
    "                    right_featureValue_set.append(  copy.deepcopy(current_featureVal_set[i])   )\n",
    "                else:\n",
    "                    left_featureValue_set.append(  copy.deepcopy(featureValue_list )   )\n",
    "                    right_featureValue_set.append(copy.deepcopy(list(set(current_featureVal_set[i] )-set(featureValue_list)))   )\n",
    "            \n",
    "            \n",
    "#             #debug\n",
    "#             if current_depth==8:\n",
    "#                 if left_featureValue_set== [[2], [11, 13], [103]] or \\\n",
    "#                 right_featureValue_set==right_featureValue_set:\n",
    "#                     print '????? debug ????', current_nodeIdx,left_featureValue_set,right_featureValue_set\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            tmpqu.append(  (current_nodeIdx*2+1,left_featureValue_set, current_nodeIdx)  )\n",
    "            tmpqu.append(  (current_nodeIdx*2+2,right_featureValue_set,current_nodeIdx) )\n",
    "        \n",
    "        qu=copy.deepcopy(tmpqu)\n",
    "        current_depth= current_depth+1\n",
    "        \n",
    "        #result\n",
    "        recog_config_cnt_depth.append( recog_config_cnt )\n",
    "    \n",
    "    \n",
    "# print out leaf nodes\n",
    "\n",
    "    print 'non_leaf_node count at previous depth', non_leaf_cnt\n",
    "    print 'bottom depth', current_depth,':', len(qu)\n",
    "    \n",
    "    for i in qu:\n",
    "        current_nodeIdx= i[0] \n",
    "        current_featureVal_set= i[1]\n",
    "        parent_nodeIdx= i[2]\n",
    "        \n",
    "#         debug \n",
    "#         print '----debug----', current_featureVal_set\n",
    "        \n",
    "        if 2*parent_nodeIdx+1 == current_nodeIdx: \n",
    "#         parent_nodeIdx == (current_nodeIdx*2+1):\n",
    "            tree_esti_val= tree_esti[ parent_nodeIdx*2]\n",
    "        else:\n",
    "            tree_esti_val= tree_esti[ parent_nodeIdx*2+1] \n",
    "        \n",
    "        if config_check( current_featureVal_set, featureNum) ==1:\n",
    "            recog_config_cnt= recog_config_cnt+1\n",
    "            tmpval=lookup_featureConfig_val( feature_ConfigToVal, current_featureVal_set, featureNum)\n",
    "            \n",
    "            #result\n",
    "            recog_conf.append( (tree_esti_val, tmpval) )\n",
    "            \n",
    "            print current_nodeIdx,current_featureVal_set, 'has the estimate:', tree_esti_val,\\\n",
    "                    'true:',tmpvalcur_depth\n",
    "        else:\n",
    "            print current_nodeIdx,current_featureVal_set, 'has the estimate:', tree_esti_val\n",
    "    \n",
    "    #result\n",
    "    recog_config_cnt_depth.append( recog_config_cnt )\n",
    "        \n",
    "    print 'inter-node number:', interNode_cnt\n",
    "    print ' *******  number of identified configurations:', recog_config_cnt\n",
    "    \n",
    "    tmpval=0\n",
    "    for i in range(0,recog_config_cnt):\n",
    "        tmpval=tmpval+ (recog_conf[i][0]-recog_conf[i][1])*(recog_conf[i][0]-recog_conf[i][1])\n",
    "    \n",
    "    return (recog_conf, recog_config_cnt_depth, tmpval*1.0/recog_config_cnt)\n",
    "\n",
    "# len( tree_history_split)\n",
    "# len(tree_history_esti)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # training and testing data preparation\n",
    "# # sample from the original data RDD variable: dta\n",
    "\n",
    "# # outlier threshold to filter outliers in the testing dataset\n",
    "# outlier_threshold = 10000\n",
    "# # 5000\n",
    "# print dta.count()\n",
    "\n",
    "# dta_train_all = dta.cache().sample(False, .9, 12345)\n",
    "# dta_test_all = dta.cache().sample(False, .1, 43243)\n",
    "\n",
    "# print 'before filtering:'\n",
    "# print dta_train_all.count(), dta_train_all.getNumPartitions()\n",
    "# print dta_test_all.count(),  dta_test_all.getNumPartitions()\n",
    "\n",
    "# # training data set can be set to have outerliers or not\n",
    "# dta_train = dta_train_all\n",
    "# # .filter(lambda line: line[0]<outlier_threshold )\n",
    "# tmprdd= dta_train.map(lambda line:line[0] )\n",
    "# print tmprdd.max(), tmprdd.min()\n",
    "\n",
    "\n",
    "# tmprdd = dta_train.filter(lambda line: line[0]<outlier_threshold )\n",
    "# ymax = tmprdd.map(lambda line: line[0]).max()\n",
    "# ymin = tmprdd.map(lambda line: line[0]).min()\n",
    "# print ymax, ymin\n",
    "\n",
    "# # filter outliers for testing data\n",
    "# dta_test = dta_test_all.filter(lambda line: line[0]< outlier_threshold )\n",
    "\n",
    "# print 'number of training data instance:',dta_train.count()\n",
    "# print 'number of testing data instance:',dta_test.count()\n",
    "# print dta_train.first()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
