{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os \n",
    "import sys \n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "from datetime  import datetime \n",
    "\n",
    "import numpy as np  # learn \n",
    "import pandas as pd # learn\n",
    "from pandas import *\n",
    "from numpy import *\n",
    "\n",
    "from scipy import stats # look at scipy\n",
    "from scipy import linalg\n",
    "from scipy import *\n",
    "\n",
    " \n",
    "import matplotlib as mplt # learn matplolib \n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "import seaborn as sns \n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc={\"figure.figsize\": (14, 6)})\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import sklearn as sk\n",
    "import itertools\n",
    "\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import heapq\n",
    "# from SyntheticDataGenerator import *\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "\n",
    "\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from SyntheticDataGenerator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#block1: generate synthetic dataset.\n",
    "\n",
    "# the text file for the produced dataset is stored locally\n",
    "# the meaning of the file name 'syndata_f5v5_6zeros_1percen.txt':\n",
    "# f5: 5 features, v5: each feature has 5 values, 6zeros: values of outliers are up to 6000000, \n",
    "# 1percet: the percentage of outerliers in the dataset\n",
    "filename = '/home/tguo/data/tian-syn/syn-10m-1p-6z.txt'\n",
    "\n",
    "# three main parameters: rows: number of data instance, cols: number of features\n",
    "# featureValues: number of feature values for each feature \n",
    "rows = 10000000 \n",
    "cols = 5\n",
    "featureValues= 5\n",
    "\n",
    "# call class SyntheticDataGenerator to generate synthetic data \n",
    "data = SyntheticDataGenerator(rows, cols,featureValues+1,0.0,1000000) \n",
    "data.writeData(filename)\n",
    "\n",
    "\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23.0, 0, 9, 13, 2, 3, 46, 209)\n",
      "41955835\n",
      "1438.0 -1437.0\n"
     ]
    }
   ],
   "source": [
    "#  load flight data\n",
    "dta_rdd = sc.textFile(\"hdfs://computer61.ant-net/user/tguo/airline_data.csv\")\n",
    "\n",
    "dta_rdd = dta_rdd.map(lambda line: line.split(\",\")).map(lambda r:\n",
    "                                                                (float(r[0]),int( float(r[1])), \n",
    "                                                                 int(float(r[2])),int(float(r[3])),\n",
    "                                                                 int(float(r[4])),int(float(r[5])),\n",
    "#                                                                  int(float(r[6])),\n",
    "                                                                 int(float(r[7])), int(float(r[8])))).cache()\n",
    "\n",
    "\n",
    "print dta_rdd.first()\n",
    "print dta_rdd.count()\n",
    "\n",
    "tmprdd= dta_rdd.map(lambda line:line[0]).cache()\n",
    "print tmprdd.max(), tmprdd.min()\n",
    "\n",
    "\n",
    "# cate_map={}\n",
    "# for i in range(0, feature_num):\n",
    "#     cate_map.update({i: dta_rdd.map(lambda line: line[ i+1  ]).distinct().count()}  ) \n",
    "\n",
    "# print cate_map\n",
    "cate_map = {0: 9, 1: 12, 2: 31, 3: 7, 4: 15, 5: 266, 6: 265}\n",
    "# cate_map = {0: 1, 1: 3, 2: 31, 3: 7, 4: 14, 5: 237, 6: 237}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,1,5,3,3,57.0517172965\n",
      "(57.0517172965, 1, 1, 5, 3, 3)\n",
      "10000000\n",
      "8\n",
      "1000004.6965 -3.252894382\n"
     ]
    }
   ],
   "source": [
    "#block3: load synthetic data\n",
    "# dta_RDD = sc.textFile(\"hdfs://computer61.ant-net/user/tguo/syn-30m-1p10mag.csv\",10)\n",
    "\n",
    "# syn-10m-1p-4z.txt\n",
    "dta_rdd = sc.textFile(\"file:///home/tguo/data/tian-syn/syn-10m-10p-6z.txt\").cache()\n",
    "\n",
    "# syn-10m-10p-6z.txt\n",
    "# dta_rdd = sc.textFile(\"file:///home/tguo/data/tian-syn/syn-10m-1p.txt\").cache()\n",
    "print dta_rdd.first()\n",
    "\n",
    "# data type conversion and organize data of the form (dependent variable value, \n",
    "# feature-values) \n",
    "dta_splited = dta_rdd.map(lambda line: line.split(\",\")).map(lambda r:\n",
    "                                                        ( float(r[5]),\n",
    "                                                        int(float(r[0])), int( float(r[1])), \n",
    "                                                        int(float(r[2])),int(float(r[3])),\n",
    "                                                        int(float(r[4]))\n",
    "#                                                          ,int(float(r[5])),\n",
    "#                                                         int(float(r[6])),int(float(r[7])),\n",
    "#                                                         int(float(r[8]))\n",
    "#                                                          ,int(float(r[9])),\n",
    "                                                        ) )\n",
    "\n",
    "\n",
    "dta= dta_splited\n",
    "print dta.first()\n",
    "print dta.count()\n",
    "print dta.getNumPartitions()\n",
    "\n",
    "# cate_map={0: 6, 1: 6, 2: 6, 3: 6, 4: 6}\n",
    "\n",
    "tmprdd= dta.map(lambda line: line[0]).cache()\n",
    "print tmprdd.max(), tmprdd.min()\n",
    "\n",
    "dta_rdd= dta\n",
    "\n",
    "cate_map = {0: 6, 1: 6, 2: 6, 3: 6, 4: 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#block3: load BT data\n",
    "# dta_RDD = sc.textFile(\"hdfs://computer61.ant-net/user/tguo/udpjitter-H1april2014_regTree_mllib.csv\")\n",
    "\n",
    "\n",
    "dta_rdd = sc.textFile(\"hdfs://computer61.ant-net/user/tguo/bt_data.csv\").cache()\n",
    "# tmpfirst = dta_rdd\n",
    "# dta_rdd = dta_rdd.filter( lambda line: line!=tmfirst )\n",
    "\n",
    "# \"file:///home/tguo/data/tian-syn/syn-30m.txt\"\n",
    "\n",
    "# data type conversion and organize data of the form (dependent variable value, feature-values) \n",
    "dta_splited = dta_rdd.map(lambda line: line.split(\",\")).map(lambda r:\n",
    "                                                                (float(r[0]),\n",
    "#                                                                  int( float(r[1])), \n",
    "                                                                 int(float(r[2])),int(float(r[3])),\n",
    "                                                                 int(float(r[4])),int(float(r[5])),\n",
    "#                                                                  int(float(r[6])),\n",
    "                                                                 int(float(r[7])),\n",
    "                                                                 int(float(r[8])),int(float(r[9])),\n",
    "                                                                 int(float(r[10]))))\n",
    "\n",
    "dta= dta_splited\n",
    "dta.cache()\n",
    "print dta.first()\n",
    "print dta.count()\n",
    "\n",
    "dta_rdd = dta\n",
    "\n",
    "cate_map={0: 1, 1: 15, 2: 24, 3: 10, 4: 5, 5: 21, 6: 13, 7: 4}\n",
    "# cate_map={0: 2662, 1: 1, 2: 15, 3: 24, 4: 10, 5: 2712, 6: 5, 7: 21, 8: 13, 9: 4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_scale= 3\n",
    "number_workers=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 9, 1: 12, 2: 31, 3: 7, 4: 15, 5: 266, 6: 265}\n"
     ]
    }
   ],
   "source": [
    "# dta_rdd=dta.cache()\n",
    "cate_map={}\n",
    "for i in range(0, feature_num):\n",
    "    cate_map.update({i: dta_rdd.map(lambda line: line[ i+1  ]).distinct().count()}  ) \n",
    "\n",
    "print cate_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adjust the outlier scale\n",
    "\n",
    "def outlier_scale(line):\n",
    "    if line[0] >= 150:\n",
    "        return  (out_scale * np.random.normal(100, 1, 1)[0], )+ line[1:feature_num+1] \n",
    "    return line\n",
    "\n",
    "dta_rdd = dta_rdd.map(lambda line: outlier_scale(line) ).cache()\n",
    "print dta_rdd.getNumPartitions()\n",
    "print dta_rdd.first()\n",
    "\n",
    "tmprdd= dta_rdd.map(lambda line: line[0]).cache()\n",
    "print tmprdd.max(), tmprdd.min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare data\n",
    "\n",
    "# outlier_threshold= (out_scale-5)*1000\n",
    "outlier_threshold= 150\n",
    "# 5000\n",
    "# 150\n",
    "# 4000\n",
    "# train_dta\n",
    "labeled_dta=  dta_rdd.map(lambda line: LabeledPoint(line[0],line[1:feature_num+1]) )\n",
    "print labeled_dta.first()\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(train_dta, test_dta) = labeled_dta.randomSplit([0.5, 0.5])\n",
    "\n",
    "train_dta.cache() \n",
    "test_dta.cache()\n",
    "print 'test dataset before filtering:', test_dta.count()\n",
    "# print train_dta.count()\n",
    "# print test_dta.count()\n",
    "# train_dta = train_dta.filter( lambda line: line.label<outlier_threshold)\n",
    "test_dta = test_dta.filter( lambda line: line.label<outlier_threshold)\n",
    "\n",
    "\n",
    "tmprdd= train_dta.map(lambda line:line.label )\n",
    "print tmprdd.max(), tmprdd.min()\n",
    "\n",
    "tmp_rdd = train_dta.filter( lambda line: line.label<outlier_threshold).map( lambda line: line.label).cache()\n",
    "ymax= tmp_rdd.max()\n",
    "ymin= tmp_rdd.min()\n",
    "print ymax, ymin\n",
    "\n",
    "# tmp_rdd = test_dta.map( lambda line: line.label)\n",
    "# ymax= tmp_rdd.max()\n",
    "# ymin= tmp_rdd.min()\n",
    "# print ymax,yminnumber_workers\n",
    "\n",
    "print train_dta.count(), test_dta.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model on test instances and compute Normalized root-mean-square deviation (NRMSE)\n",
    "\n",
    "def precision_test(model, testDta):\n",
    "    pred = model.predict(testDta.map(lambda x: x.features))\n",
    "    local_pred = pred.collect()\n",
    "\n",
    "    labels = testDta.map(lambda x: x.label)    \n",
    "    local_labels = labels.collect()\n",
    "\n",
    "    local_resid=map( lambda x,y: (x-y)*(x-y), local_pred,local_labels )\n",
    "    return sqrt(sum(local_resid)/len(local_pred))/(ymax-ymin)\n",
    "# sqrt(sum(local_resid)/len(local_pred))\n",
    "# sqrt(sum(local_resid)/len(local_pred))/(ymax-ymin)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# regressin tree\n",
    "\n",
    "\n",
    "train_dta= train_dta.repartition(number_workers)\n",
    "print train_dta.getNumPartitions()\n",
    "print train_dta.first()\n",
    "\n",
    "run_time=[]\n",
    "run_err=[]\n",
    "\n",
    "for dep in range(1,10):\n",
    "    \n",
    "    start_time = time.time() \n",
    "    \n",
    "    regTree = DecisionTree.trainRegressor(train_dta, categoricalFeaturesInfo = cate_map,\n",
    "                                    impurity='variance', maxDepth=dep, maxBins = 2713)\n",
    "    tmperr= precision_test(regTree, test_dta)\n",
    "    print tmperr\n",
    "    \n",
    "    end_time= time.time()\n",
    "    run_time.append( end_time-start_time )\n",
    "    run_err.append( tmperr  )\n",
    "#     print end_time-start_time\n",
    "\n",
    "print run_time\n",
    "print run_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random forest \n",
    "run_err=[]\n",
    "overall_err=[]\n",
    "\n",
    "for tnum in range(2,10):\n",
    "    \n",
    "    run_err=[]\n",
    "    for h in range(2,10):\n",
    "        rf = RandomForest.trainRegressor(train_dta, categoricalFeaturesInfo = cate_map,\n",
    "                                    numTrees = tnum, featureSubsetStrategy = \"auto\",\n",
    "                                    impurity = 'variance', maxDepth = h, maxBins = 2731)\n",
    "        tmperr = precision_test(rf, test_dta)\n",
    "        run_err.append( tmperr  )\n",
    "    \n",
    "    overall_err.append(min(run_err))\n",
    "    print min(run_err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  boosted\n",
    "\n",
    "run_err=[]\n",
    "\n",
    "for iter_num in range(2,40):\n",
    "    gbt = GradientBoostedTrees.trainRegressor(train_dta,\n",
    "                                            categoricalFeaturesInfo=cate_map, numIterations=iter_num,maxBins=2731)\n",
    "    tmperr  = precision_test(gbt, test_dta)\n",
    "    print tmperr\n",
    "    run_err.append( tmperr  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  prepare data, including valdiation data set \n",
    "(train_dta, tmp_dta) = labeled_dta.randomSplit([0.6, 0.4])\n",
    "(test_dta, valid_dta) = tmp_dta.randomSplit([0.75, 0.25])\n",
    "\n",
    "train_dta.cache() \n",
    "test_dta.cache()\n",
    "valid_dta.cache()\n",
    "\n",
    "tmp_rdd = train_dta.map( lambda line: line.label)\n",
    "ymax= tmp_rdd.max()\n",
    "ymin= tmp_rdd.min()\n",
    "print ymax, ymin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
