{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os \n",
    "import sys \n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "from datetime  import datetime \n",
    "\n",
    "import numpy as np  # learn \n",
    "import pandas as pd # learn\n",
    "from pandas import *\n",
    "from numpy import *\n",
    "\n",
    "from scipy import stats # look at scipy\n",
    "from scipy import linalg\n",
    "from scipy import *\n",
    "\n",
    " \n",
    "import matplotlib as mplt # learn matplolib \n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "import seaborn as sns \n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc={\"figure.figsize\": (14, 6)})\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import sklearn as sk\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'-14.0,1,3,4,0,0,0,0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data preparison: load data\n",
    "fdta_RDD = sc.textFile(\"file:///home/tguo/data/tian-test/2008_mllib.csv\")\n",
    "\n",
    "# dta_cateFeature_resetIdxAndFilter= sc.textFile(\n",
    "#     \"hdfs://computer61.ant-net/user/tguo/tian_udpjitter-H1april2014_regTree_mllib.csv\")\n",
    "\n",
    "print 'loaded data'\n",
    "print type(fdta_RDD)\n",
    "\n",
    "fdta_RDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-14.0, 1.0, 3.0, 4.0, 0.0, 0.0, 0.0, 0.0),\n",
       " (2.0, 1.0, 3.0, 4.0, 1.0, 0.0, 0.0, 0.0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fdta_ready = fdta_RDD.map(lambda line: line.split(\",\")).map(lambda r:(float(r[0]),float(r[1]),float(r[2]),float(r[3]),float(r[4]),float(r[5]),float(r[6]),float(r[7])))\n",
    "\n",
    "fdta_ready = fdta_RDD.map(lambda line: line.split(\",\")).map(lambda r:(float(r[0]),float(r[1]),float(r[2]),float(r[3]),float(r[4]),float(r[5]),\n",
    "                                                                      float(r[6]),float(r[7])))\n",
    "\n",
    "\n",
    "fdta_ready.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 depth: 11.6188529515 11.6026038776\n",
      "0 DecisionTreeModel regressor of depth 0 with 1 nodes\n",
      "1 depth: 2.95040936769 2.94747018771\n",
      "1 DecisionTreeModel regressor of depth 1 with 3 nodes\n",
      "2 depth: 0.670535696322 0.669452528488\n",
      "2 DecisionTreeModel regressor of depth 2 with 7 nodes\n"
     ]
    }
   ],
   "source": [
    "#model training: cross-validation for regression tree with categorical features\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "flabeledData=  fdta_ready.map(lambda line: LabeledPoint(line[1],[line[1],line[2],line[3],line[4],line[5],line[6],line[7]]))\n",
    "depth_up=3\n",
    "depth_low=0\n",
    "\n",
    "# flabeledData=flabeledData.filter(lambda line: line.label>0 )\n",
    "\n",
    "test_error=[]\n",
    "training_error=[]\n",
    "\n",
    "(ftrainingData,ftestData)=flabeledData.randomSplit([0.8, 0.2])\n",
    "\n",
    "ftest_labels = ftestData.map(lambda lp: lp.label)\n",
    "local_test_labels = ftest_labels.collect()\n",
    "\n",
    "ftrain_labels = ftrainingData.map(lambda lp: lp.label)\n",
    "local_train_labels = ftrain_labels.collect()\n",
    "\n",
    "test_size= len(local_test_labels) \n",
    "train_size= len(local_train_labels) \n",
    "\n",
    "\n",
    "for depth in range(depth_low, depth_up):\n",
    "    model=DecisionTree.trainRegressor(ftrainingData,impurity='variance',maxDepth=depth,maxBins=7600,\n",
    "                                      categoricalFeaturesInfo={0:13,1:32,2:8,3:7539,4:20,5:303,6:304 })\n",
    "    \n",
    "    test_predictions = model.predict(ftestData.map(lambda x: x.features))\n",
    "    local_test_predictions = test_predictions.collect()\n",
    "    \n",
    "    \n",
    "    train_predictions = model.predict(ftrainingData.map(lambda x: x.features))\n",
    "    local_train_predictions = train_predictions.collect()\n",
    "\n",
    "\n",
    "    # test error\n",
    "    local_test_resid=map( lambda x,y: (x-y)*(x-y), local_test_predictions,local_test_labels )\n",
    "    test_error.append( sum(local_test_resid)/test_size  )\n",
    "    \n",
    "    #training error \n",
    "    local_train_resid=map( lambda x,y: (x-y)*(x-y), local_train_predictions,local_train_labels )\n",
    "    training_error.append( sum(local_train_resid)/train_size ) \n",
    "    \n",
    "    print depth,\"depth:\", sum(local_test_resid)/test_size,sum(local_train_resid)/train_size\n",
    "    print depth,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(2.0, [1.0,3.0,4.0,1.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(14.0, [1.0,3.0,4.0,2.0,0.0,1.0,1.0])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftrainingData.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Mean Squared Error = 0.24245412707\n",
      "Test Mean Squared Error = 0.0454545454545\n"
     ]
    }
   ],
   "source": [
    "# test for mllib svm dataset\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Load and parse the data file into an RDD of LabeledPoint.\n",
    "data = MLUtils.loadLibSVMFile(sc, 'file:///home/tguo/sample_libsvm_data.txt')\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "\n",
    "\n",
    "for depth in range(0,2):\n",
    "    model = DecisionTree.trainRegressor(trainingData, categoricalFeaturesInfo={},\n",
    "                                    impurity='variance', maxDepth=depth, maxBins=64)\n",
    "\n",
    "    # Evaluate model on test instances and compute test error\n",
    "    predictions = model.predict(testData.map(lambda x: x.features))\n",
    "    labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "    testMSE = labelsAndPredictions.map(lambda (v, p): (v - p) * (v - p)).sum() / float(testData.count())\n",
    "    print('Test Mean Squared Error = ' + str(testMSE))\n",
    "# print('Learned regression tree model:')\n",
    "# print(model.toDebugString())\n",
    "\n",
    "# Save and load model\n",
    "# model.save(sc, \"myModelPath\")\n",
    "# sameModel = DecisionTreeModel.load(sc, \"myModelPath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tmprdd=trainingData.filter(lambda line: line.label>0)\n",
    "# tmprdd.count()\n",
    "# tmprdd.take(44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmpdta=pd.read_csv('/home/tguo/tian_src/syndata_set/3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    NORTHEAST VERNON CO. R-I\n",
       "2              KIRKWOOD R-VII\n",
       "3           CLINTON CO. R-III\n",
       "4            PATTONSBURG R-II\n",
       "5            GRAIN VALLEY R-V\n",
       "6              CALHOUN R-VIII\n",
       "7              MEHLVILLE R-IX\n",
       "8               BOONVILLE R-I\n",
       "9                 SEDALIA 200\n",
       "Name: DISTRICT_NAME, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpdta['DISTRICT_NAME'][1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SaidStudent</th>\n",
       "      <th>YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2706.827</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3060.870</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>3529.126</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>2689.189</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>2578.512</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>2565.056</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      SaidStudent  YEAR\n",
       "0        2706.827  2007\n",
       "1        3060.870  2008\n",
       "79       3529.126  2009\n",
       "748      2689.189  2010\n",
       "1295     2578.512  2011\n",
       "1404     2565.056  2012"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpdta[ tmpdta['DISTRICT_NAME'] == 'NORTHEAST VERNON CO. R-I' ][  ['SaidStudent', 'YEAR']    ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
