{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os \n",
    "import sys \n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "from datetime  import datetime \n",
    "\n",
    "import numpy as np  # learn \n",
    "import pandas as pd # learn\n",
    "from pandas import *\n",
    "from numpy import *\n",
    "\n",
    "from scipy import stats # look at scipy\n",
    "from scipy import linalg\n",
    "from scipy import *\n",
    "\n",
    " \n",
    "import matplotlib as mplt # learn matplolib \n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "import seaborn as sns \n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc={\"figure.figsize\": (14, 6)})\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import sklearn as sk\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import random\n",
    "import time\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328.0, 10, 10, 2, 2, 8, 1, 3)\n",
      "3315067\n"
     ]
    }
   ],
   "source": [
    "#BT data\n",
    "# dta_RDD = sc.textFile(\"file:///home/tguo/data/tian-test/udpjitter-H1april2014_regTree.csv\")\n",
    "\n",
    "dta_RDD = sc.textFile(\"hdfs://computer61.ant-net/user/tguo/udpjitter-H1april2014_regTree_mllib.csv\")\n",
    "\n",
    "#data format: dependent variable, feature values\n",
    "dta_splited = dta_RDD.map(lambda line: line.split(\",\")).map(lambda r:\n",
    "                                                                (float(r[0]),int( float(r[1])), \n",
    "                                                                 int(float(r[2])),int(float(r[3])),\n",
    "                                                                  int(float(r[4])),int(float(r[5])),\n",
    "                                                                 int(float(r[6])),int(float(r[7])) ))\n",
    "\n",
    "\n",
    "# dta_splited.first()\n",
    "dta= dta_splited\n",
    "dta.cache()\n",
    "print dta.first()\n",
    "print dta.count()\n",
    "\n",
    "# dta_filtered = dta.filter(lambda line: line[0] <2000 )\n",
    "# dta_filtered.cache()\n",
    "# dta_filtered.count()\n",
    "# print dta.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: (46.7446171548, 4, 13, 105)\n",
      "1000000\n"
     ]
    }
   ],
   "source": [
    "# synthetic data\n",
    "dta_RDD = sc.textFile(\"file:///home/tguo/tian_src/synthetic_data.txt\")\n",
    "dta_RDD.cache()\n",
    "\n",
    "dta_splited = dta_RDD.map(lambda line: line.split(\",\")).map(lambda r: ( float(r[3]),int(r[0]),int(r[1]),int(r[2])) )\n",
    "# ( float(r[2]),float(r[0]),float(r[1]) ) \n",
    "\n",
    "dta= dta_splited\n",
    "dta.cache()\n",
    "\n",
    "print 'original:',dta.first()\n",
    "print dta.count()\n",
    "\n",
    "# re-set index of categorical features\n",
    "\n",
    "# feature_dist=[]\n",
    "\n",
    "# tmp1 = dta.map(lambda line: line[1]).distinct().collect()\n",
    "# feature_dist.append(tmp1)\n",
    "\n",
    "# tmp1 = dta.map(lambda line: line[2]).distinct().collect()\n",
    "# feature_dist.append(tmp1)\n",
    "\n",
    "# tmp1 = dta.map(lambda line: line[3]).distinct().collect()\n",
    "# feature_dist.append(tmp1)\n",
    "\n",
    "# fea_cnt= len(feature_dist )\n",
    "# fea_map=[]\n",
    "\n",
    "# for i in range(0, fea_cnt):\n",
    "#     tmpcnt = len(feature_dist[i])\n",
    "#     val_map= dict( zip(feature_dist[i], range(0,tmpcnt)) )\n",
    "#     fea_map.append(val_map)\n",
    "\n",
    "# def reset_index( line ):\n",
    "#     tmp=[]\n",
    "#     tmp.append(line[0])\n",
    "#     for i in range(1,4):\n",
    "#         tmp.append(fea_map[i-1][ line[i] ] )\n",
    "#     return tmp\n",
    "\n",
    "# dta = dta.map( lambda line: reset_index(line)  )\n",
    "\n",
    "# print 'feature value re-indexed:',dta.first()\n",
    "# print dta.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# basic functions in one-pass method [on the cluster side]\n",
    "\n",
    "# assign each data instance to the node\n",
    "def search_nodeToData(features, tree):\n",
    "    nodeNum=len(tree)   \n",
    "    if nodeNum == 0:\n",
    "        return 0;\n",
    "    \n",
    "    current_nodeIdx=0\n",
    "    while current_nodeIdx< nodeNum:\n",
    "        split_feature= tree[current_nodeIdx][0]\n",
    "        split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "        # unbalanced tree grow\n",
    "        if split_feature==-1:\n",
    "            current_nodeIdx=current_nodeIdx*2+1+ random.randint(0, 1)\n",
    "            continue\n",
    "            \n",
    "        if (( 1<<features[split_feature]) &  split_valueSet) != 0 :\n",
    "            current_nodeIdx=current_nodeIdx*2+1\n",
    "        else:\n",
    "            current_nodeIdx=current_nodeIdx*2+2\n",
    "            \n",
    "    return current_nodeIdx - nodeNum\n",
    "\n",
    "def partition_dataToNode(list_dvAndfeatures):\n",
    "    dataToNode_map=[]\n",
    "    res=[]\n",
    "    for dvAndfeatures in list_dvAndfeatures:\n",
    "        \n",
    "        Y= dvAndfeatures[0]\n",
    "        node= search_nodeToData(dvAndfeatures[1:numFeatures+1], \\\n",
    "                                currentNode_split_fromMaster.value)\n",
    "        dataToNode_map.append(node)\n",
    "        res.append( (node, dvAndfeatures )   )\n",
    "    return res\n",
    "def dataToNode_assignment( data_rdd ):\n",
    "    dataToNode_map = data_rdd.mapPartitions( partition_dataToNode )\n",
    "    dataToNode_map.cache()\n",
    "    return dataToNode_map\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "# extract values for each feature\n",
    "def partition_featureValues(list_dvAndfeatures):\n",
    "\n",
    "    feature_valueSet={}\n",
    "    \n",
    "    for i in range(0,numFeatures):\n",
    "        feature_valueSet.update( {i: set()} )\n",
    "        \n",
    "    for dvAndfeatures in list_dvAndfeatures:\n",
    "        for i in range(0,numFeatures):\n",
    "            feature_val= dvAndfeatures[i+1]\n",
    "            feature_valueSet[i].add(feature_val)\n",
    "    return zip( feature_valueSet.keys(), feature_valueSet.values() )  \n",
    "\n",
    "def merge_featureValues(  valueSet1, valueSet2):\n",
    "    return valueSet1.union(valueSet2)\n",
    "    \n",
    "def data_featureValues_collect( data_rdd ):\n",
    "    feature_valueSet_part = data_rdd.mapPartitions( partition_featureValues )\n",
    "    feature_valueSet_local = \\\n",
    "    feature_valueSet_part.reduceByKey(lambda set1, set2: merge_featureValues(set1,set2 )).collect()\n",
    "    feature_valueSet_local.sort()\n",
    "    #test\n",
    "#     print feature_valueSet_local\n",
    "    return feature_valueSet_local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 5), (3, 100)]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#debug\n",
    "\n",
    "# tmpr=[ ('b',1),('d',54),('c',2) ]\n",
    "\n",
    "\n",
    "tmpr1=[ 1,1,100 ,5]\n",
    "\n",
    "\n",
    "# tmpr=[ (100,1,1),(83,3,4),(67,3,5),(45,2,3) ]\n",
    "\n",
    "tmprdd= sc.parallelize(tmpr1)\n",
    "\n",
    "sorted_rdd = tmprdd.sortBy(lambda line:line, ascending=True)\n",
    "tmpziprdd=sorted_rdd.zipWithIndex()\n",
    "# map(lambda line: (line[1], line[0]) )\n",
    "\n",
    "lookup_rdd= tmpziprdd.map(lambda line: (line[1], line[0]) )\n",
    "\n",
    "print lookup_rdd.collect()\n",
    "\n",
    "print lookup_rdd.lookup(2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split decision [on the local side]\n",
    "\n",
    "#implementation notes:\n",
    "# bit-operation to record the set of selected feature values in the left child\n",
    "\n",
    "#asecending order, upper: larger, lower: smaller\n",
    "def upper_tail_boundary( percentage, dat_rdd, tt_cnt):\n",
    "    tmpcnt= int(tt_cnt*percentage) \n",
    "    tmplist = dat_rdd.top( int(tt_cnt*percentage)  )\n",
    "    \n",
    "    \n",
    "    return tmplist[tmpcnt-1]\n",
    "\n",
    "def lower_tail_boundary( percentage, dat_rdd, tt_cnt):\n",
    "#     print tt_cnt,percentage, type(dat_rdd)\n",
    "    sorted_rdd = dat_rdd.sortBy(lambda line:line, ascending= True).cache()\n",
    "    tmpcnt= int(tt_cnt*percentage) \n",
    "    tmplist= dat_rdd.take( tmpcnt )\n",
    "    return tmplist[ tmpcnt-1 ]\n",
    "    \n",
    "# def trimmed_MSE_cal(dat_rdd, dat_rdd_cnt, trim_percentage ):\n",
    "    \n",
    "#     upper_bound = upper_tail_boundary( trim_percentage, dat_rdd, dat_rdd_cnt)\n",
    "# #     lower_bound = lower_tail_boundary( trim_percentage, dat_rdd, dat_rdd_cnt)\n",
    "    \n",
    "#     trimmed_rdd= dat_rdd.filter( lambda line: line < upper_bound ).cache()\n",
    "# #     trimmed_rdd= trimmed_rdd.filter(lambda line: line > lower_bound).cache()\n",
    "#     trimmed_rdd_cnt = trimmed_rdd.count()\n",
    "    \n",
    "#     if trimmed_rdd_cnt ==0:\n",
    "        \n",
    "#         print 'trimmed zeor happens!'\n",
    "#         tmp_mean=0\n",
    "#         tmp_mse=0\n",
    "        \n",
    "#     else:\n",
    "#         tmp_mean = trimmed_rdd.reduce(lambda a,b:a+b) / trimmed_rdd_cnt\n",
    "#         tmp_mse= trimmed_rdd.map(lambda line:(line-tmp_mean)*(line-tmp_mean)).reduce(lambda a,b:a+b)/trimmed_rdd_cnt\n",
    "    \n",
    "#     return (trimmed_rdd_cnt, tmp_mean, tmp_mse, trimmed_rdd)\n",
    "\n",
    "def trimmed_MSE( dataToSplit_rdd, feature_id, value_set ):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.filter(lambda l:l[1][feature_id+1] in value_set).map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "    if tmp_rdd_count ==0:\n",
    "        \n",
    "        print 'trimmed zeor happens!'\n",
    "        tmp_mean=0\n",
    "        tmp_mse=0\n",
    "    else:\n",
    "        tmp_mean = tmp_rdd.reduce(lambda a,b:a+b)*1.0 / tmp_rdd_count\n",
    "        tmp_mse= tmp_rdd.map(lambda line:(line-tmp_mean)*(line-tmp_mean)).reduce(lambda a,b:a+b)*1.0/tmp_rdd_count\n",
    "    \n",
    "    return (tmp_rdd_count, tmp_mean, tmp_mse )\n",
    "    \n",
    "def trimmed_MSE_node( dataToSplit_rdd, trim_percentage ):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "    upper_bound = upper_tail_boundary( trim_percentage, tmp_rdd, tmp_rdd_count)\n",
    "#     lower_bound = lower_tail_boundary( trim_percentage, dat_rdd, dat_rdd_cnt)\n",
    "    tmp_trim_rdd= tmp_rdd.filter( lambda line: line < upper_bound ).cache()\n",
    " \n",
    "    trimmed_rdd= dataToSplit_rdd.filter( lambda line: line[1][0] < upper_bound ).cache()\n",
    "#     trimmed_rdd= trimmed_rdd.filter(lambda line: line > lower_bound).cache()\n",
    "    trimmed_rdd_cnt = trimmed_rdd.count()\n",
    "    \n",
    "    if trimmed_rdd_cnt ==0:\n",
    "        \n",
    "        print 'trimmed zeor happens!'\n",
    "        tmp_mean=0\n",
    "        tmp_mse=0\n",
    "        \n",
    "    else:\n",
    "        tmp_mean = tmp_trim_rdd.reduce(lambda a,b:a+b) / trimmed_rdd_cnt\n",
    "        tmp_mse= tmp_trim_rdd.map(lambda line:(line-tmp_mean)*(line-tmp_mean)).reduce(lambda a,b:a+b)/trimmed_rdd_cnt\n",
    "    \n",
    "    return (trimmed_rdd_cnt, tmp_mean, tmp_mse, trimmed_rdd)\n",
    "\n",
    "def split_onOneFeature_exact_trimmedMSE(node_data, node_data_cnt, TMSE_node_ini, feature_valueList, feature_id):\n",
    "    \n",
    "    left_count=0\n",
    "    right_count= node_data_cnt\n",
    "    bestSplitMetric=  TMSE_node_ini \n",
    "    \n",
    "    #------check the number of feature values------------------\n",
    "    if len(feature_valueList) <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    #-----------------------\n",
    "\n",
    "    #this is a bit-record variable\n",
    "    leftSplit_valueSet =0 \n",
    "    leftSplit_count=0 \n",
    "    rightSplit_count=0\n",
    "    \n",
    "    bestLeft=(-1,-1)\n",
    "    bestRight=(-1,-1)\n",
    "    \n",
    "    #sort feature values according to median \n",
    "    sorted_value_map=[]\n",
    "    \n",
    "    for i in feature_valueList:\n",
    "        currentVal = i\n",
    "        #          adapt to different loss function \n",
    "#-----------------------------------        \n",
    "        tmp_rdd=node_data.filter(lambda l:l[1][feature_id+1] == i ).map(lambda l:l[1][0]).cache()  \n",
    "        tmp_rdd_count = tmp_rdd.count()\n",
    "        \n",
    "        if tmp_rdd_count==0:\n",
    "            continue\n",
    "        else:\n",
    "            tmp_mean = tmp_rdd.reduce(lambda a,b:a+b)*1.0 / tmp_rdd_count\n",
    "# ------------------------------------\n",
    "        sorted_value_map.append( (tmp_mean,i) )\n",
    "        \n",
    "        \n",
    "    sorted_value_map.sort()\n",
    "    len_values= len(sorted_value_map)\n",
    "\n",
    "    # scan the split positions    \n",
    "    left_value_set=[]\n",
    "    right_value_set= copy.deepcopy(feature_valueList)\n",
    "    \n",
    "    #--------- initialize the value set ----------------------\n",
    "    \n",
    "    # check the number of feature values\n",
    "    if len_values <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    \n",
    "    current_feature_value = sorted_value_map[0][1] \n",
    "\n",
    "    left_value_set.append(current_feature_value)\n",
    "    right_value_set.remove(current_feature_value)\n",
    "        \n",
    "    #          adapt to different loss function     \n",
    "    left = trimmed_MSE( node_data, feature_id, left_value_set )\n",
    "    right= trimmed_MSE( node_data, feature_id, right_value_set )\n",
    "        \n",
    "    leftSplit_count = left[0]\n",
    "    leftMedian= left[1]\n",
    "    leftMetric= left[2]\n",
    "        \n",
    "    rightSplit_count= right[0]\n",
    "    rightMedian= right[1]\n",
    "    rightMetric= right[2]  \n",
    "        \n",
    "    current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "    bestSplitMetric=current_splitMetric     \n",
    "    bestLeft=left\n",
    "    bestRight=right\n",
    "    #bit variable to record which value is chosen for the left child\n",
    "    leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "    \n",
    "    #--------------------------------------------------------------\n",
    "    \n",
    "    for k in range(1,len_values-1):\n",
    "        \n",
    "        current_feature_value = sorted_value_map[k][1] \n",
    "        \n",
    "        left_value_set.append(current_feature_value)\n",
    "        right_value_set.remove(current_feature_value)\n",
    "        \n",
    "#          adapt to different loss function \n",
    "        left = trimmed_MSE( node_data, feature_id, left_value_set )\n",
    "        right= trimmed_MSE( node_data, feature_id, right_value_set )\n",
    "        \n",
    "        leftSplit_count = left[0]\n",
    "        leftMedian= left[1]\n",
    "        leftMetric= left[2]\n",
    "        \n",
    "        rightSplit_count= right[0]\n",
    "        rightMedian= right[1]\n",
    "        rightMetric= right[2]  \n",
    "        \n",
    "        current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "        if current_splitMetric < bestSplitMetric:\n",
    "            bestSplitMetric=current_splitMetric     \n",
    "            bestLeft=left\n",
    "            bestRight=right\n",
    "            #bit variable to record which value is chosen for the left child\n",
    "            leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "\n",
    "    return [ leftSplit_valueSet, bestLeft[1], bestRight[1], bestSplitMetric  ] \n",
    "#          value_set, left median, right median, weighted mad\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def median_MSE( dataToSplit_rdd, feature_id, value_set ):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.filter(lambda l:l[1][feature_id+1] in value_set).map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "    if tmp_rdd_count==0:\n",
    "        return (10000, 100000, 10000000)\n",
    "    \n",
    "    tmp_mean = tmp_rdd.reduce( lambda a,b: a+b)*1.0/tmp_rdd_count\n",
    "    tmpMSE= tmp_rdd.map( lambda a: (a-tmp_mean)*(a-tmp_mean) ).reduce( lambda a,b: a+b)*1.0/tmp_rdd_count\n",
    "    \n",
    "    return (tmp_rdd_count, tmp_mean, tmpMSE)\n",
    "\n",
    "def median_MSE_node( dataToSplit_rdd):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = dataToSplit_rdd.count()\n",
    "\n",
    "    if tmp_rdd_count==0:\n",
    "        return (10000, 100000, 10000000)\n",
    "    \n",
    "    tmp_mean = tmp_rdd.reduce( lambda a,b: a+b)*1.0/tmp_rdd_count\n",
    "    tmpMSE= tmp_rdd.map( lambda a: (a-tmp_mean)*(a-tmp_mean) ).reduce( lambda a,b: a+b)*1.0/tmp_rdd_count\n",
    "    \n",
    "    return (tmp_rdd_count, tmp_mean, tmpMSE)\n",
    "\n",
    "def split_onOneFeature_exact_MSE(node_data, node_data_cnt, MSE_node_ini, feature_valueList, feature_id):\n",
    "    \n",
    "    left_count=0\n",
    "    right_count= node_data_cnt\n",
    "    bestSplitMetric=  MSE_node_ini \n",
    "    \n",
    "    #------check the number of feature values------------------\n",
    "    if len(feature_valueList) <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    #-----------------------\n",
    "\n",
    "    #this is a bit-record variable\n",
    "    leftSplit_valueSet =0 \n",
    "    leftSplit_count=0  \n",
    "    rightSplit_count=0\n",
    "    \n",
    "    bestLeft=(-1,-1)\n",
    "    bestRight=(-1,-1)\n",
    "    \n",
    "    #sort feature values according to mean \n",
    "    sorted_value_map=[]\n",
    "    \n",
    "    for i in feature_valueList:\n",
    "        currentVal = i\n",
    "#-----------------------------------        \n",
    "        tmp_rdd=node_data.filter(lambda l:l[1][feature_id+1] == i ).map(lambda l:l[1][0]).cache()  \n",
    "        tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "        if tmp_rdd_count==0:\n",
    "            continue\n",
    "        else:\n",
    "            tmp_mean = tmp_rdd.reduce( lambda a,b: a+b)*1.0/tmp_rdd_count\n",
    "       \n",
    "        tmp_mse=(tmp_rdd_count, tmp_mean, 0)\n",
    "# ------------------------------------\n",
    "        sorted_value_map.append( (tmp_mse[1],i) )\n",
    "        \n",
    "    sorted_value_map.sort()\n",
    "    len_values= len(sorted_value_map)\n",
    "\n",
    "    # scan the split positions    \n",
    "    left_value_set=[]\n",
    "    right_value_set= copy.deepcopy(feature_valueList)\n",
    "    \n",
    "    #--------- initialize the value set ----------------------\n",
    "    \n",
    "    # check the number of feature values\n",
    "    if len_values <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    \n",
    "    current_feature_value = sorted_value_map[0][1] \n",
    "\n",
    "    left_value_set.append(current_feature_value)\n",
    "    right_value_set.remove(current_feature_value)\n",
    "        \n",
    "    left = median_MSE( node_data, feature_id, left_value_set )\n",
    "    right= median_MSE( node_data, feature_id, right_value_set )\n",
    "        \n",
    "    leftSplit_count = left[0]\n",
    "    leftMedian= left[1]\n",
    "    leftMetric= left[2]\n",
    "        \n",
    "    rightSplit_count= right[0]\n",
    "    rightMedian= right[1]\n",
    "    rightMetric= right[2]  \n",
    "        \n",
    "    #debug\n",
    "    if leftSplit_count+rightSplit_count!=node_data_cnt:\n",
    "        print 'problem in left and right'\n",
    "        \n",
    "    current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "    bestSplitMetric=current_splitMetric     \n",
    "    bestLeft=left\n",
    "    bestRight=right\n",
    "    #bit variable to record which value is chosen for the left child\n",
    "    leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "    \n",
    "    #--------------------------------------------------------------\n",
    "    \n",
    "    for k in range(1,len_values-1):\n",
    "        \n",
    "        current_feature_value = sorted_value_map[k][1] \n",
    "        \n",
    "        left_value_set.append(current_feature_value)\n",
    "        right_value_set.remove(current_feature_value)\n",
    "        \n",
    "        left = median_MSE( node_data, feature_id, left_value_set )\n",
    "        right= median_MSE( node_data, feature_id, right_value_set )\n",
    "        \n",
    "        leftSplit_count = left[0]\n",
    "        leftMedian= left[1]\n",
    "        leftMetric= left[2]\n",
    "        \n",
    "        rightSplit_count= right[0]\n",
    "        rightMedian= right[1]\n",
    "        rightMetric= right[2]  \n",
    "        \n",
    "        #debug\n",
    "        if leftSplit_count+rightSplit_count!=node_data_cnt:\n",
    "            print 'problem in left and right'\n",
    "        \n",
    "        \n",
    "        current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "        if current_splitMetric < bestSplitMetric:\n",
    "            bestSplitMetric=current_splitMetric     \n",
    "            bestLeft=left\n",
    "            bestRight=right\n",
    "            #bit variable to record which value is chosen for the left child\n",
    "            leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "\n",
    "    return [ leftSplit_valueSet, bestLeft[1], bestRight[1], bestSplitMetric  ] \n",
    "#          value_set, left median, right median, weighted mad\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#calculate median and MAD for a set of data in dataToSplit_rdd\n",
    "\n",
    "def median_MAD_cal( dataToSplit_rdd ):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd\n",
    "    tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "    if tmp_rdd_count==0:\n",
    "        return (10000, 100000, 10000000)\n",
    "    \n",
    "#     optimization\n",
    "    sorted_rdd = tmp_rdd.sortBy().zipWithIndex().map(lambda line: (line[1], line[0]) )\n",
    "    \n",
    "    \n",
    "    \n",
    "    if(tmp_rdd_count%2 ==0):\n",
    "        tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "        tmphalf.sort()\n",
    "        tmpcnt= len(tmphalf)\n",
    "        tmp_median = (tmphalf[0] + tmphalf[1])*1.0/2.0\n",
    "    else:\n",
    "        tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "        tmphalf.sort()\n",
    "        tmpcnt= len(tmphalf)\n",
    "        tmp_median = tmphalf[0]\n",
    "    \n",
    "    tmpMAD_rdd = tmp_rdd.map(lambda line: abs(line-tmp_median) ).cache()\n",
    "    \n",
    "    if(tmp_rdd_count%2 ==0):\n",
    "        tmphalf = tmpMAD_rdd.top( tmp_rdd_count/2+1  )\n",
    "        tmpcnt= len(tmphalf)\n",
    "        tmp_mad = (tmphalf[tmpcnt-1] + tmphalf[tmpcnt-2])*1.0/2.0\n",
    "    else:\n",
    "        tmphalf = tmpMAD_rdd.top( tmp_rdd_count/2+1  )\n",
    "        tmpcnt= len(tmphalf)\n",
    "        tmp_mad = tmphalf[tmpcnt-1]\n",
    "    \n",
    "    return (tmp_rdd_count, tmp_median, tmp_mad)\n",
    "\n",
    "\n",
    "def median_MAD( dataToSplit_rdd, feature_id, value_set ):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.filter(lambda l:l[1][feature_id+1] in value_set).map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = tmp_rdd.count()\n",
    "    return  median_MAD_cal( tmp_rdd )\n",
    "\n",
    "def median_MAD_node( dataToSplit_rdd):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = dataToSplit_rdd.count()\n",
    "    return  median_MAD_cal( tmp_rdd )\n",
    "\n",
    "def split_onOneFeature_exact_MAD(node_data, node_data_cnt, node_ini, feature_valueList, feature_id):\n",
    "    \n",
    "    left_count=0\n",
    "    right_count= node_data_cnt\n",
    "    bestSplitMetric=  node_ini \n",
    "    \n",
    "    #------check the number of feature values------------------\n",
    "    if len(feature_valueList) <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    #-----------------------\n",
    "\n",
    "    #this is a bit-record variable\n",
    "    leftSplit_valueSet =0 \n",
    "    leftSplit_count=0 \n",
    "    rightSplit_count=0\n",
    "    \n",
    "    bestLeft=(-1,-1)\n",
    "    bestRight=(-1,-1)\n",
    "    \n",
    "    #sort feature values according to median \n",
    "    sorted_value_map=[]\n",
    "    \n",
    "    for i in feature_valueList:\n",
    "        currentVal = i\n",
    "#         dataToFeatureValue_rdd=node_data.filter(lambda l:l[1][feature_id+1]==currentVal).map(lambda l:l[1][0]).cache()\n",
    "#         dataToFeatureValue_rdd_count = dataToFeatureValue_rdd.count()\n",
    "\n",
    "#-----------------------------------        \n",
    "        tmp_rdd=node_data.filter(lambda l:l[1][feature_id+1] == i ).map(lambda l:l[1][0]).cache()  \n",
    "        tmp_rdd_count = tmp_rdd.count()   \n",
    "        tmp_mad=  median_MAD_cal( tmp_rdd )\n",
    "# ------------------------------------\n",
    "        sorted_value_map.append( (tmp_mad[1],i) )\n",
    "        \n",
    "    sorted_value_map.sort()\n",
    "    len_values= len(sorted_value_map)\n",
    "\n",
    "    # scan the split positions    \n",
    "    left_value_set=[]\n",
    "    right_value_set= copy.deepcopy(feature_valueList)\n",
    "    \n",
    "    #--------- initialize the value set ----------------------\n",
    "    \n",
    "    # check the number of feature values\n",
    "    if len_values <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    \n",
    "    current_feature_value = sorted_value_map[0][1] \n",
    "\n",
    "    left_value_set.append(current_feature_value)\n",
    "    right_value_set.remove(current_feature_value)\n",
    "        \n",
    "    left = median_MAD( node_data, feature_id, left_value_set )\n",
    "    right= median_MAD( node_data, feature_id, right_value_set )\n",
    "        \n",
    "    leftSplit_count = left[0]\n",
    "    leftMedian= left[1]\n",
    "    leftMetric= left[2]\n",
    "        \n",
    "    rightSplit_count= right[0]\n",
    "    rightMedian= right[1]\n",
    "    rightMetric= right[2]  \n",
    "        \n",
    "    current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "    bestSplitMetric=current_splitMetric     \n",
    "    bestLeft=left\n",
    "    bestRight=right\n",
    "    #bit variable to record which value is chosen for the left child\n",
    "    leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "    \n",
    "    #--------------------------------------------------------------\n",
    "    \n",
    "    for k in range(1,len_values-1):\n",
    "        \n",
    "        current_feature_value = sorted_value_map[k][1] \n",
    "        \n",
    "        left_value_set.append(current_feature_value)\n",
    "        right_value_set.remove(current_feature_value)\n",
    "        \n",
    "        left = median_MAD( node_data, feature_id, left_value_set )\n",
    "        right= median_MAD( node_data, feature_id, right_value_set )\n",
    "        \n",
    "        leftSplit_count = left[0]\n",
    "        leftMedian= left[1]\n",
    "        leftMetric= left[2]\n",
    "        \n",
    "        rightSplit_count= right[0]\n",
    "        rightMedian= right[1]\n",
    "        rightMetric= right[2]  \n",
    "        \n",
    "        current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "        if current_splitMetric < bestSplitMetric:\n",
    "            bestSplitMetric=current_splitMetric     \n",
    "            bestLeft=left\n",
    "            bestRight=right\n",
    "            #bit variable to record which value is chosen for the left child\n",
    "            leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "\n",
    "    return [ leftSplit_valueSet, bestLeft[1], bestRight[1], bestSplitMetric  ] \n",
    "#          value_set, left median, right median, weighted mad\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#calculate median and MAD for a set of data in dataToSplit_rdd\n",
    "def median_LAD( dataToSplit_rdd, feature_id, value_set ):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.filter(lambda l:l[1][feature_id+1] in value_set).map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "    if tmp_rdd_count==0:\n",
    "        return (10000, 100000, 10000000)\n",
    "    \n",
    "#----------median computation optimization---------------------------------\n",
    "#     sorted_rdd = tmp_rdd.sortBy(lambda line: line).zipWithIndex().map(lambda line: (line[1], line[0]) )\n",
    "#     if(tmp_rdd_count%2 ==0):\n",
    "#         tmp1=sorted_rdd.lookup( tmp_rdd_count/2-1  )[0]\n",
    "#         tmp2=sorted_rdd.lookup( tmp_rdd_count/2  )[0]\n",
    "#         tmp_median = (tmp1 + tmp1)*1.0/2.0\n",
    "#     else:\n",
    "#         tmp_median = sorted_rdd.lookup( tmp_rdd_count/2  )[0]\n",
    "#-------------------------------------------------------------\n",
    "    \n",
    "    if(tmp_rdd_count%2 ==0):\n",
    "        tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "        tmphalf.sort()\n",
    "        tmpcnt= len(tmphalf)\n",
    "        tmp_median = (tmphalf[0] + tmphalf[1])*1.0/2.0\n",
    "    else:\n",
    "        tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "        tmphalf.sort()\n",
    "        tmpcnt= len(tmphalf)\n",
    "        tmp_median = tmphalf[0]\n",
    "    \n",
    "    tmpMAD_rdd = tmp_rdd.map(lambda line: abs(line-tmp_median) ).cache()\n",
    "    tmpMAD = tmpMAD_rdd.reduce( lambda a,b: a+b)*1.0 / tmp_rdd_count\n",
    "    \n",
    "    return (tmp_rdd_count, tmp_median, tmpMAD)\n",
    "\n",
    "def median_LAD_node( dataToSplit_rdd):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = dataToSplit_rdd.count()\n",
    " \n",
    "    if tmp_rdd_count==0:\n",
    "        return (10000, 100000, 10000000)\n",
    "    \n",
    "    #----------median computation optimization---------------------------------\n",
    "#     sorted_rdd = tmp_rdd.sortBy(lambda line: line).zipWithIndex().map(lambda line: (line[1], line[0]) )\n",
    "#     if(tmp_rdd_count%2 ==0):\n",
    "#         tmp1=sorted_rdd.lookup( tmp_rdd_count/2-1  )[0]\n",
    "#         tmp2=sorted_rdd.lookup( tmp_rdd_count/2  )[0]\n",
    "#         tmp_median = (tmp1 + tmp1)*1.0/2.0\n",
    "#     else:\n",
    "#         tmp_median = sorted_rdd.lookup( tmp_rdd_count/2  )[0]\n",
    "    #-------------------------------------------------------------\n",
    "    \n",
    "    if(tmp_rdd_count%2 ==0):\n",
    "        tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "        tmphalf.sort()\n",
    "        tmpcnt= len(tmphalf)\n",
    "        tmp_median = (tmphalf[0] + tmphalf[1])*1.0/2.0\n",
    "    else:\n",
    "        tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "        tmphalf.sort()\n",
    "        tmpcnt= len(tmphalf)\n",
    "        tmp_median = tmphalf[0]\n",
    "    \n",
    "    tmpMAD_rdd = tmp_rdd.map(lambda line: abs(line-tmp_median) ).cache()\n",
    "    tmpMAD = tmpMAD_rdd.reduce( lambda a,b: a+b)*1.0 / tmp_rdd_count\n",
    "    \n",
    "    return (tmp_rdd_count, tmp_median, tmpMAD)\n",
    "\n",
    "def split_onOneFeature_exact_LAD(node_data, node_data_cnt, LAD_node_ini, feature_valueList, feature_id):\n",
    "    \n",
    "    left_count=0\n",
    "    right_count= node_data_cnt\n",
    "    bestSplitMetric=  LAD_node_ini \n",
    "    \n",
    "    #------check the number of feature values------------------\n",
    "    if len(feature_valueList) <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    #-----------------------\n",
    "\n",
    "    #this is a bit-record variable\n",
    "    leftSplit_valueSet =0 \n",
    "    leftSplit_count=0 \n",
    "    rightSplit_count=0\n",
    "    \n",
    "    bestLeft=(-1,-1)\n",
    "    bestRight=(-1,-1)\n",
    "    \n",
    "    #sort feature values according to median \n",
    "    sorted_value_map=[]\n",
    "    \n",
    "    for i in feature_valueList:\n",
    "#         dataToFeatureValue_rdd=node_data.filter(lambda l:l[1][feature_id+1]==currentVal).map(lambda l:l[1][0]).cache()\n",
    "#         dataToFeatureValue_rdd_count = dataToFeatureValue_rdd.count()\n",
    "\n",
    "#-----------------------------------        \n",
    "        tmp_rdd=node_data.filter(lambda l:l[1][feature_id+1] == i ).map(lambda l:l[1][0]).cache()  \n",
    "        tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "        if tmp_rdd_count==0:\n",
    "            continue\n",
    "        else:\n",
    "            if(tmp_rdd_count%2 ==0):\n",
    "                tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "                tmphalf.sort()\n",
    "                tmpcnt= len(tmphalf)\n",
    "                tmp_median = (tmphalf[0] + tmphalf[1])*1.0/2.0\n",
    "            else:\n",
    "                tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "                tmphalf.sort()\n",
    "                tmpcnt= len(tmphalf)\n",
    "                tmp_median = tmphalf[0]\n",
    "    \n",
    "            tmpMAD_rdd = tmp_rdd.map(lambda line: abs(line-tmp_median) ).cache()\n",
    "            tmpMAD = tmpMAD_rdd.reduce( lambda a,b: a+b)\n",
    "       \n",
    "        tmp_mad=(tmp_rdd_count, tmp_median, tmpMAD)\n",
    "# ------------------------------------\n",
    "        sorted_value_map.append( (tmp_mad[1],i) )\n",
    "        \n",
    "    sorted_value_map.sort()\n",
    "    len_values= len(sorted_value_map)\n",
    "\n",
    "    # scan the split positions    \n",
    "    left_value_set=[]\n",
    "    right_value_set= copy.deepcopy(feature_valueList)\n",
    "    \n",
    "    #--------- initialize the value set ----------------------\n",
    "    \n",
    "    # check the number of feature values\n",
    "    if len_values <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    \n",
    "    current_feature_value = sorted_value_map[0][1] \n",
    "\n",
    "    left_value_set.append(current_feature_value)\n",
    "    right_value_set.remove(current_feature_value)\n",
    "        \n",
    "    left = median_LAD( node_data, feature_id, left_value_set )\n",
    "    right= median_LAD( node_data, feature_id, right_value_set )\n",
    "        \n",
    "    leftSplit_count = left[0]\n",
    "    leftMedian= left[1]\n",
    "    leftMetric= left[2]\n",
    "        \n",
    "    rightSplit_count= right[0]\n",
    "    rightMedian= right[1]\n",
    "    rightMetric= right[2]  \n",
    "        \n",
    "    current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "    bestSplitMetric=current_splitMetric     \n",
    "    bestLeft=left\n",
    "    bestRight=right\n",
    "    #bit variable to record which value is chosen for the left child\n",
    "    leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "    \n",
    "    #--------------------------------------------------------------\n",
    "    \n",
    "    for k in range(1,len_values-1):\n",
    "        \n",
    "        current_feature_value = sorted_value_map[k][1] \n",
    "        \n",
    "        left_value_set.append(current_feature_value)\n",
    "        right_value_set.remove(current_feature_value)\n",
    "        \n",
    "        left = median_LAD( node_data, feature_id, left_value_set )\n",
    "        right= median_LAD( node_data, feature_id, right_value_set )\n",
    "        \n",
    "        leftSplit_count = left[0]\n",
    "        leftMedian= left[1]\n",
    "        leftMetric= left[2]\n",
    "        \n",
    "        rightSplit_count= right[0]\n",
    "        rightMedian= right[1]\n",
    "        rightMetric= right[2]  \n",
    "        \n",
    "        current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "        if current_splitMetric < bestSplitMetric:\n",
    "            bestSplitMetric=current_splitMetric     \n",
    "            bestLeft=left\n",
    "            bestRight=right\n",
    "            #bit variable to record which value is chosen for the left child\n",
    "            leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "\n",
    "    return [ leftSplit_valueSet, bestLeft[1], bestRight[1], bestSplitMetric  ] \n",
    "#          value_set, left median, right median, weighted mad\n",
    "\n",
    "def find_bestSplit_exact( dataToNode, current_NumNodes, nodes_tree, nodes_tree_test, feature_valueList, loss_func):\n",
    "    \n",
    "    tmpnode_cnt=[]\n",
    "    presplit=[]\n",
    "        \n",
    "#------------- grow the unbalanced tree------------------------    \n",
    "    cur_node_num= len(nodes_tree)\n",
    "# -------------------------------------------------------------\n",
    "        \n",
    "    for i in range(0, current_NumNodes):\n",
    "    \n",
    "    #------------- grow the unbalanced tree ---------------------------------- \n",
    "        if cur_node_num >= 3:\n",
    "            parent_node = int(math.ceil(  (i + cur_node_num -2.0)*1.0/ 2 ))\n",
    "            parent_node_esti =nodes_tree_test[2* parent_node +1 ] \n",
    "            tmp_split_feature= nodes_tree[parent_node][0]\n",
    "            \n",
    "            if tmp_split_feature == -1:\n",
    "                nodes_tree.append(  (-1,  -1  )  )\n",
    "                nodes_tree_test.append( parent_node_esti  )\n",
    "                nodes_tree_test.append( parent_node_esti )\n",
    "                continue\n",
    "    #-------------------------------------------------------------------------\n",
    "        # prepare data for this node\n",
    "        current_node=i\n",
    "        current_node_data = dataToNode.filter( lambda line: line[0] == current_node ).cache()\n",
    "        current_node_data_cnt = current_node_data.count()\n",
    "        \n",
    "        # split initialization  \n",
    "        if loss_func == 'tmse':\n",
    "            \n",
    "            if abs(trimm_ratio-0.0)<1e-9:\n",
    "                \n",
    "                trimmed_rdd= current_node_data\n",
    "                trimmed_rdd_cnt = trimmed_rdd.count()\n",
    "                    \n",
    "                tmp_rdd=current_node_data.map(lambda l:l[1][0]).cache()\n",
    "            \n",
    "                tmp_mean = tmp_rdd.reduce(lambda a,b:a+b) / trimmed_rdd_cnt\n",
    "                tmp_mse= tmp_rdd.map(lambda line:(line-tmp_mean)*(line-tmp_mean)).reduce(lambda a,b:a+b)/trimmed_rdd_cnt\n",
    "                \n",
    "                \n",
    "                tmp_metric=(trimmed_rdd_cnt, tmp_mean, tmp_mse, trimmed_rdd )\n",
    "                \n",
    "            else:\n",
    "                tmp_metric =  trimmed_MSE_node( current_node_data, trimm_ratio )\n",
    "                \n",
    "                \n",
    "            best_splitMetric_sofar= tmp_metric[2]\n",
    "                \n",
    "#                  (trimmed_rdd_cnt, tmp_mean, tmp_mse, trimmed_rdd)\n",
    "                \n",
    "#                 tmp_rdd=current_node_data.map(lambda l:l[1][0]).cache()\n",
    "#                 tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "#                 upper_bound = upper_tail_boundary( trimm_ratio, tmp_rdd, tmp_rdd_count)\n",
    "#         #     lower_bound = lower_tail_boundary( trim_percentage, dat_rdd, dat_rdd_cnt)\n",
    "#                 tmp_trim_rdd= tmp_rdd.filter( lambda line: line < upper_bound ).cache()\n",
    " \n",
    "#                 trimmed_rdd= current_node_data.filter( lambda line: line[1][0] < upper_bound ).cache()\n",
    "#         #     trimmed_rdd= trimmed_rdd.filter(lambda line: line > lower_bound).cache()\n",
    "#                 trimmed_rdd_cnt = trimmed_rdd.count()\n",
    "            \n",
    "#                 if trimmed_rdd_cnt ==0:\n",
    "#                     print 'trimmed zeor happens!'\n",
    "#                     tmp_mean=0\n",
    "#                     tmp_mse=0\n",
    "#                 else:\n",
    "#                     tmp_mean = tmp_trim_rdd.reduce(lambda a,b:a+b) / trimmed_rdd_cnt\n",
    "#                     tmp_mse= tmp_trim_rdd.map(lambda line:(line-tmp_mean)*(line-tmp_mean)).reduce(lambda a,b:a+b)/trimmed_rdd_cnt\n",
    "                \n",
    "#             tmp_metric=(trimmed_rdd_cnt,tmp_mean,tmp_mse)\n",
    "            \n",
    "            \n",
    "    \n",
    "        elif loss_func == 'lad':\n",
    "                tmp_metric = median_LAD_node( current_node_data)\n",
    "                best_splitMetric_sofar= tmp_metric[2]\n",
    "        elif loss_func == 'mse':\n",
    "                tmp_metric = median_MSE_node( current_node_data)\n",
    "                best_splitMetric_sofar= tmp_metric[2]\n",
    "        elif loss_func == 'mad':\n",
    "                tmp_metric = median_MAD_node( current_node_data)\n",
    "                best_splitMetric_sofar= tmp_metric[2]\n",
    "        \n",
    "        \n",
    "        best_split=(-1,-1,-1,-1)\n",
    "        best_split_feature=-1\n",
    "        best_split_featureValueSet=-1\n",
    "                \n",
    "        #debug\n",
    "        tmpnode_cnt.append( current_node_data.count()  )\n",
    "        presplit.append( best_splitMetric_sofar/100000 )\n",
    "        \n",
    "        for j in range(0,numFeatures):    \n",
    "                                    \n",
    "            if loss_func == 'tmse':\n",
    "                \n",
    "                current_node_data= tmp_metric[3]\n",
    "                current_node_data_cnt= tmp_metric[0]\n",
    "                \n",
    "                cur_split=split_onOneFeature_exact_trimmedMSE(current_node_data,current_node_data_cnt,\\\n",
    "                                           best_splitMetric_sofar,feature_valueList[j][1],j)\n",
    "            elif loss_func == 'lad':\n",
    "                 cur_split=split_onOneFeature_exact_LAD(current_node_data,current_node_data_cnt,\\\n",
    "                                           best_splitMetric_sofar,feature_valueList[j][1],j)\n",
    "            elif loss_func == 'mad':\n",
    "                 cur_split=split_onOneFeature_exact_MAD(current_node_data,current_node_data_cnt,\\\n",
    "                                           best_splitMetric_sofar,feature_valueList[j][1],j)\n",
    "            elif loss_func == 'mse':\n",
    "                 cur_split=split_onOneFeature_exact_MSE(current_node_data,current_node_data_cnt,\\\n",
    "                                           best_splitMetric_sofar,feature_valueList[j][1],j)\n",
    "                \n",
    "            if  cur_split[0]!=-1  and cur_split[3] < best_splitMetric_sofar:\n",
    "                best_split=cur_split\n",
    "                best_splitMetric_sofar=cur_split[3]\n",
    "                best_split_feature = j\n",
    "                best_split_featureValueSet= cur_split[0]                  \n",
    "                \n",
    "#------------- grow the unbalanced tree ---------------------------------- \n",
    "        if best_split_feature == -1:\n",
    "            nodes_tree.append(  (-1,  -1  )  )\n",
    "            nodes_tree_test.append( tmp_metric[1]  )\n",
    "            nodes_tree_test.append( tmp_metric[1] )\n",
    "            continue\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "        # tree for spliting nodes        \n",
    "        nodes_tree.append(  (best_split_feature,  best_split_featureValueSet  )  )\n",
    "        \n",
    "        # tree for predicting   \n",
    "        nodes_tree_test.append(best_split[1])\n",
    "        nodes_tree_test.append(best_split[2])\n",
    "        \n",
    "#-----debug---------\n",
    "#     print tmpnode_cnt\n",
    "#     print presplit\n",
    "    \n",
    "#     return nodes_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of feature-value combinations: 125\n"
     ]
    }
   ],
   "source": [
    "# training and testing process: data preparation\n",
    "\n",
    "# with outliers\n",
    "tmpdta = dta.takeSample(False, 1000000, 12243)\n",
    "dta_train_all = sc.parallelize(tmpdta).cache().sample(False, .1, 12345)\n",
    "dta_test_all = sc.parallelize(tmpdta).cache().sample(False, .1, 43243)\n",
    "\n",
    "# configurate extraction\n",
    "print 'number of feature-value combinations:',len(dta_train_all.map(lambda line:(line[1],line[2],line[3])).distinct().collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98974\n",
      "99176\n",
      "number of feature-value combinations: 125\n"
     ]
    }
   ],
   "source": [
    "# training and testing process: clearned or noisy data ?\n",
    "\n",
    "# 98974\n",
    "# 99987\n",
    "\n",
    "dta_train = dta_train_all.filter(lambda line: line[0]<1000 )\n",
    "dta_test = dta_test_all.filter(lambda line: line[0]<1000 )\n",
    "\n",
    "print dta_train.count()\n",
    "print dta_test.count()\n",
    "print 'number of feature-value combinations:',len(dta_train.map(lambda line:(line[1],line[2],line[3])).distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# training and testing process: extract all the feature-value combinations \n",
    "# in the training data set\n",
    "\n",
    "total_featureVal_set=[]\n",
    "for i in range(0, numFeatures):\n",
    "    featureValues = dta_train.map(lambda line: line[1+i]).distinct().collect()\n",
    "    total_featureVal_set.append( featureValues)\n",
    "\n",
    "print 'done'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "2 2\n",
      "4 4\n",
      "8 8\n",
      "16 16\n",
      "32 32\n",
      "64 64\n",
      "128 128\n",
      "256 256\n",
      "512 512\n",
      "error at tree height 10 : 0 0\n",
      "running time at tree height 10 : 3698.15893102\n",
      "number of leaf nodes at tree height 10 : 1024\n"
     ]
    }
   ],
   "source": [
    "# training and testing process: main process\n",
    "\n",
    "#parameters\n",
    "maxdepth=10\n",
    "numFeatures=3\n",
    "trimm_ratio = 0.00\n",
    "# tmse (trimmed mse), lad, ma d, mse\n",
    "loss_func= 'tmse'\n",
    "\n",
    "def tree_test_mapFunc_median(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], node_split)\n",
    "    return (leaf_nodes[ tmpnode ]  - line[0])*(leaf_nodes[ tmpnode ]  - line[0])\n",
    "\n",
    "def tree_test_mapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], node_split)\n",
    "    return (leaf_nodes[ tmpnode ]  - line[0])*(leaf_nodes[ tmpnode ]  - line[0])\n",
    "    \n",
    "def tree_test( testData_rdd ):\n",
    "    err_rdd=testData_rdd.map( lambda line: tree_test_mapFunc(line))\n",
    "    err_sum = err_rdd.reduce( lambda a,b: a+b)\n",
    "    return err_sum / testData_rdd.count()\n",
    "\n",
    "# variables for the tree structure \n",
    "node_split=[]\n",
    "node_test=[]\n",
    "\n",
    "test_err = []\n",
    "train_err = []\n",
    "run_time=[]\n",
    "\n",
    "tree_history_split=[]\n",
    "tree_history_esti=[]\n",
    "tree_history_leaf=[]\n",
    "tree_history_runtime=[]\n",
    "\n",
    "\n",
    "#ini parameter on the cluster\n",
    "currentNode_split_fromMaster = sc.broadcast([])\n",
    "\n",
    "\n",
    "#prepare feature value set\n",
    "feature_valueSet = data_featureValues_collect( dta_train )\n",
    "feature_valueList=[]\n",
    "for i in range(0, len(feature_valueSet)):\n",
    "    feature_valueList.append( ( i, list(feature_valueSet[i][1]) ) )\n",
    "    \n",
    "#     tune the starting depth\n",
    "for i in range(10,maxdepth+1):\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    del node_split[:]\n",
    "    del node_test[:]\n",
    "    currentNode_split_fromMaster = sc.broadcast([])\n",
    "    \n",
    "    for cur_depth in range(0,i):\n",
    "        \n",
    "        current_nodeNum= int( math.pow(2,  cur_depth)) \n",
    "        dataToNode = dataToNode_assignment( dta_train )\n",
    "        \n",
    "        #debug\n",
    "        print  dataToNode.map(lambda line:line[0]).distinct().count() , current_nodeNum\n",
    "        \n",
    "        find_bestSplit_exact(dataToNode,current_nodeNum, node_split, node_test, feature_valueList,loss_func)   \n",
    "#         ( dataToNode, current_NumNodes, nodes_tree, nodes_tree_test, feature_valueSet):\n",
    "        #for whole tree\n",
    "        currentNode_split_fromMaster = sc.broadcast(node_split)\n",
    "    \n",
    "    end = time.time()\n",
    "    leaf_nodes = node_test[ len(node_test)-  (int)( math.pow(2,i)): len(node_test)  ]\n",
    "    \n",
    "    tmp_test_err= 0 \n",
    "#     tree_test( dta_test )\n",
    "    tmp_train_err= 0\n",
    "#     tree_test( dta_train )\n",
    "#     test_err.append( tmp_test_err)\n",
    "#     train_err.append( tmp_train_err )\n",
    "\n",
    "    elapsed = end-start\n",
    "    run_time.append(elapsed)\n",
    "\n",
    "    tree_history_split.append(copy.deepcopy(node_split) )\n",
    "    tree_history_esti.append( copy.deepcopy(node_test)  )\n",
    "    tree_history_leaf.append( copy.deepcopy(leaf_nodes)  )\n",
    "    tree_history_runtime.append( copy.deepcopy(run_time)  )\n",
    "        \n",
    "    print \"error at tree height\", i,\":\",  tmp_test_err, tmp_train_err\n",
    "    print \"running time at tree height\", i,\":\",  elapsed\n",
    "    print \"number of leaf nodes at tree height\", i,\":\",  len(leaf_nodes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp_split=tree_history_split\n",
    "tmp_esti= tree_history_esti\n",
    "tmp_leaf= tree_history_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82.06865487, 3, 12, 103)\n",
      "223\n",
      "446\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "print dta_test.first()\n",
    "err_rdd=dta_test.map(lambda line:search_nodeToData(line[1:numFeatures+1], node_split) ) \n",
    "print len(node_split)\n",
    "print len(node_test)\n",
    "print len(leaf_nodes)\n",
    "\n",
    "# print node_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.995640799666\n"
     ]
    }
   ],
   "source": [
    "err_rdd=dta_test.map(lambda line:(line[0],leaf_nodes[search_nodeToData(line[1:numFeatures+1], node_split) ], search_nodeToData(line[1:numFeatures+1], node_split) ) )\n",
    "\n",
    "print err_rdd.map( lambda line: (line[0]-line[1])*(line[0]-line[1])  ).reduce(lambda a, b: a+b)/err_rdd.count()\n",
    "# err_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1023\n",
      "1\n",
      "2046\n",
      "1\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "#---------------- LAD -------------------  data backup\n",
    "lad_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "lad_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "lad_tree_history_leaf =  copy.deepcopy( tree_history_leaf)\n",
    "lad_tree_history_runtime =  copy.deepcopy( tree_history_runtime)\n",
    "\n",
    "print len(lad_tree_history_split)\n",
    "print len(lad_tree_history_split[0])\n",
    "print len(lad_tree_history_esti)\n",
    "print len(lad_tree_history_esti[0])\n",
    "\n",
    "print len(lad_tree_history_leaf)\n",
    "print len(lad_tree_history_leaf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error: [0.9956407996656219]\n",
      "training error: [0.9995826286696848]\n",
      "1\n",
      "non_leaf_node count at previous depth 0\n",
      "depth 0 : 1\n",
      "[[3, 6, 4, 2, 5], [12, 15, 13, 11, 14], [105, 102, 103, 104, 101]] non-leaf node\n",
      "non_leaf_node count at previous depth 1\n",
      "depth 1 : 2\n",
      "[[3, 6, 4, 2, 5], [12, 15, 13, 11, 14], [103, 104, 105]] non-leaf node\n",
      "[[3, 6, 4, 2, 5], [12, 15, 13, 11, 14], [101, 102]] non-leaf node\n",
      "non_leaf_node count at previous depth 2\n",
      "depth 2 : 4\n",
      "[[3, 6, 4, 2, 5], [15], [103, 104, 105]] non-leaf node\n",
      "[[3, 6, 4, 2, 5], [11, 12, 13, 14], [103, 104, 105]] non-leaf node\n",
      "[[3], [12, 15, 13, 11, 14], [101, 102]] non-leaf node\n",
      "[[2, 4, 5, 6], [12, 15, 13, 11, 14], [101, 102]] non-leaf node\n",
      "non_leaf_node count at previous depth 4\n",
      "depth 3 : 8\n",
      "[[3, 6, 4, 2, 5], [15], [103]] non-leaf node\n",
      "[[3, 6, 4, 2, 5], [15], [104, 105]] non-leaf node\n",
      "[[2, 4, 6], [11, 12, 13, 14], [103, 104, 105]] non-leaf node\n",
      "[[3, 5], [11, 12, 13, 14], [103, 104, 105]] non-leaf node\n",
      "[[3], [11, 12, 13, 15], [101, 102]] non-leaf node\n",
      "[[3], [14], [101, 102]] non-leaf node\n",
      "[[5], [12, 15, 13, 11, 14], [101, 102]] non-leaf node\n",
      "[[2, 4, 6], [12, 15, 13, 11, 14], [101, 102]] non-leaf node\n",
      "non_leaf_node count at previous depth 8\n",
      "depth 4 : 16\n",
      "[[3, 4, 5, 6], [15], [103]] non-leaf node\n",
      "[[2], [15], [103]] has the estimate: 49.9991214206 true: 50.013260243\n",
      "[[2, 3, 5], [15], [104, 105]] non-leaf node\n",
      "[[4, 6], [15], [104, 105]] non-leaf node\n",
      "[[2, 4, 6], [12, 13, 14], [103, 104, 105]] non-leaf node\n",
      "[[2, 4, 6], [11], [103, 104, 105]] non-leaf node\n",
      "[[3, 5], [11], [103, 104, 105]] non-leaf node\n",
      "[[3, 5], [12, 13, 14], [103, 104, 105]] non-leaf node\n",
      "[[3], [11, 12, 15], [101, 102]] non-leaf node\n",
      "[[3], [13], [101, 102]] non-leaf node\n",
      "[[3], [14], [101]] has the estimate: 64.0048846438 true: 64.0108174403\n",
      "[[3], [14], [102]] has the estimate: 98.0066675884 true: 98.0117318257\n",
      "[[5], [11, 14], [101, 102]] non-leaf node\n",
      "[[5], [12, 13, 15], [101, 102]] non-leaf node\n",
      "[[2, 4, 6], [12, 13, 14, 15], [101, 102]] non-leaf node\n",
      "[[2, 4, 6], [11], [101, 102]] non-leaf node\n",
      "non_leaf_node count at previous depth 13\n",
      "depth 5 : 26\n",
      "[[3, 5, 6], [15], [103]] non-leaf node\n",
      "[[4], [15], [103]] has the estimate: 28.0058711142 true: 28.0016550121\n",
      "[[2, 3, 5], [15], [105]] non-leaf node\n",
      "[[2, 3, 5], [15], [104]] non-leaf node\n",
      "[[4, 6], [15], [104]] non-leaf node\n",
      "[[4, 6], [15], [105]] non-leaf node\n",
      "[[2], [12, 13, 14], [103, 104, 105]] non-leaf node\n",
      "[[4, 6], [12, 13, 14], [103, 104, 105]] non-leaf node\n",
      "[[2, 4, 6], [11], [104]] non-leaf node\n",
      "[[2, 4, 6], [11], [105, 103]] non-leaf node\n",
      "[[3, 5], [11], [105]] non-leaf node\n",
      "[[3, 5], [11], [104, 103]] non-leaf node\n",
      "[[3, 5], [12, 13, 14], [104, 105]] non-leaf node\n",
      "[[3, 5], [12, 13, 14], [103]] non-leaf node\n",
      "[[3], [12], [101, 102]] non-leaf node\n",
      "[[3], [11, 15], [101, 102]] non-leaf node\n",
      "[[3], [13], [101]] has the estimate: 43.0053549745 true: 43.002373141\n",
      "[[3], [13], [102]] has the estimate: 44.0270377039 true: 44.0231889534\n",
      "[[5], [11, 14], [101]] non-leaf node\n",
      "[[5], [11, 14], [102]] non-leaf node\n",
      "[[5], [12, 13, 15], [102]] non-leaf node\n",
      "[[5], [12, 13, 15], [101]] non-leaf node\n",
      "[[2, 4, 6], [14, 15], [101, 102]] non-leaf node\n",
      "[[2, 4, 6], [12, 13], [101, 102]] non-leaf node\n",
      "[[2, 4, 6], [11], [101]] non-leaf node\n",
      "[[2, 4, 6], [11], [102]] non-leaf node\n",
      "non_leaf_node count at previous depth 23\n",
      "depth 6 : 46\n",
      "[[5], [15], [103]] has the estimate: 18.9778984604 true: 18.9899820897\n",
      "[[3, 6], [15], [103]] non-leaf node\n",
      "[[2], [15], [105]] has the estimate: 4.00165873843 true: 4.00215857602\n",
      "[[3, 5], [15], [105]] non-leaf node\n",
      "[[3, 5], [15], [104]] non-leaf node\n",
      "[[2], [15], [104]] has the estimate: 68.9948263217 true: 68.9930862807\n",
      "[[6], [15], [104]] has the estimate: 19.9916834318 true: 19.9932240324\n",
      "[[4], [15], [104]] has the estimate: 40.0201473551 true: 40.0279386103\n",
      "[[4], [15], [105]] has the estimate: 46.0130510888 true: 46.0123991499\n",
      "[[6], [15], [105]] has the estimate: 56.0251672235 true: 56.0185754936\n",
      "[[2], [13, 14], [103, 104, 105]] non-leaf node\n",
      "[[2], [12], [103, 104, 105]] non-leaf node\n",
      "[[4, 6], [12, 13, 14], [103]] non-leaf node\n",
      "[[4, 6], [12, 13, 14], [104, 105]] non-leaf node\n",
      "[[2, 6], [11], [104]] non-leaf node\n",
      "[[4], [11], [104]] has the estimate: 73.9759418647 true: 73.9894958373\n",
      "[[4], [11], [105, 103]] non-leaf node\n",
      "[[2, 6], [11], [105, 103]] non-leaf node\n",
      "[[3], [11], [105]] has the estimate: 5.98650306166 true: 5.98850077398\n",
      "[[5], [11], [105]] has the estimate: 23.0212523983 true: 23.016862869\n",
      "[[3], [11], [104, 103]] non-leaf node\n",
      "[[5], [11], [104, 103]] non-leaf node\n",
      "[[3, 5], [12, 14], [104, 105]] non-leaf node\n",
      "[[3, 5], [13], [104, 105]] non-leaf node\n",
      "[[3, 5], [12], [103]] non-leaf node\n",
      "[[3, 5], [13, 14], [103]] non-leaf node\n",
      "[[3], [12], [101]] has the estimate: 0.98407181416 true: 1.00015268596\n",
      "[[3], [12], [102]] has the estimate: 86.990037832 true: 86.9929917877\n",
      "[[3], [11, 15], [102]] non-leaf node\n",
      "[[3], [11, 15], [101]] non-leaf node\n",
      "[[5], [14], [101]] has the estimate: 32.9947211827 true: 33.0003375314\n",
      "[[5], [11], [101]] has the estimate: 52.9916260905 true: 53.0001663025\n",
      "[[5], [11], [102]] has the estimate: 31.0109531073 true: 31.01053631\n",
      "[[5], [14], [102]] has the estimate: 86.0064516275 true: 86.0047760517\n",
      "[[5], [13], [102]] has the estimate: 15.0014498267 true: 15.0099749393\n",
      "[[5], [12, 15], [102]] non-leaf node\n",
      "[[5], [15], [101]] has the estimate: 59.0083355203 true: 58.9958273151\n",
      "[[5], [12, 13], [101]] non-leaf node\n",
      "[[2, 6], [14, 15], [101, 102]] non-leaf node\n",
      "[[4], [14, 15], [101, 102]] non-leaf node\n",
      "[[4], [12, 13], [101, 102]] non-leaf node\n",
      "[[2, 6], [12, 13], [101, 102]] non-leaf node\n",
      "[[4], [11], [101]] has the estimate: 64.988892051 true: 64.9939844406\n",
      "[[2, 6], [11], [101]] non-leaf node\n",
      "[[6], [11], [102]] has the estimate: 38.9990872155 true: 39.0058429546\n",
      "[[2, 4], [11], [102]] non-leaf node\n",
      "non_leaf_node count at previous depth 26\n",
      "depth 7 : 52\n",
      "[[6], [15], [103]] has the estimate: 21.0001800609 true: 20.9961955052\n",
      "[[3], [15], [103]] has the estimate: 22.0050322476 true: 22.0092691727\n",
      "[[3], [15], [105]] has the estimate: 33.0302823068 true: 33.0071112668\n",
      "[[5], [15], [105]] has the estimate: 38.0071455412 true: 37.9995834848\n",
      "[[5], [15], [104]] has the estimate: 31.0145964553 true: 31.019660619\n",
      "[[3], [15], [104]] has the estimate: 35.9686470877 true: 35.9701566616\n",
      "[[2], [13, 14], [104]] non-leaf node\n",
      "[[2], [13, 14], [105, 103]] non-leaf node\n",
      "[[2], [12], [104, 105]] non-leaf node\n",
      "[[2], [12], [103]] has the estimate: 78.0062375921 true: 77.9986071586\n",
      "[[4, 6], [13, 14], [103]] non-leaf node\n",
      "[[4, 6], [12], [103]] non-leaf node\n",
      "[[4, 6], [13, 14], [104, 105]] non-leaf node\n",
      "[[4, 6], [12], [104, 105]] non-leaf node\n",
      "[[2], [11], [104]] has the estimate: 41.0379335563 true: 41.0131422484\n",
      "[[6], [11], [104]] has the estimate: 42.0222350319 true: 42.0294826411\n",
      "[[4], [11], [103]] has the estimate: 23.9976352613 true: 23.9938956907\n",
      "[[4], [11], [105]] has the estimate: 59.0067443122 true: 59.0119619662\n",
      "[[6], [11], [105, 103]] non-leaf node\n",
      "[[2], [11], [105, 103]] non-leaf node\n",
      "[[3], [11], [104]] has the estimate: 43.0107172381 true: 43.00773715\n",
      "[[3], [11], [103]] has the estimate: 44.0141009865 true: 44.0146804736\n",
      "[[5], [11], [103]] has the estimate: 65.9989701762 true: 65.9915412161\n",
      "[[5], [11], [104]] has the estimate: 97.0070914367 true: 96.9936626722\n",
      "[[5], [12, 14], [104, 105]] non-leaf node\n",
      "[[3], [12, 14], [104, 105]] non-leaf node\n",
      "[[3, 5], [13], [105]] non-leaf node\n",
      "[[3, 5], [13], [104]] non-leaf node\n",
      "[[5], [12], [103]] has the estimate: 46.9866226721 true: 46.9837469047\n",
      "[[3], [12], [103]] has the estimate: 83.0392829027 true: 83.0245273577\n",
      "[[3, 5], [13], [103]] non-leaf node\n",
      "[[3, 5], [14], [103]] non-leaf node\n",
      "[[3], [11], [102]] has the estimate: 9.00273447386 true: 8.99262494321\n",
      "[[3], [15], [102]] has the estimate: 24.0124447304 true: 24.0029988435\n",
      "[[3], [15], [101]] has the estimate: 24.9822151594 true: 24.9849783511\n",
      "[[3], [11], [101]] has the estimate: 95.9909205747 true: 96.0067309348\n",
      "[[5], [12], [102]] has the estimate: 52.0238611213 true: 52.0129117353\n",
      "[[5], [15], [102]] has the estimate: 72.0064305287 true: 71.9985880265\n",
      "[[5], [13], [101]] has the estimate: 78.9901063941 true: 79.0007800594\n",
      "[[5], [12], [101]] has the estimate: 98.0022728251 true: 98.0036535328\n",
      "[[2, 6], [14, 15], [102]] non-leaf node\n",
      "[[2, 6], [14, 15], [101]] non-leaf node\n",
      "[[4], [14, 15], [101]] non-leaf node\n",
      "[[4], [14, 15], [102]] non-leaf node\n",
      "[[4], [12, 13], [102]] non-leaf node\n",
      "[[4], [12, 13], [101]] non-leaf node\n",
      "[[2, 6], [12, 13], [102]] non-leaf node\n",
      "[[2, 6], [12, 13], [101]] non-leaf node\n",
      "[[2], [11], [101]] has the estimate: 85.9916665507 true: 85.9993586397\n",
      "[[6], [11], [101]] has the estimate: 94.9838380186 true: 94.990249165\n",
      "[[4], [11], [102]] has the estimate: 89.9915188978 true: 89.9969200464\n",
      "[[2], [11], [102]] has the estimate: 91.9720407436 true: 91.9817408928\n",
      "non_leaf_node count at previous depth 23\n",
      "depth 8 : 46\n",
      "[[2], [14], [104]] has the estimate: 3.00733360837 true: 3.00303463103\n",
      "[[2], [13], [104]] has the estimate: 6.99357302519 true: 7.01377299614\n",
      "[[2], [13], [105, 103]] non-leaf node\n",
      "[[2], [14], [105, 103]] non-leaf node\n",
      "[[2], [12], [105]] has the estimate: 40.0043542383 true: 40.0044200243\n",
      "[[2], [12], [104]] has the estimate: 44.0157791173 true: 44.0076353072\n",
      "[[4, 6], [14], [103]] non-leaf node\n",
      "[[4, 6], [13], [103]] non-leaf node\n",
      "[[6], [12], [103]] has the estimate: 43.0116333101 true: 42.9951159708\n",
      "[[4], [12], [103]] has the estimate: 43.9815313216 true: 43.9874883511\n",
      "[[4, 6], [13], [104, 105]] non-leaf node\n",
      "[[4, 6], [14], [104, 105]] non-leaf node\n",
      "[[4, 6], [12], [105]] non-leaf node\n",
      "[[4, 6], [12], [104]] non-leaf node\n",
      "[[6], [11], [105]] has the estimate: 59.9892380189 true: 59.9984621632\n",
      "[[6], [11], [103]] has the estimate: 78.9903943849 true: 78.9955525935\n",
      "[[2], [11], [103]] has the estimate: 65.9883869941 true: 65.9944841136\n",
      "[[2], [11], [105]] has the estimate: 72.0061185406 true: 72.0052752676\n",
      "[[5], [12, 14], [104]] non-leaf node\n",
      "[[5], [12, 14], [105]] non-leaf node\n",
      "[[3], [12, 14], [105]] non-leaf node\n",
      "[[3], [12, 14], [104]] non-leaf node\n",
      "[[3], [13], [105]] has the estimate: 45.9960221993 true: 45.99155227\n",
      "[[5], [13], [105]] has the estimate: 71.0148798107 true: 70.9990752173\n",
      "[[5], [13], [104]] has the estimate: 84.9979361518 true: 84.98527666\n",
      "[[3], [13], [104]] has the estimate: 94.9725436527 true: 94.9783130834\n",
      "[[5], [13], [103]] has the estimate: 87.0202157068 true: 86.9979121527\n",
      "[[3], [13], [103]] has the estimate: 92.0164350909 true: 92.0129763866\n",
      "[[3], [14], [103]] has the estimate: 95.996754406 true: 96.0069772528\n",
      "[[5], [14], [103]] has the estimate: 98.9952902817 true: 98.9890166399\n",
      "[[2, 6], [15], [102]] non-leaf node\n",
      "[[2, 6], [14], [102]] non-leaf node\n",
      "[[2, 6], [15], [101]] non-leaf node\n",
      "[[2, 6], [14], [101]] non-leaf node\n",
      "[[4], [15], [101]] has the estimate: 9.98711125406 true: 9.99415170596\n",
      "[[4], [14], [101]] has the estimate: 67.024178404 true: 67.0235774897\n",
      "[[4], [14], [102]] has the estimate: 75.0143158978 true: 75.0094048181\n",
      "[[4], [15], [102]] has the estimate: 92.0063814543 true: 92.0121357852\n",
      "[[4], [12], [102]] has the estimate: 30.9946543117 true: 30.9845076033\n",
      "[[4], [13], [102]] has the estimate: 79.0250423614 true: 79.0174192626\n",
      "[[4], [13], [101]] has the estimate: 23.9932821866 true: 24.0014912123\n",
      "[[4], [12], [101]] has the estimate: 67.0238555541 true: 67.0191683247\n",
      "[[2], [12, 13], [102]] non-leaf node\n",
      "[[6], [12, 13], [102]] non-leaf node\n",
      "[[2, 6], [13], [101]] non-leaf node\n",
      "[[2, 6], [12], [101]] non-leaf node\n",
      "non_leaf_node count at previous depth 20\n",
      "depth 9 : 40\n",
      "[[2], [13], [103]] has the estimate: 7.00731824534 true: 7.00138108126\n",
      "[[2], [13], [105]] has the estimate: 17.9805089885 true: 17.9884210098\n",
      "[[2], [14], [105]] has the estimate: 14.9795530028 true: 14.9815963992\n",
      "[[2], [14], [103]] has the estimate: 93.9806973802 true: 93.9945228452\n",
      "[[4], [14], [103]] has the estimate: 18.9798777465 true: 18.9902495167\n",
      "[[6], [14], [103]] has the estimate: 43.0035952528 true: 43.0126956745\n",
      "[[6], [13], [103]] has the estimate: 24.9650580438 true: 24.9702929476\n",
      "[[4], [13], [103]] has the estimate: 29.9819014059 true: 29.9881783815\n",
      "[[4, 6], [13], [104]] non-leaf node\n",
      "[[4, 6], [13], [105]] non-leaf node\n",
      "[[4, 6], [14], [105]] non-leaf node\n",
      "[[4, 6], [14], [104]] non-leaf node\n",
      "[[4], [12], [105]] has the estimate: 37.0195316942 true: 37.0105918854\n",
      "[[6], [12], [105]] has the estimate: 59.0038719338 true: 59.0115838965\n",
      "[[6], [12], [104]] has the estimate: 55.0071062883 true: 55.0236423302\n",
      "[[4], [12], [104]] has the estimate: 84.011516197 true: 84.0115118173\n",
      "[[5], [14], [104]] has the estimate: 10.0169959663 true: 10.0096926789\n",
      "[[5], [12], [104]] has the estimate: 18.9969063413 true: 19.0011851842\n",
      "[[5], [14], [105]] has the estimate: 18.9924209024 true: 19.0011316901\n",
      "[[5], [12], [105]] has the estimate: 88.0110875817 true: 88.0138987012\n",
      "[[3], [12], [105]] has the estimate: 36.9818298449 true: 36.9964813123\n",
      "[[3], [14], [105]] has the estimate: 94.9781458985 true: 94.9859320411\n",
      "[[3], [14], [104]] has the estimate: 63.9875488886 true: 63.9890660809\n",
      "[[3], [12], [104]] has the estimate: 87.991178963 true: 88.0093942938\n",
      "[[6], [15], [102]] has the estimate: 12.0002028299 true: 12.0095505068\n",
      "[[2], [15], [102]] has the estimate: 69.0072550605 true: 68.9910417034\n",
      "[[2], [14], [102]] has the estimate: 26.9900576411 true: 26.988800372\n",
      "[[6], [14], [102]] has the estimate: 43.9986258273 true: 43.9928846374\n",
      "[[6], [15], [101]] has the estimate: 38.0022911139 true: 38.0161690526\n",
      "[[2], [15], [101]] has the estimate: 74.9870226401 true: 74.9900677363\n",
      "[[2], [14], [101]] has the estimate: 33.0040166129 true: 33.0007037147\n",
      "[[6], [14], [101]] has the estimate: 66.021555134 true: 66.0112470676\n",
      "[[2], [13], [102]] has the estimate: 48.9917064225 true: 49.0077432971\n",
      "[[2], [12], [102]] has the estimate: 64.9954677347 true: 64.9964949594\n",
      "[[6], [12], [102]] has the estimate: 69.9896529486 true: 69.9898118374\n",
      "[[6], [13], [102]] has the estimate: 75.9813648181 true: 75.996306234\n",
      "[[6], [13], [101]] has the estimate: 69.0007357301 true: 68.9930255652\n",
      "[[2], [13], [101]] has the estimate: 82.9963731087 true: 83.0010942975\n",
      "[[2], [12], [101]] has the estimate: 79.9919546017 true: 79.9845139398\n",
      "[[6], [12], [101]] has the estimate: 99.0023067963 true: 98.9955779951\n",
      "bottom depth 10 : 8\n",
      "[[4], [13], [104]] has the estimate: 47.0194906275 true: 40.9892770289\n",
      "[[6], [13], [104]] has the estimate: 47.0194906275 true: 47.010797649\n",
      "[[4], [13], [105]] has the estimate: 55.9902972564 true: 46.998760482\n",
      "[[6], [13], [105]] has the estimate: 55.9902972564 true: 55.9995807432\n",
      "[[6], [14], [105]] has the estimate: 50.9964759721 true: 0.980560668562\n",
      "[[4], [14], [105]] has the estimate: 50.9964759721 true: 51.0082898193\n",
      "[[4], [14], [104]] has the estimate: 59.9928258973 true: 51.0221327523\n",
      "[[6], [14], [104]] has the estimate: 59.9928258973 true: 59.9858668153\n",
      "inter-node number: 117\n",
      " *******  number of identified configurations: 125\n"
     ]
    }
   ],
   "source": [
    "# LAD result statistic: node infor.\n",
    "\n",
    "# lad_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "# lad_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "\n",
    "print 'test error:',test_err\n",
    "print 'training error:',train_err\n",
    "print len(lad_tree_history_split)\n",
    "\n",
    "recog_conf=bfs_tree(lad_tree_history_split[ 0 ], total_featureVal_set, maxdepth-1,  lad_tree_history_esti[0], dta_train )  \n",
    "\n",
    "print len(recog_conf[0])\n",
    "print recog_conf[1]\n",
    "print recog_conf[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error at depth 1 : 763.63942691\n",
      "test error at depth 2 : 721.144419273\n",
      "test error at depth 3 : 668.947731652\n",
      "test error at depth 4 : 655.282307764\n",
      "test error at depth 5 : 581.619474391\n",
      "test error at depth 6 : 370.340723793\n",
      "test error at depth 7 : 243.995752216\n",
      "test error at depth 8 : 194.326904916\n",
      "test error at depth 9 : 20.6967571431\n",
      "test error at depth 10 : 0.995640799666\n"
     ]
    }
   ],
   "source": [
    "# LAD result statistic : test error\n",
    "\n",
    "pre=0\n",
    "tree_esti = lad_tree_history_esti[0]\n",
    "tree_split =  lad_tree_history_split[ 0 ]\n",
    "\n",
    "for i in range(1,maxdepth+1):\n",
    "            \n",
    "    end_bound= pre+ (int)( math.pow(2,i)) \n",
    "    test_leaf_nodes = tree_esti[ pre: end_bound  ]\n",
    "    pre= end_bound        \n",
    "    test_tree_split= tree_split[ 0:(int)( math.pow(2,i)-1)  ]\n",
    "    \n",
    "#     print len(test_tree_split)\n",
    "#     print len(test_leaf_nodes)\n",
    "\n",
    "    test_error_tree( i, dta_test  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n"
     ]
    }
   ],
   "source": [
    "#--------------- MAD --------------- result backup\n",
    "\n",
    "mad_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "mad_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "mad_tree_history_leaf =  copy.deepcopy( tree_history_leaf)\n",
    "mad_tree_history_runtime =  copy.deepcopy( tree_history_runtime)\n",
    "\n",
    "\n",
    "print len(mad_tree_history_split)\n",
    "print len(mad_tree_history_split[0])\n",
    "print len(mad_tree_history_esti)\n",
    "print len(mad_tree_history_esti[0])\n",
    "\n",
    "print len(mad_tree_history_leaf)\n",
    "print len(mad_tree_history_leaf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MAD result statistic\n",
    "# mad_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "# mad_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "\n",
    "# for i in range(1, maxdepth):\n",
    "#     print '-------tree with depth: ',i,'-----------'\n",
    "#     bfs_tree(mse_tree_history_esti[i], total_featureVal_set, i)\n",
    "\n",
    "print 'test error:',test_err\n",
    "print 'training error:',train_err\n",
    "print len(mad_tree_history_split)\n",
    "\n",
    "bfs_tree(mad_tree_history_split[ maxdepth-1 ], total_featureVal_set, maxdepth-1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MAD result statistic : test error\n",
    "\n",
    "pre=0\n",
    "tree_esti = mad_tree_history_esti[0]\n",
    "tree_split =  mad_tree_history_split[ 0 ]\n",
    "\n",
    "for i in range(1,maxdepth+1):\n",
    "            \n",
    "    end_bound= pre+ (int)( math.pow(2,i)) \n",
    "    test_leaf_nodes = tree_esti[ pre: end_bound  ]\n",
    "    pre= end_bound        \n",
    "    test_tree_split= tree_split[ 0:(int)( math.pow(2,i)-1)  ]\n",
    "    \n",
    "#     print len(test_tree_split)\n",
    "#     print len(test_leaf_nodes)\n",
    "\n",
    "    test_error_tree( i, dta_test  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1023\n",
      "1\n",
      "2046\n",
      "1\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "#---------------MSE-------------- result backup\n",
    "\n",
    "mse_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "mse_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "mse_tree_history_leaf =  copy.deepcopy( tree_history_leaf)\n",
    "mse_tree_history_runtime =  copy.deepcopy( tree_history_runtime)\n",
    "\n",
    "\n",
    "print len(mse_tree_history_split)\n",
    "print len(mse_tree_history_split[0])\n",
    "print len(mse_tree_history_esti)\n",
    "print len(mse_tree_history_esti[0])\n",
    "\n",
    "print len(mse_tree_history_leaf)\n",
    "print len(mse_tree_history_leaf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error: []\n",
      "training error: []\n",
      "1\n",
      "non_leaf_node count at previous depth 0\n",
      "depth 0 : 1\n",
      "[[3, 6, 4, 2, 5], [12, 15, 13, 11, 14], [105, 102, 103, 104, 101]] non-leaf node\n",
      "non_leaf_node count at previous depth 1\n",
      "depth 1 : 2\n",
      "[[2], [12, 15, 13, 11, 14], [105, 102, 103, 104, 101]] non-leaf node\n",
      "[[3, 4, 5, 6], [12, 15, 13, 11, 14], [105, 102, 103, 104, 101]] non-leaf node\n",
      "non_leaf_node count at previous depth 2\n",
      "depth 2 : 4\n",
      "[[2], [12, 15, 13, 11, 14], [104, 105]] non-leaf node\n",
      "[[2], [12, 15, 13, 11, 14], [101, 102, 103]] non-leaf node\n",
      "[[3, 4, 5, 6], [15], [105, 102, 103, 104, 101]] non-leaf node\n",
      "[[3, 4, 5, 6], [11, 12, 13, 14], [105, 102, 103, 104, 101]] non-leaf node\n",
      "non_leaf_node count at previous depth 4\n",
      "depth 3 : 8\n",
      "[[2], [11, 13, 14], [104, 105]] non-leaf node\n",
      "[[2], [12, 15], [104, 105]] non-leaf node\n",
      "[[2], [12, 15, 13, 11, 14], [102, 103]] non-leaf node\n",
      "[[2], [12, 15, 13, 11, 14], [101]] non-leaf node\n",
      "[[6], [15], [105, 102, 103, 104, 101]] non-leaf node\n",
      "[[3, 4, 5], [15], [105, 102, 103, 104, 101]] non-leaf node\n",
      "[[3, 4, 5, 6], [11, 12, 13, 14], [105]] non-leaf node\n",
      "[[3, 4, 5, 6], [11, 12, 13, 14], [104, 101, 102, 103]] non-leaf node\n",
      "non_leaf_node count at previous depth 8\n",
      "depth 4 : 16\n",
      "[[2], [11, 13, 14], [104]] non-leaf node\n",
      "[[2], [11, 13, 14], [105]] non-leaf node\n",
      "[[2], [12, 15], [104]] non-leaf node\n",
      "[[2], [12, 15], [105]] non-leaf node\n",
      "[[2], [13, 14, 15], [102, 103]] non-leaf node\n",
      "[[2], [11, 12], [102, 103]] non-leaf node\n",
      "[[2], [11, 13], [101]] non-leaf node\n",
      "[[2], [12, 14, 15], [101]] non-leaf node\n",
      "[[6], [15], [102]] has the estimate: 75.0241216344 true: 12.0472007918\n",
      "[[6], [15], [104, 105, 101, 103]] non-leaf node\n",
      "[[3, 4, 5], [15], [101, 103, 104, 105]] non-leaf node\n",
      "[[3, 4, 5], [15], [102]] non-leaf node\n",
      "[[3, 4, 5, 6], [11, 13], [105]] non-leaf node\n",
      "[[3, 4, 5, 6], [12, 14], [105]] non-leaf node\n",
      "[[3, 4, 5, 6], [12, 13, 14], [104, 101, 102, 103]] non-leaf node\n",
      "[[3, 4, 5, 6], [11], [104, 101, 102, 103]] non-leaf node\n",
      "non_leaf_node count at previous depth 15\n",
      "depth 5 : 30\n",
      "[[2], [11], [104]] has the estimate: 53.1358615297 true: 41.0353254845\n",
      "[[2], [13, 14], [104]] non-leaf node\n",
      "[[2], [13, 14], [105]] non-leaf node\n",
      "[[2], [11], [105]] has the estimate: 119.912162957 true: 72.0114719371\n",
      "[[2], [15], [104]] has the estimate: 120.089908397 true: 68.9629558988\n",
      "[[2], [12], [104]] has the estimate: 121.2463388 true: 44.0633750192\n",
      "[[2], [15], [105]] has the estimate: 128.179049088 true: 4.00219652747\n",
      "[[2], [12], [105]] has the estimate: 143.644169497 true: 40.0313041475\n",
      "[[2], [13, 14, 15], [102]] non-leaf node\n",
      "[[2], [13, 14, 15], [103]] non-leaf node\n",
      "[[2], [11, 12], [103]] non-leaf node\n",
      "[[2], [11, 12], [102]] non-leaf node\n",
      "[[2], [11], [101]] has the estimate: 136.123806941 true: 85.9944274041\n",
      "[[2], [13], [101]] has the estimate: 175.504081032 true: 83.0641514957\n",
      "[[2], [14], [101]] has the estimate: 175.426516092 true: 33.0401838621\n",
      "[[2], [12, 15], [101]] non-leaf node\n",
      "[[6], [15], [101]] has the estimate: 114.697218127 true: 38.0707082793\n",
      "[[6], [15], [104, 105, 103]] non-leaf node\n",
      "[[4, 5], [15], [101, 103, 104, 105]] non-leaf node\n",
      "[[3], [15], [101, 103, 104, 105]] non-leaf node\n",
      "[[3], [15], [102]] has the estimate: 150.405900943 true: 23.9625677351\n",
      "[[4, 5], [15], [102]] non-leaf node\n",
      "[[3, 5, 6], [11, 13], [105]] non-leaf node\n",
      "[[4], [11, 13], [105]] non-leaf node\n",
      "[[4, 6], [12, 14], [105]] non-leaf node\n",
      "[[3, 5], [12, 14], [105]] non-leaf node\n",
      "[[4, 5, 6], [12, 13, 14], [104, 101, 102, 103]] non-leaf node\n",
      "[[3], [12, 13, 14], [104, 101, 102, 103]] non-leaf node\n",
      "[[3], [11], [104, 101, 102, 103]] non-leaf node\n",
      "[[4, 5, 6], [11], [104, 101, 102, 103]] non-leaf node\n",
      "non_leaf_node count at previous depth 19\n",
      "depth 6 : 38\n",
      "[[2], [13], [104]] has the estimate: 80.0637384696 true: 7.03395506407\n",
      "[[2], [14], [104]] has the estimate: 91.0316761691 true: 3.0027508578\n",
      "[[2], [14], [105]] has the estimate: 90.1831350424 true: 14.9163855783\n",
      "[[2], [13], [105]] has the estimate: 91.8911272528 true: 18.0361054367\n",
      "[[2], [14, 15], [102]] non-leaf node\n",
      "[[2], [13], [102]] has the estimate: 138.522548725 true: 48.9884006393\n",
      "[[2], [13], [103]] has the estimate: 121.765635397 true: 7.04921513468\n",
      "[[2], [14, 15], [103]] non-leaf node\n",
      "[[2], [12], [103]] has the estimate: 128.298631882 true: 77.9958115171\n",
      "[[2], [11], [103]] has the estimate: 197.031940218 true: 66.0087560856\n",
      "[[2], [11], [102]] has the estimate: 156.019783026 true: 91.9335367433\n",
      "[[2], [12], [102]] has the estimate: 227.01856057 true: 65.0402774817\n",
      "[[2], [15], [101]] has the estimate: 208.073919279 true: 74.9369559498\n",
      "[[2], [12], [101]] has the estimate: 220.959882836 true: 79.9788623623\n",
      "[[6], [15], [103, 105]] non-leaf node\n",
      "[[6], [15], [104]] has the estimate: 145.228976354 true: 20.0117960529\n",
      "[[4, 5], [15], [101, 103, 104]] non-leaf node\n",
      "[[4, 5], [15], [105]] non-leaf node\n",
      "[[3], [15], [103]] has the estimate: 108.444744498 true: 22.0028669441\n",
      "[[3], [15], [104, 105, 101]] non-leaf node\n",
      "[[4], [15], [102]] has the estimate: 208.751615469 true: 92.0311198812\n",
      "[[5], [15], [102]] has the estimate: 223.62880373 true: 72.0525420135\n",
      "[[3, 5, 6], [13], [105]] non-leaf node\n",
      "[[3, 5, 6], [11], [105]] non-leaf node\n",
      "[[4], [11], [105]] has the estimate: 157.874245489 true: 58.9558428993\n",
      "[[4], [13], [105]] has the estimate: 160.955670332 true: 46.9871285229\n",
      "[[4, 6], [14], [105]] non-leaf node\n",
      "[[4, 6], [12], [105]] non-leaf node\n",
      "[[3, 5], [12], [105]] non-leaf node\n",
      "[[3, 5], [14], [105]] non-leaf node\n",
      "[[4, 5, 6], [12, 13, 14], [104]] non-leaf node\n",
      "[[4, 5, 6], [12, 13, 14], [101, 102, 103]] non-leaf node\n",
      "[[3], [12, 13, 14], [101, 103]] non-leaf node\n",
      "[[3], [12, 13, 14], [104, 102]] non-leaf node\n",
      "[[3], [11], [102, 103, 104]] non-leaf node\n",
      "[[3], [11], [101]] has the estimate: 191.660227518 true: 95.9642227771\n",
      "[[4, 5, 6], [11], [101, 102, 104]] non-leaf node\n",
      "[[4, 5, 6], [11], [103]] non-leaf node\n",
      "non_leaf_node count at previous depth 19\n",
      "depth 7 : 38\n",
      "[[2], [14], [102]] has the estimate: 79.7375501987 true: 26.9738916025\n",
      "[[2], [15], [102]] has the estimate: 105.570071378 true: 69.0159606318\n",
      "[[2], [14], [103]] has the estimate: 144.491958569 true: 93.9537311672\n",
      "[[2], [15], [103]] has the estimate: 152.406621565 true: 50.096554595\n",
      "[[6], [15], [105]] has the estimate: 135.259938179 true: 55.9782638144\n",
      "[[6], [15], [103]] has the estimate: 138.819559957 true: 21.0143744806\n",
      "[[4], [15], [101, 103, 104]] non-leaf node\n",
      "[[5], [15], [101, 103, 104]] non-leaf node\n",
      "[[5], [15], [105]] has the estimate: 117.681510036 true: 37.9844489418\n",
      "[[4], [15], [105]] has the estimate: 158.4908344 true: 46.0838238983\n",
      "[[3], [15], [101]] has the estimate: 157.986181801 true: 24.9945160202\n",
      "[[3], [15], [104, 105]] non-leaf node\n",
      "[[5], [13], [105]] has the estimate: 119.840517147 true: 70.9897816187\n",
      "[[3, 6], [13], [105]] non-leaf node\n",
      "[[3, 6], [11], [105]] non-leaf node\n",
      "[[5], [11], [105]] has the estimate: 137.741415434 true: 23.0617723397\n",
      "[[6], [14], [105]] has the estimate: 75.8356596868 true: 0.932495420567\n",
      "[[4], [14], [105]] has the estimate: 162.32872118 true: 50.9585987436\n",
      "[[4], [12], [105]] has the estimate: 134.469274451 true: 37.0283167459\n",
      "[[6], [12], [105]] has the estimate: 192.091857825 true: 59.016648045\n",
      "[[3], [12], [105]] has the estimate: 97.4121182934 true: 36.9572677153\n",
      "[[5], [12], [105]] has the estimate: 200.207513576 true: 87.9905451245\n",
      "[[5], [14], [105]] has the estimate: 126.799447717 true: 18.9605451895\n",
      "[[3], [14], [105]] has the estimate: 253.779964768 true: 95.0063896209\n",
      "[[5], [12, 13, 14], [104]] non-leaf node\n",
      "[[4, 6], [12, 13, 14], [104]] non-leaf node\n",
      "[[4], [12, 13, 14], [101, 102, 103]] non-leaf node\n",
      "[[5, 6], [12, 13, 14], [101, 102, 103]] non-leaf node\n",
      "[[3], [12, 13], [101, 103]] non-leaf node\n",
      "[[3], [14], [101, 103]] non-leaf node\n",
      "[[3], [12], [104, 102]] non-leaf node\n",
      "[[3], [13, 14], [104, 102]] non-leaf node\n",
      "[[3], [11], [103]] has the estimate: 106.094660832 true: 44.0271996417\n",
      "[[3], [11], [104, 102]] non-leaf node\n",
      "[[4, 6], [11], [101, 102, 104]] non-leaf node\n",
      "[[5], [11], [101, 102, 104]] non-leaf node\n",
      "[[4, 5], [11], [103]] non-leaf node\n",
      "[[6], [11], [103]] has the estimate: 227.044288048 true: 78.9669522955\n",
      "non_leaf_node count at previous depth 17\n",
      "depth 8 : 34\n",
      "[[4], [15], [101, 104]] non-leaf node\n",
      "[[4], [15], [103]] has the estimate: 163.630479933 true: 28.0414101739\n",
      "[[5], [15], [103]] has the estimate: 93.4443363234 true: 18.9588365801\n",
      "[[5], [15], [104, 101]] non-leaf node\n",
      "[[3], [15], [104]] has the estimate: 182.377301394 true: 36.0281796575\n",
      "[[3], [15], [105]] has the estimate: 197.374778281 true: 32.9357737994\n",
      "[[3], [13], [105]] has the estimate: 130.141631601 true: 45.9884461035\n",
      "[[6], [13], [105]] has the estimate: 131.260233907 true: 56.0145726411\n",
      "[[3], [11], [105]] has the estimate: 131.809129797 true: 5.94799312481\n",
      "[[6], [11], [105]] has the estimate: 133.201192829 true: 60.0222207182\n",
      "[[5], [12, 14], [104]] non-leaf node\n",
      "[[5], [13], [104]] has the estimate: 205.050727922 true: 85.0166748893\n",
      "[[4, 6], [13], [104]] non-leaf node\n",
      "[[4, 6], [12, 14], [104]] non-leaf node\n",
      "[[4], [12, 13, 14], [102, 103]] non-leaf node\n",
      "[[4], [12, 13, 14], [101]] non-leaf node\n",
      "[[5, 6], [12, 13, 14], [101, 102]] non-leaf node\n",
      "[[5, 6], [12, 13, 14], [103]] non-leaf node\n",
      "[[3], [12, 13], [101]] non-leaf node\n",
      "[[3], [12, 13], [103]] non-leaf node\n",
      "[[3], [14], [101]] has the estimate: 173.190668884 true: 63.9998848045\n",
      "[[3], [14], [103]] has the estimate: 173.733629681 true: 95.9595642024\n",
      "[[3], [12], [104]] has the estimate: 155.674834392 true: 88.01639687\n",
      "[[3], [12], [102]] has the estimate: 202.438829083 true: 87.0175606518\n",
      "[[3], [13, 14], [102]] non-leaf node\n",
      "[[3], [13, 14], [104]] non-leaf node\n",
      "[[3], [11], [102]] has the estimate: 120.717148096 true: 9.00938849647\n",
      "[[3], [11], [104]] has the estimate: 129.379263005 true: 43.0189743283\n",
      "[[4, 6], [11], [104]] non-leaf node\n",
      "[[4, 6], [11], [101, 102]] non-leaf node\n",
      "[[5], [11], [101, 102]] non-leaf node\n",
      "[[5], [11], [104]] has the estimate: 258.186220515 true: 97.0549977642\n",
      "[[4], [11], [103]] has the estimate: 214.123183383 true: 23.9830225896\n",
      "[[5], [11], [103]] has the estimate: 214.986797173 true: 65.9755831538\n",
      "non_leaf_node count at previous depth 16\n",
      "depth 9 : 32\n",
      "[[4], [15], [104]] has the estimate: 97.5862085722 true: 40.0153444063\n",
      "[[4], [15], [101]] has the estimate: 108.821408055 true: 10.0284561992\n",
      "[[5], [15], [101]] has the estimate: 144.100356981 true: 59.0264689632\n",
      "[[5], [15], [104]] has the estimate: 163.065467911 true: 31.0231697459\n",
      "[[5], [14], [104]] has the estimate: 47.9554238491 true: 9.96780065981\n",
      "[[5], [12], [104]] has the estimate: 69.5208428322 true: 18.98030566\n",
      "[[4], [13], [104]] has the estimate: 80.3100627505 true: 41.0516592466\n",
      "[[6], [13], [104]] has the estimate: 93.8632358434 true: 47.0243781273\n",
      "[[4, 6], [14], [104]] non-leaf node\n",
      "[[4, 6], [12], [104]] non-leaf node\n",
      "[[4], [12, 14], [102, 103]] non-leaf node\n",
      "[[4], [13], [102, 103]] non-leaf node\n",
      "[[4], [13, 14], [101]] non-leaf node\n",
      "[[4], [12], [101]] has the estimate: 228.213847645 true: 66.9984492051\n",
      "[[5, 6], [14], [101, 102]] non-leaf node\n",
      "[[5, 6], [12, 13], [101, 102]] non-leaf node\n",
      "[[5, 6], [12, 13], [103]] non-leaf node\n",
      "[[5, 6], [14], [103]] non-leaf node\n",
      "[[3], [13], [101]] has the estimate: 102.619318445 true: 42.9267778604\n",
      "[[3], [12], [101]] has the estimate: 110.73774076 true: 0.995016382114\n",
      "[[3], [13], [103]] has the estimate: 141.668467052 true: 92.0067499012\n",
      "[[3], [12], [103]] has the estimate: 156.525536437 true: 82.9730159638\n",
      "[[3], [13], [102]] has the estimate: 170.005961621 true: 43.9771010288\n",
      "[[3], [14], [102]] has the estimate: 237.377164818 true: 98.0827886987\n",
      "[[3], [14], [104]] has the estimate: 253.154789967 true: 64.0198044223\n",
      "[[3], [13], [104]] has the estimate: 281.418189337 true: 94.9959237263\n",
      "[[6], [11], [104]] has the estimate: 116.503408897 true: 42.1019409905\n",
      "[[4], [11], [104]] has the estimate: 174.826043871 true: 73.9282383826\n",
      "[[4], [11], [101, 102]] non-leaf node\n",
      "[[6], [11], [101, 102]] non-leaf node\n",
      "[[5], [11], [101]] has the estimate: 128.136434735 true: 52.9720505533\n",
      "[[5], [11], [102]] has the estimate: 172.107963134 true: 30.9791007428\n",
      "bottom depth 10 : 22\n",
      "[[6], [14], [104]] has the estimate: 164.797832702 true: 60.0257131569\n",
      "[[4], [14], [104]] has the estimate: 177.760609821 true: 51.0202238742\n",
      "[[4], [12], [104]] has the estimate: 179.878357628 true: 83.9568700641\n",
      "[[6], [12], [104]] has the estimate: 202.801101245 true: 55.1041205432\n",
      "[[4], [12, 14], [103]] has the estimate: 106.949545831\n",
      "[[4], [12, 14], [102]] has the estimate: 143.255452216\n",
      "[[4], [13], [102]] has the estimate: 156.362685776 true: 79.0491984266\n",
      "[[4], [13], [103]] has the estimate: 190.044933248 true: 30.0339704142\n",
      "[[4], [13], [101]] has the estimate: 125.533196262 true: 23.995414092\n",
      "[[4], [14], [101]] has the estimate: 145.211765025 true: 67.0040520374\n",
      "[[6], [14], [101, 102]] has the estimate: 112.020864031\n",
      "[[5], [14], [101, 102]] has the estimate: 165.382360797\n",
      "[[5], [12, 13], [101, 102]] has the estimate: 157.228733525\n",
      "[[6], [12, 13], [101, 102]] has the estimate: 194.132006287\n",
      "[[6], [12, 13], [103]] has the estimate: 138.076900504\n",
      "[[5], [12, 13], [103]] has the estimate: 185.659648586\n",
      "[[6], [14], [103]] has the estimate: 210.230845887 true: 42.9899536304\n",
      "[[5], [14], [103]] has the estimate: 236.242382315 true: 98.9040786072\n",
      "[[4], [11], [101]] has the estimate: 167.308579033 true: 65.0169132598\n",
      "[[4], [11], [102]] has the estimate: 176.967360608 true: 90.0372206241\n",
      "[[6], [11], [102]] has the estimate: 169.895955147 true: 38.9968112979\n",
      "[[6], [11], [101]] has the estimate: 194.412400361 true: 94.9873515467\n",
      "inter-node number: 91\n",
      " *******  number of identified configurations: 105\n",
      "105\n",
      "[0, 0, 0, 0, 1, 12, 31, 52, 70, 91, 105]\n",
      "11747.4007048\n"
     ]
    }
   ],
   "source": [
    "# MSE result statistic\n",
    "\n",
    "# mse_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "# mse_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "\n",
    "# for i in range(1, maxdepth):\n",
    "#     print '-------tree with depth: ',i,'-----------'\n",
    "#     bfs_tree(mse_tree_history_esti[i], total_featureVal_set, i)\n",
    "\n",
    "print 'test error:',test_err\n",
    "print 'training error:',train_err\n",
    "print len(mse_tree_history_split)\n",
    "\n",
    "recog_conf = bfs_tree(mse_tree_history_split[ 0 ], total_featureVal_set, maxdepth-1,  mse_tree_history_esti[0], dta_train )\n",
    "\n",
    "print len(recog_conf[0])\n",
    "print recog_conf[1]\n",
    "print recog_conf[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error at depth 1 : 10951.9519154\n",
      "test error at depth 2 : 10874.4913409\n",
      "test error at depth 3 : 10898.5890795\n",
      "test error at depth 4 : 11074.6896286\n",
      "test error at depth 5 : 11027.8917796\n",
      "test error at depth 6 : 11258.8055739\n",
      "test error at depth 7 : 11308.4776686\n",
      "test error at depth 8 : 11397.2901348\n",
      "test error at depth 9 : 11471.4427182\n",
      "test error at depth 10 : 11477.0047595\n",
      "[10951.951915391994, 10874.491340893754, 10898.589079538771, 11074.689628642509, 11027.89177964135, 11258.80557388515, 11308.477668631882, 11397.29013480827, 11471.442718201137, 11477.004759472811]\n"
     ]
    }
   ],
   "source": [
    "# MSE result statistic : test error\n",
    "\n",
    "pre=0\n",
    "tree_esti = mse_tree_history_esti[0]\n",
    "tree_split =  mse_tree_history_split[ 0 ]\n",
    "test_error_depth = []\n",
    "\n",
    "for i in range(1,maxdepth+1):\n",
    "            \n",
    "    end_bound= pre+ (int)( math.pow(2,i)) \n",
    "    test_leaf_nodes = tree_esti[ pre: end_bound  ]\n",
    "    pre= end_bound        \n",
    "    test_tree_split= tree_split[ 0:(int)( math.pow(2,i)-1)  ]\n",
    "#     print len(test_tree_split)\n",
    "#     print len(test_leaf_nodes)\n",
    "#     test_error_tree( i, dta_test  )\n",
    "    \n",
    "    test_error_depth.append( test_error_tree( i, dta_test  ) )\n",
    "\n",
    "print test_error_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1023\n",
      "1\n",
      "2046\n",
      "1\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "#--------------TMSE-------: result backup\n",
    "\n",
    "tmse_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "tmse_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "tmse_tree_history_leaf =  copy.deepcopy( tree_history_leaf)\n",
    "tmse_tree_history_runtime =  copy.deepcopy( tree_history_runtime)\n",
    "\n",
    "\n",
    "print len(tmse_tree_history_split)\n",
    "print len(tmse_tree_history_split[0])\n",
    "print len(tmse_tree_history_esti)\n",
    "print len(tmse_tree_history_esti[0])\n",
    "\n",
    "print len(tmse_tree_history_leaf)\n",
    "print len(tmse_tree_history_leaf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error: []\n",
      "training error: []\n",
      "1\n",
      "non_leaf_node count at previous depth 0\n",
      "depth 0 : 1\n",
      "[[3, 6, 4, 2, 5], [12, 15, 13, 11, 14], [105, 102, 103, 104, 101]] non-leaf node\n",
      "non_leaf_node count at previous depth 1\n",
      "depth 1 : 2\n",
      "[[3, 6, 4, 2, 5], [15], [105, 102, 103, 104, 101]] non-leaf node\n",
      "[[3, 6, 4, 2, 5], [11, 12, 13, 14], [105, 102, 103, 104, 101]] non-leaf node\n",
      "non_leaf_node count at previous depth 2\n",
      "depth 2 : 4\n",
      "[[3, 6], [15], [105, 102, 103, 104, 101]] non-leaf node\n",
      "[[2, 4, 5], [15], [105, 102, 103, 104, 101]] non-leaf node\n",
      "[[3, 6, 4, 2, 5], [11, 12, 13, 14], [104, 105]] non-leaf node\n",
      "[[3, 6, 4, 2, 5], [11, 12, 13, 14], [101, 102, 103]] non-leaf node\n",
      "non_leaf_node count at previous depth 4\n",
      "depth 3 : 8\n",
      "[[3, 6], [15], [101, 102, 103, 104]] non-leaf node\n",
      "[[3, 6], [15], [105]] non-leaf node\n",
      "[[2, 4, 5], [15], [101, 103, 105]] non-leaf node\n",
      "[[2, 4, 5], [15], [104, 102]] non-leaf node\n",
      "[[2], [11, 12, 13, 14], [104, 105]] non-leaf node\n",
      "[[3, 4, 5, 6], [11, 12, 13, 14], [104, 105]] non-leaf node\n",
      "[[4], [11, 12, 13, 14], [101, 102, 103]] non-leaf node\n",
      "[[2, 3, 5, 6], [11, 12, 13, 14], [101, 102, 103]] non-leaf node\n",
      "non_leaf_node count at previous depth 8\n",
      "depth 4 : 16\n",
      "[[3, 6], [15], [102, 103]] non-leaf node\n",
      "[[3, 6], [15], [104, 101]] non-leaf node\n",
      "[[3], [15], [105]] has the estimate: 58.5884300432 true: 32.9357737994\n",
      "[[6], [15], [105]] has the estimate: 55.9604908027 true: 55.9782638144\n",
      "[[2, 4, 5], [15], [103, 105]] non-leaf node\n",
      "[[2, 4, 5], [15], [101]] non-leaf node\n",
      "[[2, 4, 5], [15], [104]] non-leaf node\n",
      "[[2, 4, 5], [15], [102]] non-leaf node\n",
      "[[2], [13, 14], [104, 105]] non-leaf node\n",
      "[[2], [11, 12], [104, 105]] non-leaf node\n",
      "[[3, 4, 5, 6], [11, 14], [104, 105]] non-leaf node\n",
      "[[3, 4, 5, 6], [12, 13], [104, 105]] non-leaf node\n",
      "[[4], [11, 12, 13, 14], [103]] non-leaf node\n",
      "[[4], [11, 12, 13, 14], [101, 102]] non-leaf node\n",
      "[[2, 3, 5, 6], [11, 12, 13, 14], [102]] non-leaf node\n",
      "[[2, 3, 5, 6], [11, 12, 13, 14], [101, 103]] non-leaf node\n",
      "non_leaf_node count at previous depth 14\n",
      "depth 5 : 28\n",
      "[[6], [15], [102, 103]] non-leaf node\n",
      "[[3], [15], [102, 103]] non-leaf node\n",
      "[[3, 6], [15], [104]] non-leaf node\n",
      "[[3, 6], [15], [101]] non-leaf node\n",
      "[[2, 5], [15], [103, 105]] non-leaf node\n",
      "[[4], [15], [103, 105]] non-leaf node\n",
      "[[4], [15], [101]] has the estimate: 10.0136805514 true: 10.0284561992\n",
      "[[2, 5], [15], [101]] non-leaf node\n",
      "[[4, 5], [15], [104]] non-leaf node\n",
      "[[2], [15], [104]] has the estimate: 68.9408632786 true: 68.9629558988\n",
      "[[2, 5], [15], [102]] non-leaf node\n",
      "[[4], [15], [102]] has the estimate: 92.0229490008 true: 92.0311198812\n",
      "[[2], [13, 14], [104]] non-leaf node\n",
      "[[2], [13, 14], [105]] non-leaf node\n",
      "[[2], [12], [104, 105]] non-leaf node\n",
      "[[2], [11], [104, 105]] non-leaf node\n",
      "[[5, 6], [11, 14], [104, 105]] non-leaf node\n",
      "[[3, 4], [11, 14], [104, 105]] non-leaf node\n",
      "[[4, 6], [12, 13], [104, 105]] non-leaf node\n",
      "[[3, 5], [12, 13], [104, 105]] non-leaf node\n",
      "[[4], [11, 13, 14], [103]] non-leaf node\n",
      "[[4], [12], [103]] has the estimate: 43.9342089192 true: 43.9519402509\n",
      "[[4], [12, 13], [101, 102]] non-leaf node\n",
      "[[4], [11, 14], [101, 102]] non-leaf node\n",
      "[[2, 3, 5, 6], [11, 13], [102]] non-leaf node\n",
      "[[2, 3, 5, 6], [12, 14], [102]] non-leaf node\n",
      "[[2, 3, 5, 6], [12, 13, 14], [101, 103]] non-leaf node\n",
      "[[2, 3, 5, 6], [11], [101, 103]] non-leaf node\n",
      "non_leaf_node count at previous depth 24\n",
      "depth 6 : 48\n",
      "[[6], [15], [102]] has the estimate: 12.0260042472 true: 12.0472007918\n",
      "[[6], [15], [103]] has the estimate: 21.0077126349 true: 21.0143744806\n",
      "[[3], [15], [103]] has the estimate: 21.9833353172 true: 22.0028669441\n",
      "[[3], [15], [102]] has the estimate: 23.9592211837 true: 23.9625677351\n",
      "[[6], [15], [104]] has the estimate: 20.0081537585 true: 20.0117960529\n",
      "[[3], [15], [104]] has the estimate: 36.0281796575 true: 36.0281796575\n",
      "[[3], [15], [101]] has the estimate: 24.9902500862 true: 24.9945160202\n",
      "[[6], [15], [101]] has the estimate: 38.0508770997 true: 38.0707082793\n",
      "[[2, 5], [15], [105]] non-leaf node\n",
      "[[2, 5], [15], [103]] non-leaf node\n",
      "[[4], [15], [103]] has the estimate: 28.0381485828 true: 28.0414101739\n",
      "[[4], [15], [105]] has the estimate: 46.0765802906 true: 46.0838238983\n",
      "[[5], [15], [101]] has the estimate: 59.0100654447 true: 59.0264689632\n",
      "[[2], [15], [101]] has the estimate: 74.9332547982 true: 74.9369559498\n",
      "[[5], [15], [104]] has the estimate: 31.0189879012 true: 31.0231697459\n",
      "[[4], [15], [104]] has the estimate: 39.9922375345 true: 40.0153444063\n",
      "[[2], [15], [102]] has the estimate: 68.9861048228 true: 69.0159606318\n",
      "[[5], [15], [102]] has the estimate: 84.8611010455 true: 72.0525420135\n",
      "[[2], [14], [104]] has the estimate: 2.98966793933 true: 3.0027508578\n",
      "[[2], [13], [104]] has the estimate: 7.0112846817 true: 7.03395506407\n",
      "[[2], [14], [105]] has the estimate: 14.8977174485 true: 14.9163855783\n",
      "[[2], [13], [105]] has the estimate: 18.0143122807 true: 18.0361054367\n",
      "[[2], [12], [105]] has the estimate: 40.0194335435 true: 40.0313041475\n",
      "[[2], [12], [104]] has the estimate: 44.0458150268 true: 44.0633750192\n",
      "[[2], [11], [104]] has the estimate: 41.0003599181 true: 41.0353254845\n",
      "[[2], [11], [105]] has the estimate: 71.9868679547 true: 72.0114719371\n",
      "[[5, 6], [14], [104, 105]] non-leaf node\n",
      "[[5, 6], [11], [104, 105]] non-leaf node\n",
      "[[3, 4], [11], [104, 105]] non-leaf node\n",
      "[[3, 4], [14], [104, 105]] non-leaf node\n",
      "[[4, 6], [13], [104, 105]] non-leaf node\n",
      "[[4, 6], [12], [104, 105]] non-leaf node\n",
      "[[3, 5], [12], [104, 105]] non-leaf node\n",
      "[[3, 5], [13], [104, 105]] non-leaf node\n",
      "[[4], [11, 14], [103]] non-leaf node\n",
      "[[4], [13], [103]] has the estimate: 42.5258585974 true: 30.0339704142\n",
      "[[4], [12, 13], [101]] non-leaf node\n",
      "[[4], [12, 13], [102]] non-leaf node\n",
      "[[4], [11, 14], [101]] non-leaf node\n",
      "[[4], [11, 14], [102]] non-leaf node\n",
      "[[3, 5], [11, 13], [102]] non-leaf node\n",
      "[[2, 6], [11, 13], [102]] non-leaf node\n",
      "[[2, 5, 6], [12, 14], [102]] non-leaf node\n",
      "[[3], [12, 14], [102]] non-leaf node\n",
      "[[2, 3, 6], [12, 13, 14], [101, 103]] non-leaf node\n",
      "[[5], [12, 13, 14], [101, 103]] non-leaf node\n",
      "[[2, 3, 5, 6], [11], [103]] non-leaf node\n",
      "[[2, 3, 5, 6], [11], [101]] non-leaf node\n",
      "non_leaf_node count at previous depth 23\n",
      "depth 7 : 46\n",
      "[[2], [15], [105]] has the estimate: 3.99470822726 true: 4.00219652747\n",
      "[[5], [15], [105]] has the estimate: 37.9640896966 true: 37.9844489418\n",
      "[[5], [15], [103]] has the estimate: 18.9410245791 true: 18.9588365801\n",
      "[[2], [15], [103]] has the estimate: 50.0857598212 true: 50.096554595\n",
      "[[5, 6], [14], [105]] non-leaf node\n",
      "[[5, 6], [14], [104]] non-leaf node\n",
      "[[5, 6], [11], [105]] non-leaf node\n",
      "[[5, 6], [11], [104]] non-leaf node\n",
      "[[3], [11], [104, 105]] non-leaf node\n",
      "[[4], [11], [104, 105]] non-leaf node\n",
      "[[4], [14], [104, 105]] non-leaf node\n",
      "[[3], [14], [104, 105]] non-leaf node\n",
      "[[4, 6], [13], [104]] non-leaf node\n",
      "[[4, 6], [13], [105]] non-leaf node\n",
      "[[4, 6], [12], [105]] non-leaf node\n",
      "[[4, 6], [12], [104]] non-leaf node\n",
      "[[5], [12], [104, 105]] non-leaf node\n",
      "[[3], [12], [104, 105]] non-leaf node\n",
      "[[3, 5], [13], [105]] non-leaf node\n",
      "[[3, 5], [13], [104]] non-leaf node\n",
      "[[4], [14], [103]] has the estimate: 18.9684261873 true: 18.9846489999\n",
      "[[4], [11], [103]] has the estimate: 75.3995854581 true: 23.9830225896\n",
      "[[4], [13], [101]] has the estimate: 23.9841542193 true: 23.995414092\n",
      "[[4], [12], [101]] has the estimate: 79.5863746766 true: 66.9984492051\n",
      "[[4], [12], [102]] has the estimate: 31.0151623789 true: 31.0322361829\n",
      "[[4], [13], [102]] has the estimate: 79.0311766523 true: 79.0491984266\n",
      "[[4], [11], [101]] has the estimate: 65.0062597391 true: 65.0169132598\n",
      "[[4], [14], [101]] has the estimate: 66.9861032743 true: 67.0040520374\n",
      "[[4], [14], [102]] has the estimate: 74.9532099709 true: 74.9689172281\n",
      "[[4], [11], [102]] has the estimate: 90.0229829062 true: 90.0372206241\n",
      "[[3, 5], [11], [102]] non-leaf node\n",
      "[[3, 5], [13], [102]] non-leaf node\n",
      "[[6], [11, 13], [102]] non-leaf node\n",
      "[[2], [11, 13], [102]] non-leaf node\n",
      "[[2], [12, 14], [102]] non-leaf node\n",
      "[[5, 6], [12, 14], [102]] non-leaf node\n",
      "[[3], [12], [102]] has the estimate: 87.0100934656 true: 87.0175606518\n",
      "[[3], [14], [102]] has the estimate: 98.0827886987 true: 98.0827886987\n",
      "[[2, 3, 6], [13], [101, 103]] non-leaf node\n",
      "[[2, 3, 6], [12, 14], [101, 103]] non-leaf node\n",
      "[[5], [12, 14], [101, 103]] non-leaf node\n",
      "[[5], [13], [101, 103]] non-leaf node\n",
      "[[3], [11], [103]] has the estimate: 44.0031980136 true: 44.0271996417\n",
      "[[2, 5, 6], [11], [103]] non-leaf node\n",
      "[[5], [11], [101]] has the estimate: 52.953470112 true: 52.9720505533\n",
      "[[2, 3, 6], [11], [101]] non-leaf node\n",
      "non_leaf_node count at previous depth 28\n",
      "depth 8 : 56\n",
      "[[6], [14], [105]] has the estimate: 0.913579256071 true: 0.932495420567\n",
      "[[5], [14], [105]] has the estimate: 18.9504832622 true: 18.9605451895\n",
      "[[5], [14], [104]] has the estimate: 9.93623420584 true: 9.96780065981\n",
      "[[6], [14], [104]] has the estimate: 60.0148581404 true: 60.0257131569\n",
      "[[5], [11], [105]] has the estimate: 23.0551501336 true: 23.0617723397\n",
      "[[6], [11], [105]] has the estimate: 59.9999969484 true: 60.0222207182\n",
      "[[6], [11], [104]] has the estimate: 42.0813608606 true: 42.1019409905\n",
      "[[5], [11], [104]] has the estimate: 122.186877798 true: 97.0549977642\n",
      "[[3], [11], [105]] has the estimate: 5.94346910329 true: 5.94799312481\n",
      "[[3], [11], [104]] has the estimate: 43.0006977566 true: 43.0189743283\n",
      "[[4], [11], [105]] has the estimate: 58.9406971113 true: 58.9558428993\n",
      "[[4], [11], [104]] has the estimate: 73.9146241318 true: 73.9282383826\n",
      "[[4], [14], [105]] has the estimate: 50.9463355462 true: 50.9585987436\n",
      "[[4], [14], [104]] has the estimate: 51.0164004996 true: 51.0202238742\n",
      "[[3], [14], [104]] has the estimate: 115.162784964 true: 64.0198044223\n",
      "[[3], [14], [105]] has the estimate: 107.400798748 true: 95.0063896209\n",
      "[[4], [13], [104]] has the estimate: 41.0267990263 true: 41.0516592466\n",
      "[[6], [13], [104]] has the estimate: 47.0007345796 true: 47.0243781273\n",
      "[[4], [13], [105]] has the estimate: 46.9793695238 true: 46.9871285229\n",
      "[[6], [13], [105]] has the estimate: 55.9977693337 true: 56.0145726411\n",
      "[[4], [12], [105]] has the estimate: 37.0136047142 true: 37.0283167459\n",
      "[[6], [12], [105]] has the estimate: 59.0116676165 true: 59.016648045\n",
      "[[6], [12], [104]] has the estimate: 55.1041205432 true: 55.1041205432\n",
      "[[4], [12], [104]] has the estimate: 83.9441494312 true: 83.9568700641\n",
      "[[5], [12], [104]] has the estimate: 18.9548937887 true: 18.98030566\n",
      "[[5], [12], [105]] has the estimate: 87.9809077516 true: 87.9905451245\n",
      "[[3], [12], [105]] has the estimate: 36.9346140007 true: 36.9572677153\n",
      "[[3], [12], [104]] has the estimate: 87.993304923 true: 88.01639687\n",
      "[[3], [13], [105]] has the estimate: 45.9693970002 true: 45.9884461035\n",
      "[[5], [13], [105]] has the estimate: 70.9632507811 true: 70.9897816187\n",
      "[[3], [13], [104]] has the estimate: 145.400100024 true: 94.9959237263\n",
      "[[5], [13], [104]] has the estimate: 85.0098029004 true: 85.0166748893\n",
      "[[3], [11], [102]] has the estimate: 8.99886003241 true: 9.00938849647\n",
      "[[5], [11], [102]] has the estimate: 30.9791007428 true: 30.9791007428\n",
      "[[5], [13], [102]] has the estimate: 14.9587456538 true: 14.9587456538\n",
      "[[3], [13], [102]] has the estimate: 43.9721672951 true: 43.9771010288\n",
      "[[6], [11], [102]] has the estimate: 38.9928503623 true: 38.9968112979\n",
      "[[6], [13], [102]] has the estimate: 76.0482945644 true: 76.0566138823\n",
      "[[2], [13], [102]] has the estimate: 48.9732941825 true: 48.9884006393\n",
      "[[2], [11], [102]] has the estimate: 91.9139845409 true: 91.9335367433\n",
      "[[2], [14], [102]] has the estimate: 26.9488890149 true: 26.9738916025\n",
      "[[2], [12], [102]] has the estimate: 78.7402638472 true: 65.0402774817\n",
      "[[6], [12, 14], [102]] non-leaf node\n",
      "[[5], [12, 14], [102]] non-leaf node\n",
      "[[2, 3, 6], [13], [103]] non-leaf node\n",
      "[[2, 3, 6], [13], [101]] non-leaf node\n",
      "[[2, 3, 6], [12, 14], [101]] non-leaf node\n",
      "[[2, 3, 6], [12, 14], [103]] non-leaf node\n",
      "[[5], [14], [101, 103]] non-leaf node\n",
      "[[5], [12], [101, 103]] non-leaf node\n",
      "[[5], [13], [101]] has the estimate: 78.9707034356 true: 78.9781796176\n",
      "[[5], [13], [103]] has the estimate: 86.98031436 true: 86.9948139793\n",
      "[[2, 5], [11], [103]] non-leaf node\n",
      "[[6], [11], [103]] has the estimate: 78.9669522955 true: 78.9669522955\n",
      "[[2], [11], [101]] has the estimate: 85.9697178104 true: 85.9944274041\n",
      "[[3, 6], [11], [101]] non-leaf node\n",
      "non_leaf_node count at previous depth 10\n",
      "depth 9 : 20\n",
      "[[6], [14], [102]] has the estimate: 43.9768551756 true: 44.0005878945\n",
      "[[6], [12], [102]] has the estimate: 70.0029483546 true: 70.0071138717\n",
      "[[5], [12], [102]] has the estimate: 51.9132061445 true: 51.9356296313\n",
      "[[5], [14], [102]] has the estimate: 85.9725318402 true: 85.9819077959\n",
      "[[2, 6], [13], [103]] non-leaf node\n",
      "[[3], [13], [103]] has the estimate: 91.9833883895 true: 92.0067499012\n",
      "[[3], [13], [101]] has the estimate: 42.9039437799 true: 42.9267778604\n",
      "[[2, 6], [13], [101]] non-leaf node\n",
      "[[2, 3], [12, 14], [101]] non-leaf node\n",
      "[[6], [12, 14], [101]] non-leaf node\n",
      "[[6], [12, 14], [103]] non-leaf node\n",
      "[[2, 3], [12, 14], [103]] non-leaf node\n",
      "[[5], [14], [101]] has the estimate: 33.0129976187 true: 33.0241456091\n",
      "[[5], [14], [103]] has the estimate: 98.9040786072 true: 98.9040786072\n",
      "[[5], [12], [103]] has the estimate: 59.63705204 true: 46.9440763592\n",
      "[[5], [12], [101]] has the estimate: 98.0562664642 true: 98.0778430528\n",
      "[[5], [11], [103]] has the estimate: 65.9755831538 true: 65.9755831538\n",
      "[[2], [11], [103]] has the estimate: 66.0049621959 true: 66.0087560856\n",
      "[[6], [11], [101]] has the estimate: 94.977431944 true: 94.9873515467\n",
      "[[3], [11], [101]] has the estimate: 95.9510443703 true: 95.9642227771\n",
      "bottom depth 10 : 12\n",
      "[[2], [13], [103]] has the estimate: 7.04921513468 true: 7.04921513468\n",
      "[[6], [13], [103]] has the estimate: 24.9354739815 true: 24.9568590066\n",
      "[[6], [13], [101]] has the estimate: 69.0242027564 true: 69.0242027564\n",
      "[[2], [13], [101]] has the estimate: 83.0379854077 true: 83.0641514957\n",
      "[[3], [12, 14], [101]] has the estimate: 32.4780166425\n",
      "[[2], [12, 14], [101]] has the estimate: 55.4317639955\n",
      "[[6], [14], [101]] has the estimate: 66.0243315817 true: 66.0243315817\n",
      "[[6], [12], [101]] has the estimate: 99.0154661342 true: 99.0429218156\n",
      "[[6], [14], [103]] has the estimate: 42.9861288387 true: 42.9899536304\n",
      "[[6], [12], [103]] has the estimate: 42.995343071 true: 42.9990841506\n",
      "[[2, 3], [12], [103]] has the estimate: 80.512622078\n",
      "[[2, 3], [14], [103]] has the estimate: 94.8846233929\n",
      "inter-node number: 109\n",
      " *******  number of identified configurations: 117\n",
      "117\n",
      "[0, 0, 0, 0, 2, 6, 31, 49, 95, 109, 117]\n",
      "86.0729165188\n"
     ]
    }
   ],
   "source": [
    "# TMSE result statistic\n",
    "\n",
    "# tmse_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "# tmse_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "\n",
    "# for i in range(1, maxdepth):\n",
    "#     print '-------tree with depth: ',i,'-----------'\n",
    "#     bfs_tree(mse_tree_history_esti[i], total_featureVal_set, i)\n",
    "\n",
    "print 'test error:',test_err\n",
    "print 'training error:',train_err\n",
    "print len(tmse_tree_history_split)   \n",
    "\n",
    "recog_conf=bfs_tree(tmse_tree_history_split[ 0 ],total_featureVal_set,maxdepth-1,tmse_tree_history_esti[0],dta_train ) \n",
    "\n",
    "print len(recog_conf[0])\n",
    "print recog_conf[1]\n",
    "print recog_conf[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error at depth 1 : 722.061178985\n",
      "test error at depth 2 : 677.890121689\n",
      "test error at depth 3 : 604.811011388\n",
      "test error at depth 4 : 526.53048095\n",
      "test error at depth 5 : 463.571501548\n",
      "test error at depth 6 : 348.338174181\n",
      "test error at depth 7 : 228.428619434\n",
      "test error at depth 8 : 107.664019556\n",
      "test error at depth 9 : 10.1351299399\n",
      "test error at depth 10 : 0.994190527863\n",
      "[722.0611789853202, 677.8901216887728, 604.8110113875669, 526.5304809500908, 463.57150154818726, 348.33817418080304, 228.42861943390577, 107.66401955584888, 10.135129939896686, 0.9941905278626019]\n"
     ]
    }
   ],
   "source": [
    "# TMSE result statistic : test error\n",
    "\n",
    "pre=0\n",
    "tree_esti = tmse_tree_history_esti[0]\n",
    "tree_split =  tmse_tree_history_split[ 0 ]\n",
    "\n",
    "test_error_depth=[]\n",
    "\n",
    "for i in range(1,maxdepth+1):\n",
    "            \n",
    "    end_bound= pre+ (int)( math.pow(2,i)) \n",
    "    test_leaf_nodes = tree_esti[ pre: end_bound  ]\n",
    "    pre= end_bound        \n",
    "    test_tree_split= tree_split[ 0:(int)( math.pow(2,i)-1)  ]\n",
    "    \n",
    "#     print len(test_tree_split)\n",
    "#     print len(test_leaf_nodes)\n",
    "\n",
    "    test_error_depth.append( test_error_tree( i, dta_test  ) )\n",
    "\n",
    "print test_error_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data inspection for the nodes of the trained tree\n",
    "\n",
    "def feature_value_extract( valueSet):\n",
    "    \n",
    "    tmpval=1\n",
    "    tmp_valSet= valueSet\n",
    "    valSet_list=[]\n",
    "    cnt=0\n",
    "    \n",
    "    while tmpval <= tmp_valSet:\n",
    "        if tmpval & tmp_valSet !=0:\n",
    "            valSet_list.append(cnt)\n",
    "        tmpval=tmpval<<1\n",
    "        cnt=cnt+1\n",
    "        \n",
    "    return valSet_list\n",
    "\n",
    "def dfs_tree(tree, current_nodeIdx, current_featureVal_set, current_depth):\n",
    "    \n",
    "    nodeNum=len(tree)   \n",
    "    if nodeNum == 0:\n",
    "        return\n",
    "    \n",
    "    split_feature= tree[current_nodeIdx][0]\n",
    "    split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "    if split_feature==-1:\n",
    "        print current_featureVal_set\n",
    "        return;\n",
    "    \n",
    "    featureValue_list = feature_value_extract( split_valueSet)\n",
    "    \n",
    "    left_featureValue_set=[]\n",
    "    right_featureValue_set=[]\n",
    "    featureNum= len(current_featureVal_set)\n",
    "    \n",
    "    for i in range(0, featureNum):\n",
    "        if i != split_feature:\n",
    "            left_featureValue_set.append(  copy.deepcopy(current_featureVal_set[i])   )\n",
    "            right_featureValue_set.append(  copy.deepcopy(current_featureVal_set[i])   )\n",
    "        else:\n",
    "            left_featureValue_set.append(  copy.deepcopy(featureValue_list )   )\n",
    "            right_featureValue_set.append(copy.deepcopy(list(set(current_featureVal_set[i] )-set(featureValue_list)))   )\n",
    "            \n",
    "        \n",
    "    traverse_tree(tree, current_nodeIdx*2+1,left_featureValue_set,current_depth+1)\n",
    "    traverse_tree(tree, current_nodeIdx*2+2,right_featureValue_set, current_depth+1)\n",
    "    \n",
    "    return \n",
    "\n",
    "def training_data_summary( train_rdd ):\n",
    "\n",
    "#     always remove outliers before\n",
    "    train_rdd_clean= train_rdd.filter(lambda line: line[0]<1000)\n",
    "    distinct_featureVals = train_rdd_clean.map(lambda line: (line[1], line[2],line[3]) ).distinct().collect()\n",
    "   \n",
    "    featureConfig_val={}\n",
    "    for i in distinct_featureVals:\n",
    "        \n",
    "        tmprdd=train_rdd_clean.filter(lambda line:(line[1], line[2],line[3]) ==i ).map(lambda line: line[0])\n",
    "        tmpval=tmprdd.reduce(lambda a,b:a+b)\n",
    "        tmpval= tmpval/tmprdd.count()*1.0\n",
    "        \n",
    "        featureConfig_val.update( {i:tmpval} )\n",
    "    \n",
    "#     print featureConfig_val\n",
    "    return featureConfig_val\n",
    "        \n",
    "\n",
    "def config_check( featureVal_set, feature_num):\n",
    "    \n",
    "    for i in range(0, feature_num):\n",
    "        if len(featureVal_set[i]) >1 :\n",
    "            return -1\n",
    "    return 1\n",
    "\n",
    "def lookup_featureConfig_val( configToVal_dict, value_set, feature_num):\n",
    "    \n",
    "    tmptuple=()\n",
    "    for i in range(0, feature_num):\n",
    "        tmptuple = tmptuple + ( value_set[i][0]  ,)\n",
    "#     print '++++++++++++', tmptuple\n",
    "    return configToVal_dict[tmptuple]\n",
    "    \n",
    "def test_error_mapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], test_tree_split)\n",
    "    return (test_leaf_nodes[ tmpnode ]  - line[0])*(test_leaf_nodes[ tmpnode ]  - line[0])\n",
    "    \n",
    "def test_error_tree( current_depth, testRDD  ):\n",
    "    \n",
    "    test_cnt= testRDD.count()\n",
    "    err_rdd=testRDD.map( lambda line: test_error_mapFunc(line))\n",
    "    err_sum = err_rdd.reduce( lambda a,b: a+b)\n",
    "    \n",
    "    print 'test error at depth',current_depth, ':', err_sum*1.0/test_cnt\n",
    "    return err_sum*1.0/test_cnt\n",
    "    \n",
    "def bfs_tree(tree, total_featureVal_set, depth, tree_esti, trainRDD ):\n",
    "    \n",
    "    nodeNum=len(tree)   \n",
    "    if nodeNum == 0:\n",
    "        return\n",
    "    \n",
    "    feature_ConfigToVal = training_data_summary( trainRDD )\n",
    "    \n",
    "    interNode_cnt=0\n",
    "    non_leaf_cnt=0\n",
    "    recog_config_cnt=0\n",
    "    \n",
    "    qu=[]\n",
    "    current_depth=0\n",
    "    qu.append((0,total_featureVal_set,0))\n",
    "    \n",
    "    \n",
    "    recog_config_cnt_depth=[]\n",
    "    recog_conf=[]\n",
    "    \n",
    "    while current_depth<= depth:\n",
    "        \n",
    "        print 'non_leaf_node count at previous depth', non_leaf_cnt\n",
    "        \n",
    "        print 'depth', current_depth,':', len(qu)\n",
    "        tmpqu=[]\n",
    "        \n",
    "        non_leaf_cnt=0\n",
    "        \n",
    "        for i in qu:\n",
    "            current_nodeIdx= i[0] \n",
    "            current_featureVal_set= i[1]\n",
    "            \n",
    "            split_feature= tree[current_nodeIdx][0]\n",
    "            split_valueSet= tree[current_nodeIdx][1]\n",
    "                 \n",
    "            if split_feature==-1: \n",
    "                \n",
    "                if config_check( current_featureVal_set, featureNum) ==1:\n",
    "                    recog_config_cnt= recog_config_cnt+1\n",
    "                    tmpval=lookup_featureConfig_val( feature_ConfigToVal, current_featureVal_set, featureNum)\n",
    "                    \n",
    "                    print current_featureVal_set, 'has the estimate:', tree_esti[ current_nodeIdx*2+1],\\\n",
    "                    'true:',tmpval\n",
    "                    #result\n",
    "                    recog_conf.append( (tree_esti[ current_nodeIdx*2+1], tmpval) )\n",
    "                    \n",
    "                else:\n",
    "                    print current_featureVal_set, 'has the estimate:', tree_esti[ current_nodeIdx*2+1],'!!! not seperated !!!'\n",
    "                \n",
    "                interNode_cnt=interNode_cnt+1\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                non_leaf_cnt=non_leaf_cnt+1\n",
    "                print current_featureVal_set,'non-leaf node'\n",
    "                \n",
    "                \n",
    "            featureValue_list = feature_value_extract( split_valueSet)\n",
    "    \n",
    "            left_featureValue_set=[]\n",
    "            right_featureValue_set=[]\n",
    "            featureNum= len(current_featureVal_set)\n",
    "    \n",
    "            for i in range(0, featureNum):\n",
    "                if i != split_feature:\n",
    "                    left_featureValue_set.append(  copy.deepcopy(current_featureVal_set[i])   )\n",
    "                    right_featureValue_set.append(  copy.deepcopy(current_featureVal_set[i])   )\n",
    "                else:\n",
    "                    left_featureValue_set.append(  copy.deepcopy(featureValue_list )   )\n",
    "                    right_featureValue_set.append(copy.deepcopy(list(set(current_featureVal_set[i] )-set(featureValue_list)))   )\n",
    "            \n",
    "            tmpqu.append(  (current_nodeIdx*2+1,left_featureValue_set, current_nodeIdx)  )\n",
    "            tmpqu.append(  (current_nodeIdx*2+2,right_featureValue_set,current_nodeIdx) )\n",
    "        \n",
    "        qu=copy.deepcopy(tmpqu)\n",
    "        current_depth= current_depth+1\n",
    "        \n",
    "        #result\n",
    "        recog_config_cnt_depth.append( recog_config_cnt )\n",
    "    \n",
    "    \n",
    "# print out leaf nodes\n",
    "    print 'bottom depth', current_depth,':', len(qu)\n",
    "    \n",
    "    for i in qu:\n",
    "        current_nodeIdx= i[0] \n",
    "        current_featureVal_set= i[1]\n",
    "        parent_nodeIdx= i[2]\n",
    "        \n",
    "        if 2*parent_nodeIdx+1 == current_nodeIdx: \n",
    "#         parent_nodeIdx == (current_nodeIdx*2+1):\n",
    "            tree_esti_val= tree_esti[ parent_nodeIdx*2]\n",
    "        else:\n",
    "            tree_esti_val= tree_esti[ parent_nodeIdx*2+1] \n",
    "        \n",
    "        if config_check( current_featureVal_set, featureNum) ==1:\n",
    "            recog_config_cnt= recog_config_cnt+1\n",
    "            tmpval=lookup_featureConfig_val( feature_ConfigToVal, current_featureVal_set, featureNum)\n",
    "            \n",
    "            #result\n",
    "            recog_conf.append( (tree_esti_val, tmpval) )\n",
    "            \n",
    "            print current_featureVal_set, 'has the estimate:', tree_esti_val,\\\n",
    "                    'true:',tmpval\n",
    "        else:\n",
    "            print current_featureVal_set, 'has the estimate:', tree_esti_val\n",
    "    \n",
    "    #result\n",
    "    recog_config_cnt_depth.append( recog_config_cnt )\n",
    "        \n",
    "    print 'inter-node number:', interNode_cnt\n",
    "    print ' *******  number of identified configurations:', recog_config_cnt\n",
    "    \n",
    "    tmpval=0\n",
    "    for i in range(0,recog_config_cnt):\n",
    "        tmpval=tmpval+ (recog_conf[i][0]-recog_conf[i][1])*(recog_conf[i][0]-recog_conf[i][1])\n",
    "    \n",
    "    return (recog_conf, recog_config_cnt_depth, tmpval*1.0/recog_config_cnt)\n",
    "\n",
    "# len( tree_history_split)\n",
    "# len(tree_history_esti)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#do not run this part\n",
    "# training and testing process\n",
    "\n",
    "# without outliers\n",
    "tmpdta = dta.filter(lambda line: line[0]<1000).takeSample(False, 100000, 12243)\n",
    "dta_train = sc.parallelize(tmpdta).cache().sample(False, .7, 12345)\n",
    "dta_test = sc.parallelize(tmpdta).cache().sample(False, .3, 43243)\n",
    "\n",
    "maxdepth=10\n",
    "numFeatures=3\n",
    "\n",
    "node_split=[]\n",
    "node_test=[]\n",
    "run_time=[]\n",
    "\n",
    "#------------ estimate phase ---------------------\n",
    "# def search_nodeToData(features, tree):\n",
    "#     nodeNum=len(tree)   \n",
    "#     if nodeNum == 0:\n",
    "#         return 0;\n",
    "    \n",
    "#     current_nodeIdx=0\n",
    "#     while current_nodeIdx< nodeNum:\n",
    "#         split_feature= tree[current_nodeIdx][0]\n",
    "#         split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "#         if split_feature==-1:\n",
    "#             return -1\n",
    "#         if (( 1<<features[split_feature]) &  split_valueSet) != 0 :\n",
    "#             current_nodeIdx=current_nodeIdx*2+1\n",
    "#         else:\n",
    "#             current_nodeIdx=current_nodeIdx*2+2        \n",
    "#     return current_nodeIdx - nodeNum\n",
    "\n",
    "def tree_test_mapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], node_split)\n",
    "    return (leaf_nodes[ tmpnode ]  - line[0])*(leaf_nodes[ tmpnode ]  - line[0])\n",
    "    \n",
    "def tree_test( testData_rdd ):\n",
    "    err_rdd=testData_rdd.map( lambda line: tree_test_mapFunc(line))\n",
    "    err_sum = err_rdd.reduce( lambda a,b: a+b)\n",
    "    return err_sum / testData_rdd.count()\n",
    "#------------------------------------------------\n",
    "    \n",
    "#ini parameter on the cluster\n",
    "currentNode_split_fromMaster = sc.broadcast([])\n",
    "test_err = []\n",
    "train_err = []\n",
    "#prepare feature value set\n",
    "feature_valueSet = data_featureValues_collect( dta_train )\n",
    "feature_valueList=[]\n",
    "for i in range(0, len(feature_valueSet)):\n",
    "    feature_valueList.append( ( i, list(feature_valueSet[i][1]) ) )\n",
    "    \n",
    "    \n",
    "for i in range(6,maxdepth):\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    del node_split[:]\n",
    "    del node_test[:]\n",
    "    currentNode_split_fromMaster = sc.broadcast([])\n",
    "    \n",
    "    for cur_depth in range(0,i):\n",
    "        \n",
    "        current_nodeNum= int( math.pow(2,  cur_depth)) \n",
    "        \n",
    "        dataToNode = dataToNode_assignment( dta_train )\n",
    "        find_bestSplit_exact(dataToNode,current_nodeNum, node_split, node_test, feature_valueList)   \n",
    "#         ( dataToNode, current_NumNodes, nodes_tree, nodes_tree_test, feature_valueSet):\n",
    "        #for whole tree\n",
    "        currentNode_split_fromMaster = sc.broadcast(node_split)\n",
    "    \n",
    "    end = time.time()\n",
    "    elapsed = end - start    \n",
    "    leaf_nodes = node_test[ len(node_test)-  (int)( math.pow(2,i)): len(node_test)  ]\n",
    "\n",
    "    \n",
    "    tmp_test_err= tree_test( dta_test )\n",
    "    tmp_train_err= tree_test( dta_train )\n",
    "    \n",
    "    test_err.append( tmp_test_err)\n",
    "    train_err.append( tmp_train_err )\n",
    "    run_time.append(elapsed)\n",
    "    \n",
    "    print \"error at tree height\", i,\":\",  tmp_test_err, tmp_train_err\n",
    "    print \"running time at tree height\", i,\":\",  elapsed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "error at tree height 1 : 836.120673266 993197.408799\n",
    "running time at tree height 1 : 71.6192398071\n",
    "error at tree height 2 : 805.794778995 994017.544055\n",
    "running time at tree height 2 : 138.060220957\n",
    "error at tree height 3 : 765.500220461 994394.511005\n",
    "running time at tree height 3 : 208.832412004\n",
    "    \n",
    "error at tree height 4 : 734.829314704 993707.63729\n",
    "running time at tree height 4 : 281.407808065\n",
    "    \n",
    "error at tree height 5 : 644.236932264 994344.82116\n",
    "running time at tree height 5 : 4879.72735\n",
    "\n",
    "error at tree height 6 : 529.4375226396904 994434.7416043472\n",
    "\n",
    "error at tree height 7 : 770.5749206468843 992877.3588177576\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# singal-run training  process  \n",
    "dta_train = dta.sample(False, .7, 12345)\n",
    "dta_test = dta.sample(False, .3, 43243)\n",
    "dta_test = dta_test.filter(lambda line: line[0] <1000 )\n",
    "\n",
    "#parameters\n",
    "maxdepth=\n",
    "numFeatures=3\n",
    "\n",
    "node_split=[]\n",
    "node_test=[]\n",
    "#ini parameter\n",
    "currentNode_split_fromMaster = sc.broadcast([])\n",
    "\n",
    "#prepare feature value set\n",
    "feature_valueSet = data_featureValues_collect( dta_train )\n",
    "feature_valueList=[]\n",
    "for i in range(0, len(feature_valueSet)):\n",
    "    feature_valueList.append( ( i, list(feature_valueSet[i][1]) ) )\n",
    "    \n",
    "\n",
    "for i in range(0,maxdepth):\n",
    "    \n",
    "    start = time.time()\n",
    "    current_nodeNum= int( math.pow(2,  i))\n",
    "    \n",
    "    print 'current split decision:',currentNode_split_fromMaster.value\n",
    "\n",
    "    dataToNode = dataToNode_assignment( dta_train )\n",
    "    cluster_end = time.time() \n",
    "    find_bestSplit_exact(dataToNode,current_nodeNum,node_split,node_test, feature_valueList)   \n",
    "#   ( dataToNode, current_NumNodes, nodes_tree, nodes_tree_test, feature_valueSet):\n",
    "    currentNode_split_fromMaster = sc.broadcast(node_split)\n",
    "  \n",
    "    end = time.time() \n",
    "\n",
    "    print i,'-th level running time: ', cluster_end - start,'sec', end- cluster_end, 'sec'\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69015"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#select one feature-value\n",
    "feature_id=1\n",
    "feature_val=11\n",
    "\n",
    "dataToFeatureValue_rdd=node_data.filter(lambda line:line[1][feature_id+1]==feature_val).map(lambda line:line[1][0])\n",
    "\n",
    "dataToFeatureValue_rdd_count = dataToFeatureValue_rdd.count()\n",
    "        \n",
    "# med_tmpLarge= dataToFeatureValue_rdd.top( dataToFeatureValue_rdd_count/2+1  )\n",
    "med_tmp = dataToFeatureValue_rdd.top( dataToFeatureValue_rdd_count/2  )\n",
    "med_tmp.sort()\n",
    "tmpcnt= len(med_tmp)\n",
    "print med_tmp[ tmpcnt-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437547\n"
     ]
    }
   ],
   "source": [
    "#select several feature-values\n",
    "feature_values=[1,2,3]\n",
    "\n",
    "dataToFeatureValue_rdd=node_data.filter(lambda line:line[1][feature_id+1] in feature_values).map(lambda line:line[1][0])\n",
    "\n",
    "dataToFeatureValue_rdd_count = dataToFeatureValue_rdd.count()\n",
    "\n",
    "print dataToFeatureValue_rdd_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
