{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os \n",
    "import sys \n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "from datetime  import datetime \n",
    "\n",
    "import numpy as np  # learn \n",
    "import pandas as pd # learn\n",
    "from pandas import *\n",
    "from numpy import *\n",
    "\n",
    "from scipy import stats # look at scipy\n",
    "from scipy import linalg\n",
    "from scipy import *\n",
    "\n",
    " \n",
    "import matplotlib as mplt # learn matplolib \n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "import seaborn as sns \n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc={\"figure.figsize\": (14, 6)})\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import sklearn as sk\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import random\n",
    "import time\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328.0, 10, 10, 2, 2, 8, 1, 3)\n",
      "3315067\n"
     ]
    }
   ],
   "source": [
    "#BT data\n",
    "# dta_RDD = sc.textFile(\"file:///home/tguo/data/tian-test/udpjitter-H1april2014_regTree.csv\")\n",
    "\n",
    "dta_RDD = sc.textFile(\"hdfs://computer61.ant-net/user/tguo/udpjitter-H1april2014_regTree_mllib.csv\")\n",
    "\n",
    "#data format: dependent variable, feature values\n",
    "dta_splited = dta_RDD.map(lambda line: line.split(\",\")).map(lambda r:\n",
    "                                                                (float(r[0]),int( float(r[1])), \n",
    "                                                                 int(float(r[2])),int(float(r[3])),\n",
    "                                                                  int(float(r[4])),int(float(r[5])),\n",
    "                                                                 int(float(r[6])),int(float(r[7])) ))\n",
    "\n",
    "\n",
    "# dta_splited.first()\n",
    "dta= dta_splited\n",
    "dta.cache()\n",
    "print dta.first()\n",
    "print dta.count()\n",
    "\n",
    "# dta_filtered = dta.filter(lambda line: line[0] <2000 )\n",
    "# dta_filtered.cache()\n",
    "# dta_filtered.count()\n",
    "# print dta.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46.7446171548, 4, 13, 105)\n",
      "1000000\n"
     ]
    }
   ],
   "source": [
    "# synthetic data\n",
    "dta_RDD = sc.textFile(\"file:///home/tguo/tian_src/synthetic_data.txt\")\n",
    "dta_RDD.cache()\n",
    "\n",
    "dta_splited = dta_RDD.map(lambda line: line.split(\",\")).map(lambda r: ( float(r[3]),int(r[0]),int(r[1]),int(r[2])) )\n",
    "# ( float(r[2]),float(r[0]),float(r[1]) ) \n",
    "\n",
    "dta= dta_splited\n",
    "dta.cache()\n",
    "print dta.first()\n",
    "print dta.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#statistical on synthetic data\n",
    "\n",
    "dta.map( lambda line: line[0]>1000).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# basic functions in one-pass method [on the cluster side]\n",
    "\n",
    "# assign each data instance to the node\n",
    "def search_nodeToData(features, tree):\n",
    "    nodeNum=len(tree)   \n",
    "    if nodeNum == 0:\n",
    "        return 0;\n",
    "    \n",
    "    current_nodeIdx=0\n",
    "    while current_nodeIdx< nodeNum:\n",
    "        split_feature= tree[current_nodeIdx][0]\n",
    "        split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "        if split_feature==-1:\n",
    "            return -1\n",
    "    \n",
    "        if (( 1<<features[split_feature]) &  split_valueSet) != 0 :\n",
    "            current_nodeIdx=current_nodeIdx*2+1\n",
    "        else:\n",
    "            current_nodeIdx=current_nodeIdx*2+2        \n",
    "    return current_nodeIdx - nodeNum\n",
    "\n",
    "def partition_dataToNode(list_dvAndfeatures):\n",
    "    dataToNode_map=[]\n",
    "    res=[]\n",
    "    for dvAndfeatures in list_dvAndfeatures:\n",
    "        \n",
    "        Y= dvAndfeatures[0]\n",
    "        node= search_nodeToData(dvAndfeatures[1:numFeatures+1], \\\n",
    "                                currentNode_split_fromMaster.value)\n",
    "        dataToNode_map.append(node)\n",
    "        res.append( (node, dvAndfeatures )   )\n",
    "    return res\n",
    "def dataToNode_assignment( data_rdd ):\n",
    "    dataToNode_map = data_rdd.mapPartitions( partition_dataToNode )\n",
    "    dataToNode_map.cache()\n",
    "    return dataToNode_map\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "# extract values for each feature\n",
    "def partition_featureValues(list_dvAndfeatures):\n",
    "\n",
    "    feature_valueSet={}\n",
    "    \n",
    "    for i in range(0,numFeatures):\n",
    "        feature_valueSet.update( {i: set()} )\n",
    "        \n",
    "    for dvAndfeatures in list_dvAndfeatures:\n",
    "        for i in range(0,numFeatures):\n",
    "            feature_val= dvAndfeatures[i+1]\n",
    "            feature_valueSet[i].add(feature_val)\n",
    "    return zip( feature_valueSet.keys(), feature_valueSet.values() )  \n",
    "\n",
    "def merge_featureValues(  valueSet1, valueSet2):\n",
    "    return valueSet1.union(valueSet2)\n",
    "    \n",
    "def data_featureValues_collect( data_rdd ):\n",
    "    feature_valueSet_part = data_rdd.mapPartitions( partition_featureValues )\n",
    "    feature_valueSet_local = \\\n",
    "    feature_valueSet_part.reduceByKey(lambda set1, set2: merge_featureValues(set1,set2 )).collect()\n",
    "    feature_valueSet_local.sort()\n",
    "    #test\n",
    "#     print feature_valueSet_local\n",
    "    return feature_valueSet_local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split decision [on the local side]\n",
    "\n",
    "#implementation notes:\n",
    "# bit-operation to record the set of selected feature values in the left child\n",
    "\n",
    "\n",
    "#calculate median and MAD for a set of data in dataToSplit_rdd\n",
    "def median_MAD( dataToSplit_rdd, feature_id, value_set ):\n",
    "    \n",
    "    tmp_rdd=dataToSplit_rdd.filter(lambda l:l[1][feature_id+1] in value_set).map(lambda l:l[1][0]).cache()\n",
    "    tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "    #debug\n",
    "#     print 'debug:', tmp_rdd_count\n",
    "    \n",
    "    if tmp_rdd_count==0:\n",
    "        return (10000, 100000, 10000000)\n",
    "    \n",
    "    if(tmp_rdd_count%2 ==0):\n",
    "        tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "        tmphalf.sort()\n",
    "        tmpcnt= len(tmphalf)\n",
    "        tmp_median = (tmphalf[0] + tmphalf[1])*1.0/2.0\n",
    "    else:\n",
    "        tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "        tmphalf.sort()\n",
    "        tmpcnt= len(tmphalf)\n",
    "        tmp_median = tmphalf[0]\n",
    "    \n",
    "    tmpMAD_rdd = tmp_rdd.map(lambda line: abs(line-tmp_median) ).cache()\n",
    "    tmpMAD = tmpMAD_rdd.reduce( lambda a,b: a+b)\n",
    "    \n",
    "    return (tmp_rdd_count, tmp_median, tmpMAD)\n",
    "\n",
    "def split_onOneFeature_exact(node_data, node_data_cnt, MAD_node_ini, feature_valueList, feature_id):\n",
    "    \n",
    "    left_count=0\n",
    "    right_count= node_data_cnt\n",
    "    bestSplitMetric=  MAD_node_ini \n",
    "    \n",
    "    #this is a bit-record variable\n",
    "    leftSplit_valueSet =0 \n",
    "    leftSplit_count=0 \n",
    "    rightSplit_count=0\n",
    "    \n",
    "    bestLeft=(-1,-1)\n",
    "    bestRight=(-1,-1)\n",
    "    \n",
    "    #sort feature values according to median \n",
    "    sorted_value_map=[]\n",
    "    tmp_value_set=[1]\n",
    "    \n",
    "    for i in feature_valueList:\n",
    "        currentVal = i\n",
    "        dataToFeatureValue_rdd=node_data.filter(lambda l:l[1][feature_id+1]==currentVal).map(lambda l:l[1][0]).cache()\n",
    "        dataToFeatureValue_rdd_count = dataToFeatureValue_rdd.count()\n",
    "        \n",
    "        tmp_value_set[0]=i\n",
    "#         tmp_mad = median_MAD_singleVal( dataToFeatureValue_rdd, feature_id, i)    \n",
    "#-----------------------------------        \n",
    "        tmp_rdd=node_data.filter(lambda l:l[1][feature_id+1] == i ).map(lambda l:l[1][0]).cache()\n",
    "        tmp_rdd_count = tmp_rdd.count()\n",
    "    \n",
    "        if tmp_rdd_count==0:\n",
    "            continue;\n",
    "        else:\n",
    "            if(tmp_rdd_count%2 ==0):\n",
    "                tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "                tmphalf.sort()\n",
    "                tmpcnt= len(tmphalf)\n",
    "                tmp_median = (tmphalf[0] + tmphalf[1])*1.0/2.0\n",
    "            else:\n",
    "                tmphalf = tmp_rdd.top( tmp_rdd_count/2+1  )\n",
    "                tmphalf.sort()\n",
    "                tmpcnt= len(tmphalf)\n",
    "                tmp_median = tmphalf[0]\n",
    "    \n",
    "            tmpMAD_rdd = tmp_rdd.map(lambda line: abs(line-tmp_median) ).cache()\n",
    "            tmpMAD = tmpMAD_rdd.reduce( lambda a,b: a+b)\n",
    "       \n",
    "        tmp_mad=(tmp_rdd_count, tmp_median, tmpMAD)\n",
    "# ------------------------------------\n",
    "        sorted_value_map.append( (tmp_mad[1],i) )\n",
    "        \n",
    "    sorted_value_map.sort()\n",
    "    len_values= len(sorted_value_map)\n",
    "    \n",
    "    # scan the split positions    \n",
    "    left_value_set=[]\n",
    "    right_value_set= copy.deepcopy(feature_valueList)\n",
    "    \n",
    "    for k in range(0,len_values-1):\n",
    "        \n",
    "        current_feature_value = sorted_value_map[k][1] \n",
    "        \n",
    "        left_value_set.append(current_feature_value)\n",
    "        right_value_set.remove(current_feature_value)\n",
    "        \n",
    "        left = median_MAD( node_data, feature_id, left_value_set )\n",
    "        right= median_MAD( node_data, feature_id, right_value_set )\n",
    "        \n",
    "        #debug\n",
    "#         print left[1],right[1]\n",
    "        \n",
    "        leftSplit_count = left[0]\n",
    "        leftMedian= left[1]\n",
    "        leftMetric= left[2]\n",
    "        \n",
    "        rightSplit_count= right[0]\n",
    "        rightMedian= right[1]\n",
    "        rightMetric= right[2]  \n",
    "        \n",
    "        current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "        if current_splitMetric < bestSplitMetric:\n",
    "            bestSplitMetric=current_splitMetric     \n",
    "            bestLeft=left\n",
    "            bestRight=right\n",
    "            #bit variable to record which value is chosen for the left child\n",
    "            leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "\n",
    "    return [ leftSplit_valueSet, bestLeft[1], bestRight[1], bestSplitMetric  ] \n",
    "#     value_set, left median, right median, weighted mad\n",
    "\n",
    "def find_bestSplit_exact( dataToNode, current_NumNodes, nodes_tree, nodes_tree_test, feature_valueList):\n",
    "    \n",
    "    tmpnode_cnt=[]\n",
    "    presplit=[]\n",
    "        \n",
    "    for i in range(0, current_NumNodes):\n",
    "       \n",
    "        #prepare data for this node\n",
    "        current_node=i\n",
    "        current_node_data = dataToNode.filter( lambda line: line[0] == current_node ).cache()\n",
    "        current_node_data_cnt = current_node_data.count()\n",
    "        \n",
    "        tmp_medianMAD = median_MAD( current_node_data, 2, feature_valueList[2][1] )\n",
    "        best_splitMetric_sofar= tmp_medianMAD[2]\n",
    "        \n",
    "        #debug\n",
    "#         print tmp_medianMAD[1]\n",
    "        \n",
    "        best_split=(-1,-1,-1,-1)\n",
    "        best_split_feature=-1\n",
    "        best_split_featureValueSet=-1\n",
    "                \n",
    "        #debug\n",
    "        tmpnode_cnt.append( current_node_data.count()  )\n",
    "        presplit.append( tmp_medianMAD[2]/100000 )\n",
    "        \n",
    "        for j in range(0,numFeatures):    \n",
    "                                    \n",
    "            cur_split=split_onOneFeature_exact(current_node_data,current_node_data_cnt,\\\n",
    "                                           best_splitMetric_sofar,feature_valueList[j][1],j)\n",
    "    \n",
    "            if cur_split[3] < best_splitMetric_sofar:\n",
    "                best_split=cur_split\n",
    "                best_splitMetric_sofar=cur_split[3]\n",
    "                best_split_feature = j\n",
    "                best_split_featureValueSet= cur_split[0]      \n",
    "\n",
    "        nodes_tree.append(  (best_split_feature,  best_split_featureValueSet  )  )\n",
    "        \n",
    "        # tree for predicting   \n",
    "        nodes_tree_test.append(best_split[1])\n",
    "        nodes_tree_test.append(best_split[2])\n",
    "        \n",
    "    #debug\n",
    "#     print tmpnode_cnt\n",
    "#     print presplit\n",
    "    \n",
    "#     return nodes_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current split decision: []\n",
      "0 -th level running time:  0.00899815559387 sec 70.3219788074 sec\n",
      "current split decision: [(2, 40564819207303340847894502572032L)]\n",
      "1 -th level running time:  0.00852012634277 sec 68.3730518818 sec\n",
      "current split decision: [(2, 40564819207303340847894502572032L), (0, 12), (2, 10141204801825835211973625643008L)]\n",
      "2 -th level running time:  0.00760006904602 sec 67.0257480145 sec\n"
     ]
    }
   ],
   "source": [
    "# training  process  \n",
    "dta_train = dta.sample(False, .7, 12345)\n",
    "dta_test = dta.sample(False, .3, 43243)\n",
    "dta_test = dta_test.filter(lambda line: line[0] <1000 )\n",
    "\n",
    "#parameters\n",
    "maxdepth=3\n",
    "numFeatures=3\n",
    "\n",
    "node_split=[]\n",
    "node_test=[]\n",
    "#ini parameter\n",
    "currentNode_split_fromMaster = sc.broadcast([])\n",
    "\n",
    "#prepare feature value set\n",
    "feature_valueSet = data_featureValues_collect( dta_train )\n",
    "feature_valueList=[]\n",
    "for i in range(0, len(feature_valueSet)):\n",
    "    feature_valueList.append( ( i, list(feature_valueSet[i][1]) ) )\n",
    "    \n",
    "\n",
    "for i in range(0,maxdepth):\n",
    "    \n",
    "    start = time.time()\n",
    "    current_nodeNum= int( math.pow(2,  i))\n",
    "    \n",
    "    print 'current split decision:',currentNode_split_fromMaster.value\n",
    "\n",
    "    dataToNode = dataToNode_assignment( dta_train )\n",
    "    cluster_end = time.time() \n",
    "    find_bestSplit_exact(dataToNode,current_nodeNum,node_split,node_test, feature_valueList)   \n",
    "#   ( dataToNode, current_NumNodes, nodes_tree, nodes_tree_test, feature_valueSet):\n",
    "    currentNode_split_fromMaster = sc.broadcast(node_split)\n",
    "  \n",
    "    end = time.time() \n",
    "\n",
    "    print i,'-th level running time: ', cluster_end - start,'sec', end- cluster_end, 'sec'\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#do not run this part \n",
    "# training and testing process\n",
    "\n",
    "# with outliers\n",
    "tmpdta = dta.takeSample(False, 100000, 12243)\n",
    "dta_train = sc.parallelize(tmpdta).cache().sample(False, .7, 12345)\n",
    "dta_test = sc.parallelize(tmpdta).cache().sample(False, .3, 43243)\n",
    "\n",
    "maxdepth=10\n",
    "numFeatures=3\n",
    "\n",
    "node_split=[]\n",
    "node_test=[]\n",
    "run_time=[]\n",
    "\n",
    "def search_nodeToData(features, tree):\n",
    "    nodeNum=len(tree)   \n",
    "    if nodeNum == 0:\n",
    "        return 0;\n",
    "    \n",
    "    current_nodeIdx=0\n",
    "    while current_nodeIdx< nodeNum:\n",
    "        split_feature= tree[current_nodeIdx][0]\n",
    "        split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "        if split_feature==-1:\n",
    "            return -1\n",
    "        if (( 1<<features[split_feature]) &  split_valueSet) != 0 :\n",
    "            current_nodeIdx=current_nodeIdx*2+1\n",
    "        else:\n",
    "            current_nodeIdx=current_nodeIdx*2+2        \n",
    "    return current_nodeIdx - nodeNum\n",
    "\n",
    "def tree_test_mapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], node_split)\n",
    "    return (leaf_nodes[ tmpnode ]  - line[0])*(leaf_nodes[ tmpnode ]  - line[0])\n",
    "    \n",
    "def tree_test( testData_rdd ):\n",
    "    err_rdd=testData_rdd.map( lambda line: tree_test_mapFunc(line))\n",
    "    err_sum = err_rdd.reduce( lambda a,b: a+b)\n",
    "    return err_sum / testData_rdd.count()\n",
    "    \n",
    "    \n",
    "#ini parameter on the cluster\n",
    "currentNode_split_fromMaster = sc.broadcast([])\n",
    "test_err = []\n",
    "train_err = []\n",
    "#prepare feature value set\n",
    "feature_valueSet = data_featureValues_collect( dta_train )\n",
    "feature_valueList=[]\n",
    "for i in range(0, len(feature_valueSet)):\n",
    "    feature_valueList.append( ( i, list(feature_valueSet[i][1]) ) )\n",
    "    \n",
    "    \n",
    "for i in range(6,maxdepth):\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    del node_split[:]\n",
    "    del node_test[:]\n",
    "    currentNode_split_fromMaster = sc.broadcast([])\n",
    "    \n",
    "    for cur_depth in range(0,i):\n",
    "        \n",
    "        current_nodeNum= int( math.pow(2,  cur_depth)) \n",
    "        \n",
    "        dataToNode = dataToNode_assignment( dta_train )\n",
    "        find_bestSplit_exact(dataToNode,current_nodeNum, node_split, node_test, feature_valueList)   \n",
    "#         ( dataToNode, current_NumNodes, nodes_tree, nodes_tree_test, feature_valueSet):\n",
    "        #for whole tree\n",
    "        currentNode_split_fromMaster = sc.broadcast(node_split)\n",
    "    \n",
    "    end = time.time()\n",
    "    elapsed = end - start    \n",
    "    leaf_nodes = node_test[ len(node_test)-  (int)( math.pow(2,i)): len(node_test)  ]\n",
    "\n",
    "    \n",
    "    tmp_test_err= tree_test( dta_test )\n",
    "    tmp_train_err= tree_test( dta_train )\n",
    "    \n",
    "    test_err.append( tmp_test_err)\n",
    "    train_err.append( tmp_train_err )\n",
    "    run_time.append(elapsed)\n",
    "    \n",
    "    print \"error at tree height\", i,\":\",  tmp_test_err, tmp_train_err\n",
    "    print \"running time at tree height\", i,\":\",  elapsed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error at tree height 1 : 838.106352707 836.715110029\n",
      "running time at tree height 1 : 17.4006810188\n",
      "error at tree height 2 : 808.261180267 807.090877025\n",
      "running time at tree height 2 : 44.3271620274\n",
      "error at tree height 3 : 771.689689018 763.051804178\n",
      "running time at tree height 3 : 89.2100429535\n",
      "error at tree height"
     ]
    }
   ],
   "source": [
    "# training and testing process\n",
    "\n",
    "# without outliers\n",
    "tmpdta = dta.filter(lambda line: line[0]<1000).takeSample(False, 100000, 12243)\n",
    "dta_train = sc.parallelize(tmpdta).cache().sample(False, .7, 12345)\n",
    "dta_test = sc.parallelize(tmpdta).cache().sample(False, .3, 43243)\n",
    "\n",
    "maxdepth=10\n",
    "numFeatures=3\n",
    "\n",
    "node_split=[]\n",
    "node_test=[]\n",
    "run_time=[]\n",
    "\n",
    "def search_nodeToData(features, tree):\n",
    "    nodeNum=len(tree)   \n",
    "    if nodeNum == 0:\n",
    "        return 0;\n",
    "    \n",
    "    current_nodeIdx=0\n",
    "    while current_nodeIdx< nodeNum:\n",
    "        split_feature= tree[current_nodeIdx][0]\n",
    "        split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "        if split_feature==-1:\n",
    "            return -1\n",
    "        if (( 1<<features[split_feature]) &  split_valueSet) != 0 :\n",
    "            current_nodeIdx=current_nodeIdx*2+1\n",
    "        else:\n",
    "            current_nodeIdx=current_nodeIdx*2+2        \n",
    "    return current_nodeIdx - nodeNum\n",
    "\n",
    "def tree_test_mapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], node_split)\n",
    "    return (leaf_nodes[ tmpnode ]  - line[0])*(leaf_nodes[ tmpnode ]  - line[0])\n",
    "    \n",
    "def tree_test( testData_rdd ):\n",
    "    err_rdd=testData_rdd.map( lambda line: tree_test_mapFunc(line))\n",
    "    err_sum = err_rdd.reduce( lambda a,b: a+b)\n",
    "    return err_sum / testData_rdd.count()\n",
    "    \n",
    "    \n",
    "#ini parameter on the cluster\n",
    "currentNode_split_fromMaster = sc.broadcast([])\n",
    "test_err = []\n",
    "train_err = []\n",
    "#prepare feature value set\n",
    "feature_valueSet = data_featureValues_collect( dta_train )\n",
    "feature_valueList=[]\n",
    "for i in range(0, len(feature_valueSet)):\n",
    "    feature_valueList.append( ( i, list(feature_valueSet[i][1]) ) )\n",
    "    \n",
    "    \n",
    "for i in range(1,maxdepth):\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    del node_split[:]\n",
    "    del node_test[:]\n",
    "    currentNode_split_fromMaster = sc.broadcast([])\n",
    "    \n",
    "    for cur_depth in range(0,i):\n",
    "        \n",
    "        current_nodeNum= int( math.pow(2,  cur_depth)) \n",
    "        \n",
    "        dataToNode = dataToNode_assignment( dta_train )\n",
    "        find_bestSplit_exact(dataToNode,current_nodeNum, node_split, node_test, feature_valueList)   \n",
    "#         ( dataToNode, current_NumNodes, nodes_tree, nodes_tree_test, feature_valueSet):\n",
    "        #for whole tree\n",
    "        currentNode_split_fromMaster = sc.broadcast(node_split)\n",
    "    \n",
    "    end = time.time()\n",
    "    elapsed = end - start    \n",
    "    leaf_nodes = node_test[ len(node_test)-  (int)( math.pow(2,i)): len(node_test)  ]\n",
    "\n",
    "    \n",
    "    tmp_test_err= tree_test( dta_test )\n",
    "    tmp_train_err= tree_test( dta_train )\n",
    "    \n",
    "    test_err.append( tmp_test_err)\n",
    "    train_err.append( tmp_train_err )\n",
    "    run_time.append(elapsed)\n",
    "    \n",
    "    print \"error at tree height\", i,\":\",  tmp_test_err, tmp_train_err\n",
    "    print \"running time at tree height\", i,\":\",  elapsed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[838.1063527065705,\n",
       " 808.2611802671888,\n",
       " 771.6896890183064,\n",
       " 673.2221276743697,\n",
       " 574.8742789018773,\n",
       " 578.6416418235431]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[836.7151100286254,\n",
       " 807.0908770251182,\n",
       " 763.0518041779437,\n",
       " 670.7096534920322,\n",
       " 571.0572144974398,\n",
       " 580.2175022679407]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "error at tree height 1 : 836.120673266 993197.408799\n",
    "running time at tree height 1 : 71.6192398071\n",
    "error at tree height 2 : 805.794778995 994017.544055\n",
    "running time at tree height 2 : 138.060220957\n",
    "error at tree height 3 : 765.500220461 994394.511005\n",
    "running time at tree height 3 : 208.832412004\n",
    "    \n",
    "error at tree height 4 : 734.829314704 993707.63729\n",
    "running time at tree height 4 : 281.407808065\n",
    "    \n",
    "error at tree height 5 : 644.236932264 994344.82116\n",
    "running time at tree height 5 : 4879.72735\n",
    "\n",
    "error at tree height 6 : 529.4375226396904 994434.7416043472\n",
    "\n",
    "error at tree height 7 : 770.5749206468843 992877.3588177576\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69015"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#select one feature-value\n",
    "feature_id=1\n",
    "feature_val=11\n",
    "\n",
    "dataToFeatureValue_rdd=node_data.filter(lambda line:line[1][feature_id+1]==feature_val).map(lambda line:line[1][0])\n",
    "\n",
    "dataToFeatureValue_rdd_count = dataToFeatureValue_rdd.count()\n",
    "        \n",
    "# med_tmpLarge= dataToFeatureValue_rdd.top( dataToFeatureValue_rdd_count/2+1  )\n",
    "med_tmp = dataToFeatureValue_rdd.top( dataToFeatureValue_rdd_count/2  )\n",
    "med_tmp.sort()\n",
    "tmpcnt= len(med_tmp)\n",
    "print med_tmp[ tmpcnt-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437547\n"
     ]
    }
   ],
   "source": [
    "#select several feature-values\n",
    "feature_values=[1,2,3]\n",
    "\n",
    "dataToFeatureValue_rdd=node_data.filter(lambda line:line[1][feature_id+1] in feature_values).map(lambda line:line[1][0])\n",
    "\n",
    "dataToFeatureValue_rdd_count = dataToFeatureValue_rdd.count()\n",
    "\n",
    "print dataToFeatureValue_rdd_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214062219.0\n"
     ]
    }
   ],
   "source": [
    "#MAD\n",
    "median= 100\n",
    "# dataToFeatureValue_rdd= sc.parallelize( [1,2,3,4,5,6,7,8,9] )\n",
    "\n",
    "absDevi_rdd=dataToFeatureValue_rdd.map(lambda line: abs(line-median)  )\n",
    "# absDevi_rdd.count()\n",
    "MAD=absDevi_rdd.reduce( lambda a,b: a+b)\n",
    "print MAD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "med_tmp.sort()\n",
    "tmpcnt= len(med_tmp)\n",
    "print med_tmp[ tmpcnt-1]\n",
    "print med_tmp\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
