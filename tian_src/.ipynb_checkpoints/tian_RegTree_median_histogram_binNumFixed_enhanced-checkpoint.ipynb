{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os \n",
    "import sys \n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "from datetime  import datetime \n",
    "\n",
    "import numpy as np  # learn \n",
    "import pandas as pd # learn\n",
    "from pandas import *\n",
    "from numpy import *\n",
    "\n",
    "from scipy import stats # look at scipy\n",
    "from scipy import linalg\n",
    "from scipy import *\n",
    "\n",
    " \n",
    "import matplotlib as mplt # learn matplolib \n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "import seaborn as sns \n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc={\"figure.figsize\": (14, 6)})\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import sklearn as sk\n",
    "import itertools\n",
    "\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328.0, 10, 10, 2, 2, 8, 1, 3)\n",
      "3315067\n"
     ]
    }
   ],
   "source": [
    "#BT data loading\n",
    "\n",
    "# dta_RDD = sc.textFile(\"file:///home/tguo/data/tian-test/udpjitter-H1april2014_regTree.csv\")\n",
    "\n",
    "dta_RDD = sc.textFile(\"hdfs://computer61.ant-net/user/tguo/udpjitter-H1april2014_regTree_mllib.csv\")\n",
    "\n",
    "\n",
    "dta_splited = dta_RDD.map(lambda line: line.split(\",\")).map(lambda r:\n",
    "                                                                (float(r[0]),int( float(r[1])), \n",
    "                                                                 int(float(r[2])),int(float(r[3])),\n",
    "                                                                 int(float(r[4])),int(float(r[5])),\n",
    "                                                                 int(float(r[6])),int(float(r[7])) ))\n",
    "# dta_splited.first()\n",
    "\n",
    "dta= dta_splited\n",
    "dta.cache()\n",
    "print dta.first()\n",
    "print dta.count()\n",
    "\n",
    "\n",
    "# dta_filtered = dta.filter(lambda line: line[0] <2000 )\n",
    "# dta_filtered.cache()\n",
    "# dta_filtered.count()\n",
    "# print dta.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# synthetic data: generate\n",
    "from SyntheticDataGenerator import *\n",
    "\n",
    "filename = './syndata_set/syndata_f5v5_6zeros_1percen.txt'\n",
    "rows = 2000000\n",
    "cols = 5\n",
    "featureValues=5\n",
    "\n",
    "data = SyntheticDataGenerator(rows, cols,featureValues+1) \n",
    "data.writeData(filename)\n",
    "\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: [(110.602058806, 2, 13, 103, 1004, 10004)]\n",
      "2000000\n",
      "feature value re-indexed: (110.602058806, 0, 2, 2, 3, 3)\n",
      "2000000\n",
      "done\n",
      "check:\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# synthetic data\n",
    "dta_RDD = sc.textFile(\"file:///home/tguo/tian_src/syndata_set/syndata_f5v5_6zeros_1percen.txt\")\n",
    "# synthetic_data_6zeros_0.01percen.txt\n",
    "dta_RDD.cache()\n",
    "num_cols= 5\n",
    "\n",
    "\n",
    "def cons_valueFeatures( line):\n",
    "    val_feature=[float(line[num_cols]) ]\n",
    "    for i in range(0, num_cols):\n",
    "        val_feature.append(  int(line[i]))\n",
    "    return tuple(val_feature)\n",
    "        \n",
    "dta_splited = dta_RDD.map(lambda line: line.split(\",\")).map(lambda r: cons_valueFeatures(r)) \n",
    "dta= dta_splited\n",
    "dta.cache()\n",
    "\n",
    "print 'original:',dta.take(1)\n",
    "print dta.count()\n",
    "\n",
    "# re-set index of categorical features\n",
    "\n",
    "feature_dist=[]\n",
    "\n",
    "for i in range(0, num_cols):\n",
    "    tmp1 = dta.map(lambda line: line[i+1]).distinct().collect()\n",
    "    feature_dist.append(tmp1)\n",
    "\n",
    "fea_cnt= num_cols\n",
    "fea_map=[]\n",
    "\n",
    "for i in range(0, fea_cnt):\n",
    "    tmpcnt = len(feature_dist[i])\n",
    "    val_map= dict( zip(feature_dist[i], range(0,tmpcnt)) )\n",
    "    fea_map.append(val_map)\n",
    "\n",
    "def reset_index( line ):\n",
    "    tmp=[ line[0] ]\n",
    "    for i in range(0, num_cols):\n",
    "        tmp.append( fea_map[i][ line[i+1] ] )\n",
    "    return tuple(tmp)\n",
    "\n",
    "dta = dta.map( lambda line: reset_index(line)  )\n",
    "\n",
    "print 'feature value re-indexed:',dta.first()\n",
    "print dta.count()\n",
    "  \n",
    "    \n",
    "print 'done'\n",
    "\n",
    "print 'check:'\n",
    "\n",
    "for i in range(0, num_cols):\n",
    "    tmp1 = dta.map(lambda line: line[i+1]).distinct().collect()\n",
    "    print len(tmp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#debug: histogram operations on cluster side\n",
    "\n",
    "testdta = dta.map(lambda line: line[0]).take(2000000)\n",
    "\n",
    "bin_num =100\n",
    "tmphist=[]\n",
    "\n",
    "for i in testdta:\n",
    "    update_hist_list(tmphist, i)\n",
    "\n",
    "cnt=len(tmphist)\n",
    "print cnt\n",
    "\n",
    "for i in range(0,cnt-1):\n",
    "    if tmphist[i][0]> tmphist[i+1][0]:\n",
    "        print 'left side error'\n",
    "    if tmphist[i][1]> tmphist[i+1][1]:\n",
    "        print 'right side error'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# basic functions in one-pass method  [on the cluster side]\n",
    "\n",
    "# assign each data instance to the node\n",
    "def search_nodeToData(features, tree):\n",
    "    \n",
    "    nodeNum=len(tree)   \n",
    "    if nodeNum == 0:\n",
    "        return 0;\n",
    "    \n",
    "    current_nodeIdx=0\n",
    "    while current_nodeIdx< nodeNum:\n",
    "        \n",
    "        split_feature= tree[current_nodeIdx][0]\n",
    "        split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "        # unbalanced tree grow\n",
    "        if split_feature==-1:\n",
    "            current_nodeIdx=current_nodeIdx*2+1+ random.randint(0, 1)\n",
    "            continue\n",
    "            \n",
    "        if (( 1<<features[split_feature]) &  split_valueSet) != 0 :\n",
    "            current_nodeIdx=current_nodeIdx*2+1\n",
    "        else:\n",
    "            current_nodeIdx=current_nodeIdx*2+2\n",
    "            \n",
    "    return current_nodeIdx - nodeNum\n",
    "\n",
    "# def search_nodeToData_4test(features, tree):\n",
    "    \n",
    "#     nodeNum=len(tree)   \n",
    "#     if nodeNum == 0:\n",
    "#         return 0;\n",
    "    \n",
    "#     current_nodeIdx=0\n",
    "#     while current_nodeIdx< nodeNum:\n",
    "        \n",
    "# #         split_feature= tree[current_nodeIdx][0]\n",
    "# #         split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "#         current_nodeIdx=current_nodeIdx*2+1+ random.randint(0, 1)\n",
    "        \n",
    "#         # unbalanced tree grow\n",
    "# #         if split_feature==-1:\n",
    "# #             current_nodeIdx=current_nodeIdx*2+1+ random.randint(0, 1)\n",
    "# #             continue\n",
    "# #         if (( 1<<features[split_feature]) &  split_valueSet) != 0 :\n",
    "# #             current_nodeIdx=current_nodeIdx*2+1\n",
    "# #         else:\n",
    "# #             current_nodeIdx=current_nodeIdx*2+2\n",
    "            \n",
    "#     return current_nodeIdx - nodeNum\n",
    "\n",
    "# ****\n",
    "# def update_hist(hist, newY):\n",
    "\n",
    "#     numbins=len(hist)\n",
    "    \n",
    "#     if newY in hist.keys():\n",
    "#         hist[newY][2]= hist[newY][2] +1 \n",
    "#     else:\n",
    "#         hist.update({newY: [newY,newY,1]})\n",
    "#         if numbins+1 > bin_num:\n",
    "#             hist.update({newY: [newY,newY,1]}) \n",
    "#             #data in each bin: left bound, right bound, count\n",
    "        \n",
    "#             sorted_binIds=hist.keys()\n",
    "#             # binId is the lower bound of the value range of the bin\n",
    "#             sorted_binIds.sort()\n",
    "        \n",
    "#             mindis= sorted_binIds[1]- sorted_binIds[0]\n",
    "#             mindis_bin_left=sorted_binIds[0]\n",
    "#             mindis_bin_right=sorted_binIds[1]\n",
    "            \n",
    "#             for i in range(1,numbins+1):\n",
    "#                 tmp= sorted_binIds[i]- sorted_binIds[i-1]\n",
    "#                 if tmp < mindis:\n",
    "#                     mindis=tmp\n",
    "#                     mindis_bin_left= sorted_binIds[i-1]\n",
    "#                     mindis_bin_right= sorted_binIds[i]       \n",
    "            \n",
    "# #           print hist_local[mindis_bin_right][1],hist_local[mindis_bin_left][1]\n",
    "#             hist[mindis_bin_left][1] = max(hist[mindis_bin_right][1],hist[mindis_bin_left][1] )                        \n",
    "#             hist[mindis_bin_left][2] = hist[mindis_bin_left][2] + \\\n",
    "#             hist[mindis_bin_right][2]\n",
    "#             del hist[mindis_bin_right]\n",
    "            \n",
    "#     return hist\n",
    "\n",
    "# def update_hist_list_insert( hist_list, dis_heap, dis_set, newY):\n",
    "    \n",
    "#     pos= update_hist_list_lookup( hist_list, newY )\n",
    "#     cur_cnt= len(hist_list)\n",
    "    \n",
    "#     if pos ==0:\n",
    "#         tmpl=newY\n",
    "#         tmpr=hist_list[ pos ][1]\n",
    "#         heappush(dis_heap, ( abs( tmpr-tmpl ),  tmpl,tmpr, 0,1 ) )\n",
    "#         dis_set.add(  ( tmpl,tmpr )    )\n",
    "        \n",
    "#     elif pos > cur_cnt:\n",
    "#         tmpl=hist_list[cur_cnt-1][2]\n",
    "#         tmpr= newY\n",
    "#         heappush(dis_heap, ( abs( tmpr-tmpl ), tmpl,tmpr, cur_cnt-1, cur_cnt)     )\n",
    "#         dis_set.add(  ( tmpl,tmpr )    )                 \n",
    "#     else:\n",
    "#         tmpl= hist_list[pos-1 ]\n",
    "#         tmpr= hist_list[pos]               \n",
    "#         heappush(dis_heap, ( abs( tmpr-newY ),  tmpl,tmpr, pos-1,pos+1 ) )\n",
    "#         heappush(dis_heap, ( abs( newY-tmpl ),  tmpl,tmpr, pos-1,pos+1 ) )\n",
    "                 \n",
    "#         dis_set.discard(  ( tmpl,tmpr )    )\n",
    "#         dis_set.add( (tmpl, newY  ) )\n",
    "#         dis_set.add( (newY,tmpr  ) )\n",
    "#     return\n",
    "\n",
    "# # cnt, left, right\n",
    "# def update_hist_list_interface( hist_list, dis_heap, dis_set, newY ):\n",
    "    \n",
    "#     update_hist_list_insert( hist_list, dis_heap, dis_set, newY)\n",
    "    \n",
    "#     cnt= len(hist_list)+1\n",
    "#     if cnt< bin_num:\n",
    "#         return\n",
    "#     else:\n",
    "#         tmp_dis_tuple= heappop(dis_heap)\n",
    "#         while ( tmp_dis_tuple[1],tmp_dis_tuple[2] ) not in  dis_set:\n",
    "#                  tmp_dis_tuple= heappop(dis_heap)\n",
    "    \n",
    "#         dis_set.discard( ( tmp_dis_tuple[1],tmp_dis_tuple[2] )  )         \n",
    "#         lidx= tmp_dis_tuple[3]\n",
    "#         ridx= tmp_dis_tuple[4]\n",
    "                 \n",
    "#         tmpcnt= hist_list[lidx][0]+hist_list[ridx][0]\n",
    "#         tmpl= min( hist_list[lidx][1], hist_list[ridx][1]   )    \n",
    "#         tmpl= max( hist_list[lidx][2], hist_list[ridx][2]   )\n",
    "                 \n",
    "#         hist_list[lidx][0]=tmpcnt \n",
    "#         hist_list[lidx][1]=tmpl\n",
    "#         hist_list[lidx][2]=tmpr\n",
    "                 \n",
    "#         hist_list.pop( ridx  )\n",
    "    \n",
    "#     return\n",
    "\n",
    "def update_hist_list_lookup( hist_list, newY ):\n",
    "    \n",
    "    cnt= len(hist_list)\n",
    "    l=0\n",
    "    r= len(hist_list)-1\n",
    "    \n",
    "    while l<r-1:\n",
    "        mid= l+ (r-l)/2\n",
    "        \n",
    "        if hist_list[mid][0]> newY:\n",
    "            r=mid\n",
    "        elif newY > hist_list[mid][1]:\n",
    "            l=mid\n",
    "        elif  hist_list[mid][0]  <= newY and newY<= hist_list[mid][1]:\n",
    "            return [1,mid]\n",
    "    \n",
    "    if hist_list[l][0]  <= newY and newY<= hist_list[l][1]:\n",
    "            return [1,l]\n",
    "    \n",
    "    if l+1<cnt and hist_list[l+1][0]  <= newY and newY<= hist_list[l+1][1]:\n",
    "            return [1,l+1]\n",
    "\n",
    "        \n",
    "    if newY < hist_list[l][0]:\n",
    "        return [0,l]\n",
    "    \n",
    "    if l+1<cnt and newY< hist_list[ l+1 ][0]:\n",
    "        return [0,l+1]\n",
    "    elif l+1<cnt and newY> hist_list[ l+1 ][1]:\n",
    "        return [0,l+2]\n",
    "    \n",
    "    return [0,l+1]\n",
    "\n",
    "\n",
    "def update_hist_list( hist_list, newY ):\n",
    "    \n",
    "    cnt= len(hist_list)\n",
    "    \n",
    "    if cnt==0:\n",
    "        hist_list.append([newY, newY,1])\n",
    "    else:\n",
    "        pos=update_hist_list_lookup( hist_list, newY )\n",
    "        tmpidx=pos[1]\n",
    "        \n",
    "        if pos[0]==1:\n",
    "            hist_list[tmpidx][2]= hist_list[tmpidx][2]+1\n",
    "        else:\n",
    "            hist_list.insert(tmpidx, [ newY,newY,1])\n",
    "            \n",
    "    tmpdis=-1\n",
    "    merge_l=0\n",
    "    merge_r=0\n",
    "    cnt=len(hist_list)\n",
    "    \n",
    "    minDis =  hist_list[cnt-1][1] - hist_list[0][0]\n",
    "    \n",
    "    if cnt> bin_num:\n",
    "        \n",
    "        for i in range(0,cnt-1):\n",
    "            tmpdis= hist_list[i+1][0]-hist_list[i][1]\n",
    "            if tmpdis<minDis:\n",
    "                merge_l=i\n",
    "                merge_r=i+1\n",
    "                minDis=tmpdis\n",
    "        \n",
    "        hist_list[merge_l][1]= max(hist_list[merge_l][1],  hist_list[merge_r][1])\n",
    "        hist_list[merge_l][2]= hist_list[merge_l][2] + hist_list[merge_r][2]\n",
    "        hist_list.pop(merge_r)\n",
    "        \n",
    "def partition_combiner_hist(list_dvAndfeatures):\n",
    "    \n",
    "    nodes_dict={}\n",
    "    \n",
    "    for dvAndfeatures in list_dvAndfeatures:\n",
    "        \n",
    "        Y= dvAndfeatures[0]\n",
    "        \n",
    "#         ?????\n",
    "        node= search_nodeToData(dvAndfeatures[1:numFeatures+1], \\\n",
    "                                currentNode_split_fromMaster.value)\n",
    "        \n",
    "        if node in nodes_dict:\n",
    "            \n",
    "            # new added: s um of y in a node\n",
    "            nodes_dict[node]['sumY'] = nodes_dict[node]['sumY']+Y \n",
    "            nodes_dict[node]['count'] = nodes_dict[node]['count']+1     \n",
    "            update_hist_list( nodes_dict[node]['hist_list'], Y)\n",
    "\n",
    "            for i in range(0,numFeatures):    \n",
    "                feature_val= dvAndfeatures[i+1]\n",
    "                \n",
    "                if feature_val not in nodes_dict[node][i]:\n",
    "                    \n",
    "                    nodes_dict[node][i].update( { feature_val: {}   }  )\n",
    "                    nodes_dict[node][i][feature_val].update( {'count_inFeatureValue':1} )                \n",
    "                    nodes_dict[node][i][feature_val].update( {'hist_val_list': [] } )\n",
    "                    update_hist_list( nodes_dict[node][i][feature_val]['hist_val_list'], Y)\n",
    "                    \n",
    "                else:\n",
    "                    nodes_dict[node][i][feature_val]['count_inFeatureValue'] = \\\n",
    "                    nodes_dict[node][i][feature_val]['count_inFeatureValue']+1 \n",
    "                    update_hist_list( nodes_dict[node][i][feature_val]['hist_val_list'], Y)\n",
    "                \n",
    "        else:\n",
    "            nodes_dict.update( {node: {}})  \n",
    "            # new added: sum of y in a node\n",
    "            nodes_dict[node].update( { 'sumY': Y} )\n",
    "            nodes_dict[node].update( { 'count': 1} )\n",
    "            nodes_dict[node].update( { 'hist_list': []   } )  \n",
    "            update_hist_list( nodes_dict[node]['hist_list'], Y)\n",
    "    \n",
    "            for i in range(0,numFeatures):\n",
    "                feature_val= dvAndfeatures[i+1]\n",
    "                \n",
    "                nodes_dict[node].update( { i: {}} )    \n",
    "                nodes_dict[node][i].update( { feature_val: {}   }  )\n",
    "                nodes_dict[node][i][feature_val].update( {'count_inFeatureValue':1} )                \n",
    "                nodes_dict[node][i][feature_val].update( {'hist_val_list': [] } )\n",
    "                \n",
    "                update_hist_list( nodes_dict[node][i][feature_val]['hist_val_list'], Y)\n",
    "          \n",
    "    return zip(nodes_dict.keys(), nodes_dict.values())\n",
    "\n",
    "\n",
    "def merge_hist_list(hist1, hist2):\n",
    "    \n",
    "    tmphist=[]\n",
    "    p1=0\n",
    "    p2=0\n",
    "    c1=len(hist1)\n",
    "    c2=len(hist2)\n",
    "    cnt=0\n",
    "    \n",
    "    last_choose=1\n",
    "    \n",
    "    if p1<c1 and p2<c2 and hist1[p1][0] < hist2[p2][0]:\n",
    "        tmphist.append( [0,0,0] )\n",
    "        tmphist[cnt][0]= hist1[p1][0]\n",
    "        tmphist[cnt][1]= hist1[p1][1]\n",
    "        tmphist[cnt][2]= hist1[p1][2]\n",
    "        p1=p1+1\n",
    "        cnt=cnt+1\n",
    "        last_choose=1\n",
    "        \n",
    "    elif p1<c1 and p2<c2 and hist1[p1][0] >= hist2[p2][0] :\n",
    "        tmphist.append( [0,0,0] )\n",
    "        tmphist[cnt][0]= hist2[p2][0]\n",
    "        tmphist[cnt][1]= hist2[p2][1]\n",
    "        tmphist[cnt][2]= hist2[p2][2]\n",
    "        p2=p2+1\n",
    "        cnt=cnt+1\n",
    "        last_choose=2\n",
    "    \n",
    "    while p1<c1 and p2<c2:\n",
    "        \n",
    "#         if hist2[p2][1] <= tmphist[cnt-1][1]:\n",
    "#             tmphist[cnt-1][2]= tmphist[cnt-1][2] + hist2[p2][2]\n",
    "#             p2=p2+1\n",
    "        \n",
    "#         if hist1[p1][1] <= tmphist[cnt-1][1]:\n",
    "#             tmphist[cnt-1][2]= tmphist[cnt-1][2] + hist1[p1][2]\n",
    "#             p1=p1+1\n",
    "        \n",
    "#         if last_choose ==1:\n",
    "#             while p2<c2 and hist2[p2][1] <= tmphist[cnt-1][1]:\n",
    "# #                 tmphist[cnt-1][1]= max(tmphist[cnt-1][1], hist2[p2][1])\n",
    "#                 tmphist[cnt-1][2]= tmphist[cnt-1][2] + hist2[p2][2]\n",
    "#                 p2=p2+1\n",
    "#         elif last_choose ==2:        \n",
    "#             while p1<c1 and hist1[p1][1] <= tmphist[cnt-1][1]:\n",
    "# #                 tmphist[cnt-1][1]= max(tmphist[cnt-1][1],  hist1[p1][1])\n",
    "#                 tmphist[cnt-1][2]= tmphist[cnt-1][2] + hist1[p1][2]\n",
    "#                 p1=p1+1\n",
    "        \n",
    "        \n",
    "        \n",
    "#         if last_choose ==1:\n",
    "        while p2<c2 and hist2[p2][1] <= tmphist[cnt-1][1]:\n",
    "#                 tmphist[cnt-1][1]= max(tmphist[cnt-1][1], hist2[p2][1])\n",
    "            tmphist[cnt-1][2]= tmphist[cnt-1][2] + hist2[p2][2]\n",
    "            p2=p2+1\n",
    "#         elif last_choose ==2:        \n",
    "        while p1<c1 and hist1[p1][1] <= tmphist[cnt-1][1]:\n",
    "#                 tmphist[cnt-1][1]= max(tmphist[cnt-1][1],  hist1[p1][1])\n",
    "            tmphist[cnt-1][2]= tmphist[cnt-1][2] + hist1[p1][2]\n",
    "            p1=p1+1\n",
    "        \n",
    "        \n",
    "        \n",
    "        if p1<c1 and p2<c2 and hist1[p1][0] < hist2[p2][0]:\n",
    "            tmphist.append( [0,0,0] )\n",
    "            tmphist[cnt][0]= hist1[p1][0]\n",
    "            tmphist[cnt][1]= hist1[p1][1]\n",
    "            tmphist[cnt][2]= hist1[p1][2]\n",
    "            p1=p1+1\n",
    "            cnt=cnt+1  \n",
    "            last_choose =1\n",
    "                \n",
    "        elif p1<c1 and p2<c2 and hist1[p1][0] >= hist2[p2][0]:\n",
    "            tmphist.append( [0,0,0] )\n",
    "            tmphist[cnt][0]= hist2[p2][0]\n",
    "            tmphist[cnt][1]= hist2[p2][1]\n",
    "            tmphist[cnt][2]= hist2[p2][2]\n",
    "            p2=p2+1\n",
    "            cnt=cnt+1\n",
    "            last_choose =2\n",
    "    \n",
    "    \n",
    "    while p2<c2 and hist2[p2][1] <= tmphist[cnt-1][1]:\n",
    "            tmphist[cnt-1][2]= tmphist[cnt-1][2] + hist2[p2][2]\n",
    "            p2=p2+1\n",
    "    while p1<c1 and hist1[p1][1] <= tmphist[cnt-1][1]:\n",
    "            tmphist[cnt-1][2]= tmphist[cnt-1][2] + hist1[p1][2]\n",
    "            p1=p1+1\n",
    "\n",
    "    \n",
    "    while p1<c1:\n",
    "        \n",
    "        tmphist.append( [0,0,0] )\n",
    "        tmphist[cnt][0]= hist1[p1][0]\n",
    "        tmphist[cnt][1]= hist1[p1][1]\n",
    "        tmphist[cnt][2]= hist1[p1][2]\n",
    "        cnt=cnt+1\n",
    "        p1=p1+1\n",
    "        \n",
    "    while p2<c2:\n",
    "        \n",
    "        tmphist.append( [0,0,0] )\n",
    "        tmphist[cnt][0]= hist2[p2][0]\n",
    "        tmphist[cnt][1]= hist2[p2][1]\n",
    "        tmphist[cnt][2]= hist2[p2][2]\n",
    "        cnt=cnt+1\n",
    "        p2=p2+1\n",
    "\n",
    "#         ????\n",
    "    cnt= len(tmphist)\n",
    "    if cnt< bin_num:\n",
    "#         hist1=copy.deepcopy(tmphist)\n",
    "        return tmphist\n",
    "\n",
    "    \n",
    "    tmpdis=[]  \n",
    "    for i in range(0, cnt-1):\n",
    "        tmpdis.append( ( abs( tmphist[i+1][0]-tmphist[i][1]), i, i+1 ) )\n",
    "#         tmpdis.append( ( abs( tmphist[i+1][1]-tmphist[i][0]), i, i+1 ) )\n",
    "        \n",
    "    \n",
    "    heapq.heapify(tmpdis)\n",
    "    cnt_remove= cnt- bin_num\n",
    "    bin_remove=[]\n",
    "    \n",
    "    root_bin=range(0, cnt)\n",
    "\n",
    "    for i in range(0, cnt_remove):\n",
    "        \n",
    "        tmptuple= heapq.heappop(tmpdis)\n",
    "        \n",
    "        merge_l= tmptuple[1]\n",
    "        merge_r= tmptuple[2]\n",
    "        \n",
    "        \n",
    "        tmproot= merge_l \n",
    "        while root_bin[ tmproot] != tmproot:\n",
    "            tmproot= root_bin[tmproot]\n",
    "        \n",
    "        tmphist[tmproot][1]= max(tmphist[tmproot][1], tmphist[merge_r][1])\n",
    "        tmphist[tmproot][2]= tmphist[tmproot][2] + tmphist[merge_r][2]\n",
    " \n",
    "        root_bin[merge_r]= tmproot    \n",
    "    \n",
    "        bin_remove.append( merge_r )\n",
    "    \n",
    "    bin_remove.sort()\n",
    "    tmpdiff=0\n",
    "    for i in bin_remove:\n",
    "        tmphist.pop(i-tmpdiff)\n",
    "        tmpdiff=tmpdiff+1\n",
    "#       ????\n",
    "#     hist1= copy.deepcopy(tmphist)\n",
    "    return tmphist         \n",
    "\n",
    "\n",
    "def merge_parttion_combiner_hist(nodeToFeatureToValue_1,nodeToFeatureToValue_2 ):\n",
    "# optimization: calculate median and amd for feature-value    \n",
    "    \n",
    "    # new added: sum of Y in a node\n",
    "    nodeToFeatureToValue_1['sumY']= nodeToFeatureToValue_1['sumY']+\\\n",
    "    nodeToFeatureToValue_2['sumY']\n",
    "    \n",
    "    nodeToFeatureToValue_1['count']= nodeToFeatureToValue_1['count']+\\\n",
    "    nodeToFeatureToValue_2['count'] \n",
    "        \n",
    "    nodeToFeatureToValue_1['hist_list']=merge_hist_list(nodeToFeatureToValue_1['hist_list'],\\\n",
    "                                                   nodeToFeatureToValue_2['hist_list'])\n",
    "    \n",
    "    for i in range(0, numFeatures): #feature\n",
    "        for j in nodeToFeatureToValue_1[i].keys(): #feature value\n",
    "            \n",
    "            feature_val=j\n",
    "            \n",
    "            if feature_val in nodeToFeatureToValue_2[i].keys():                  \n",
    "                \n",
    "                nodeToFeatureToValue_1[i][j]['hist_val_list'] = \\\n",
    "                merge_hist_list(nodeToFeatureToValue_1[i][j]['hist_val_list'], \n",
    "                           nodeToFeatureToValue_2[i][j]['hist_val_list'])     \n",
    "                \n",
    "                nodeToFeatureToValue_1[i][j]['count_inFeatureValue']=nodeToFeatureToValue_1[i][j]['count_inFeatureValue']+\\\n",
    "                nodeToFeatureToValue_2[i][j]['count_inFeatureValue']\n",
    "                \n",
    "    for i in range(0, numFeatures):\n",
    "        for j in nodeToFeatureToValue_2[i].keys():\n",
    "\n",
    "            feature_val=j\n",
    "            \n",
    "            if feature_val not in nodeToFeatureToValue_1[i].keys():\n",
    "                nodeToFeatureToValue_1[i].update({feature_val: {} })\n",
    "                nodeToFeatureToValue_1[i][feature_val]= copy.deepcopy(nodeToFeatureToValue_2[i][feature_val])\n",
    "                              \n",
    "    return  nodeToFeatureToValue_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23.5, 11.375)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17.0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# debug for absolute median deviation \n",
    "tmphis=[ [1,1,1] , [15,15,1],  [32,32,1],[38,38,1]  ]\n",
    "# print type(tmphis)\n",
    "# sorted_his= sorted(tmphis.items(), key= lambda line: line[0])\n",
    "\n",
    "print  LAD_hist_list(tmphis, 4)\n",
    "\n",
    "floor(17.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split decision  [on the local side]\n",
    "\n",
    "#MAD: median absolute median in a histogram   \n",
    "# def MAD_hist(hist, cnt): \n",
    "\n",
    "#     sorted_his= sorted(hist.items(), key= lambda line: line[0])\n",
    "#     num_bins= len(hist)\n",
    "    \n",
    "#     cntByBounds=0\n",
    "#     curCnt=0 \n",
    "#     midCnt= cnt/2.0\n",
    "    \n",
    "#     mid_bin_idx=0\n",
    "#     flag=0\n",
    "    \n",
    "#     meanAbsDevi=0\n",
    "#     median=0\n",
    "    \n",
    "#     cntSum_beforeMid=0\n",
    "    \n",
    "#     for i in range(0, num_bins): \n",
    "        \n",
    "#         if flag==0:\n",
    "#             tmp_curCnt=curCnt+ sorted_his[i][1][2]\n",
    "#             tmp_cntByBounds =cntByBounds - \\\n",
    "#         (sorted_his[i][1][0]+sorted_his[i][1][1])/2.0*sorted_his[i][1][2]\n",
    "        \n",
    "#             if tmp_curCnt >= midCnt:\n",
    "#                 flag=1       \n",
    "                \n",
    "#                 mid_bin_idx= i\n",
    "                \n",
    "#                 cntSum_beforeMid = curCnt\n",
    "#             else:    \n",
    "#                 curCnt=tmp_curCnt\n",
    "#                 cntByBounds=tmp_cntByBounds\n",
    "#         else:\n",
    "#             curCnt= curCnt - sorted_his[i][1][2]\n",
    "#             cntByBounds= cntByBounds + \\\n",
    "#             (sorted_his[i][1][0]+sorted_his[i][1][1])/2.0*sorted_his[i][1][2]\n",
    "    \n",
    "#     if cnt == 1:\n",
    "#         for tmpkey in hist.keys():\n",
    "#             return (hist[tmpkey][0],0)\n",
    "#     elif cnt ==0:\n",
    "#         return (0,0)\n",
    "#     else:        \n",
    "\n",
    "#         sample_inMedBin = midCnt - cntSum_beforeMid     \n",
    "        \n",
    "#         if sorted_his[mid_bin_idx][1][2] ==1:\n",
    "#             median = sorted_his[mid_bin_idx][1][0]\n",
    "#         elif sorted_his[mid_bin_idx][1][2] ==2:\n",
    "#             median= (sorted_his[mid_bin_idx][1][0] +sorted_his[mid_bin_idx][1][1])*1.0/2.0\n",
    "#         else:\n",
    "#             median= sorted_his[mid_bin_idx][1][0]*1.0 + \\\n",
    "#         (1.0*sorted_his[mid_bin_idx][1][1]-1.0*sorted_his[mid_bin_idx][1][0])/(sorted_his[mid_bin_idx][1][2]+1)*sample_inMedBin\n",
    "        \n",
    "#         meanAbsDevi =  cntByBounds + curCnt*median \n",
    "        \n",
    "        \n",
    "        \n",
    "#         if sorted_his[mid_bin_idx][1][2] ==1:\n",
    "#             return (median, meanAbsDevi*1.0/cnt)\n",
    "        \n",
    "#         sample_val= sorted_his[ mid_bin_idx][1][0]*1.0 \n",
    "#         sample_interval=1.0*(sorted_his[mid_bin_idx][1][1]-sorted_his[mid_bin_idx][1][0])/(sorted_his[mid_bin_idx][1][2]\\\n",
    "#                                                                           -1.0)\n",
    "# #         print meanAbsDevi     \n",
    "    \n",
    "#         for i in range(0, sorted_his[mid_bin_idx][1][2]):\n",
    "#             sample_val= sample_val + sample_interval*i\n",
    "#             meanAbsDevi=meanAbsDevi+ abs( sample_val-median)\n",
    "        \n",
    "#         return (median, meanAbsDevi*1.0/cnt)\n",
    "    \n",
    "def MAD_LAD_hist_list(hist, cnt): \n",
    "\n",
    "    num_bins= len(hist)\n",
    "    \n",
    "    cntByBounds=0\n",
    "    curCnt=0 \n",
    "    midCnt= cnt/2.0\n",
    "    \n",
    "    mid_bin_idx=0\n",
    "    flag=0\n",
    "    \n",
    "    meanAbsDevi=0\n",
    "    median=0\n",
    "    \n",
    "    cntSum_beforeMid=0\n",
    "    \n",
    "    for i in range(0, num_bins): \n",
    "        \n",
    "        if flag==0:\n",
    "            tmp_curCnt=curCnt+ hist[i][2]\n",
    "            tmp_cntByBounds =cntByBounds - (hist[i][0]+hist[i][1])/2.0*hist[i][2]\n",
    "        \n",
    "            if tmp_curCnt >= midCnt:\n",
    "                flag=1       \n",
    "                \n",
    "                mid_bin_idx= i\n",
    "                \n",
    "                cntSum_beforeMid = curCnt\n",
    "            else:    \n",
    "                curCnt=tmp_curCnt\n",
    "                cntByBounds=tmp_cntByBounds\n",
    "        else:\n",
    "            curCnt= curCnt - hist[i][2]\n",
    "            cntByBounds= cntByBounds + \\\n",
    "            ( hist[i][0] + hist[i][1])/2.0* hist[i][2]\n",
    "    \n",
    "    if cnt == 1:\n",
    "        return (hist[0],0)\n",
    "    elif cnt ==0:\n",
    "        return (0,0)\n",
    "    else:        \n",
    "\n",
    "        sample_inMedBin = midCnt - cntSum_beforeMid    \n",
    "        \n",
    "        if hist[mid_bin_idx][2] ==1:\n",
    "            median = hist[mid_bin_idx][0]\n",
    "        elif hist[mid_bin_idx][2] ==2:\n",
    "            median= (hist[mid_bin_idx][0] + hist[mid_bin_idx][1])*1.0/2.0\n",
    "        else:\n",
    "            median= hist[mid_bin_idx][0]*1.0 + \\\n",
    "        (1.0*hist[mid_bin_idx][1]-1.0*hist[mid_bin_idx][0])/(hist[mid_bin_idx][2]+1)*sample_inMedBin\n",
    "         \n",
    "        meanAbsDevi =  cntByBounds + curCnt*median \n",
    "        \n",
    "        \n",
    "        if hist[mid_bin_idx][2] ==1:\n",
    "            return (median, meanAbsDevi*1.0/cnt)\n",
    "        \n",
    "        sample_val= hist[ mid_bin_idx][0]*1.0 \n",
    "        sample_interval=1.0*(hist[mid_bin_idx][1]-hist[mid_bin_idx][0])/(hist[mid_bin_idx][2]\\\n",
    "                                                                          -1.0)\n",
    "#         print meanAbsDevi     \n",
    "    \n",
    "        for i in range(0, hist[mid_bin_idx][2]):\n",
    "            sample_val= sample_val + sample_interval*i\n",
    "            meanAbsDevi=meanAbsDevi+ abs( sample_val-median)\n",
    "        \n",
    "        return (median, meanAbsDevi*1.0/cnt)\n",
    "\n",
    "    \n",
    "    \n",
    "def LAD_hist_list_overlapping(hist, cnt, bin_idx, sampleCnt ):\n",
    "    \n",
    "    tmpl= hist[bin_idx+1][0]\n",
    "    tmpr= hist[bin_idx][1]\n",
    "\n",
    "    tmpCnt= (hist[bin_idx][1] - tmpl)*1.0/( hist[bin_idx][1]-hist[bin_idx][0] )* hist[bin_idx][2]\n",
    "    tmpCnt= tmpCnt+(hist[bin_idx][1]-tmpl)*1.0/( hist[bin_idx+1][1]-hist[bin_idx+1][0] )* hist[bin_idx+1][2]\n",
    "    \n",
    "    median= hist[mid_bin_idx][0]*1.0 + \\\n",
    "        (1.0*hist[mid_bin_idx][1]-1.0*hist[mid_bin_idx][0])/(hist[mid_bin_idx][2]-1)*(sample_inMedBin-1)\n",
    "     \n",
    "    \n",
    "def LAD_hist_list(hist, cnt): \n",
    "    \n",
    "    #debug\n",
    "#     c1=len(hist)\n",
    "#     for i in range(0,c1-1):\n",
    "#         if hist[i][0]> hist[i+1][0]:\n",
    "#             print '(( individual histogram problem  ))',i,hist[i][0], hist[i+1][0]\n",
    "#         if hist[i][1]> hist[i+1][1]:\n",
    "#             print '[] individual histogram problem  []',i,hist[i][1], hist[i+1][1]\n",
    "    \n",
    "    num_bins= len(hist)\n",
    "    \n",
    "    cntByBounds=0\n",
    "    curCnt=0 \n",
    "    \n",
    "    if cnt%2 ==0 :\n",
    "        midCnt= cnt/2.0+1\n",
    "    else:\n",
    "        midCnt= ceil(cnt/2.0)\n",
    "    \n",
    "    mid_bin_idx=0\n",
    "    flag=0\n",
    "    \n",
    "    meanAbsDevi=0\n",
    "    median=0\n",
    "    \n",
    "    cntSum_beforeMid=0\n",
    "    \n",
    "    for i in range(0, num_bins): \n",
    "        if flag==0:\n",
    "            tmp_curCnt=curCnt+ hist[i][2]\n",
    "            tmp_cntByBounds =cntByBounds - (hist[i][0]+hist[i][1])/2.0*hist[i][2]\n",
    "        \n",
    "            if tmp_curCnt >= midCnt:\n",
    "                flag=1       \n",
    "                mid_bin_idx= i\n",
    "                cntSum_beforeMid = curCnt\n",
    "            else:    \n",
    "                curCnt=tmp_curCnt\n",
    "                cntByBounds=tmp_cntByBounds\n",
    "        else:\n",
    "            curCnt= curCnt - hist[i][2]\n",
    "            cntByBounds= cntByBounds + ( hist[i][0] + hist[i][1])/2.0* hist[i][2]\n",
    "    \n",
    "    if cnt == 1:\n",
    "        return (hist[0][0],0)\n",
    "    elif cnt ==0:\n",
    "        return (0,0)\n",
    "    else:        \n",
    "        \n",
    "        \n",
    "#         debug\n",
    "#         if mid_bin_idx+1<num_bins and hist[mid_bin_idx+1][0]< hist[mid_bin_idx][1]:\n",
    "#            print 'debug boundary intersects' \n",
    "        \n",
    "        \n",
    "        sample_inMedBin =  midCnt - cntSum_beforeMid     \n",
    "        \n",
    "        if hist[mid_bin_idx][2] ==1:\n",
    "            median = hist[mid_bin_idx][0]\n",
    "        elif hist[mid_bin_idx][2] ==2:\n",
    "            median= (hist[mid_bin_idx][0] + hist[mid_bin_idx][1])*1.0/2.0\n",
    "        else:\n",
    "            median= hist[mid_bin_idx][0]*1.0 + \\\n",
    "        (1.0*hist[mid_bin_idx][1]-1.0*hist[mid_bin_idx][0])/(hist[mid_bin_idx][2]-1)*(sample_inMedBin-1)\n",
    "        \n",
    "        if cnt%2 ==0 :\n",
    "            if sample_inMedBin == 1:\n",
    "                median = (hist[mid_bin_idx-1][1] + median)/2.0\n",
    "            else:\n",
    "                median_offset = hist[mid_bin_idx][0]*1.0 + \\\n",
    "        (1.0*hist[mid_bin_idx][1]-1.0*hist[mid_bin_idx][0])/(hist[mid_bin_idx][2]-1)*(sample_inMedBin-2)\n",
    "                median = (median + median_offset)/2.0\n",
    "    \n",
    "        meanAbsDevi =  cntByBounds + curCnt*median \n",
    "        \n",
    "        if hist[mid_bin_idx][2] ==1:\n",
    "            return (median, meanAbsDevi*1.0/cnt)\n",
    "        \n",
    "        sample_val= hist[ mid_bin_idx][0]*1.0\n",
    "        sample_interval=1.0*(hist[mid_bin_idx][1]-hist[mid_bin_idx][0])/(hist[mid_bin_idx][2]-1.0)\n",
    "        \n",
    "        for i in range(0, hist[mid_bin_idx][2]):\n",
    "            sample_val= sample_val + sample_interval*i\n",
    "            meanAbsDevi=meanAbsDevi+ abs( sample_val-median)\n",
    "        \n",
    "        return (median, meanAbsDevi*1.0/cnt)\n",
    "\n",
    "def local_merge_hist_list_no_overlapping(hist1, hist2):\n",
    "\n",
    "    tmphist=[]\n",
    "    p1=0\n",
    "    p2=0\n",
    "    c1=len(hist1)\n",
    "    c2=len(hist2)\n",
    "    cnt=0\n",
    "    \n",
    "    #debug\n",
    "#     for i in range(0,c1-1):\n",
    "#         if hist1[i][0]> hist1[i+1][0]:\n",
    "#             print '(( individual histogram problem  ))',i,hist1[i][0], hist1[i+1][0]\n",
    "#         if hist1[i][1]> hist1[i+1][1]:\n",
    "#             print '[] individual histogram problem  []',i,hist1[i][1], hist1[i+1][1]\n",
    "        \n",
    "#     for i in range(0,c2-1):\n",
    "#         if hist2[i][0]> hist2[i+1][0]:\n",
    "#             print '(( individual histogram problem  ))', i,hist2[i][0], hist2[i+1][0]\n",
    "#         if hist2[i][1]> hist2[i+1][1]:\n",
    "#             print '[] individual histogram problem  []', i,hist2[i][1], hist2[i+1][1]\n",
    "#     print 'for debug:',c1,c2\n",
    "        \n",
    " \n",
    "    last_choose=0\n",
    "    \n",
    "    if p1<c1 and p2<c2 and hist1[p1][0] < hist2[p2][0]:\n",
    "        tmphist.append( [0,0,0] )\n",
    "        tmphist[cnt][0]= hist1[p1][0]\n",
    "        tmphist[cnt][1]= hist1[p1][1]\n",
    "        tmphist[cnt][2]= hist1[p1][2]\n",
    "        p1=p1+1\n",
    "        cnt=cnt+1\n",
    "        last_choose=1\n",
    "        \n",
    "    elif p1<c1 and p2<c2 and hist1[p1][0] >= hist2[p2][0] :\n",
    "        tmphist.append( [0,0,0] )\n",
    "        tmphist[cnt][0]= hist2[p2][0]\n",
    "        tmphist[cnt][1]= hist2[p2][1]\n",
    "        tmphist[cnt][2]= hist2[p2][2]\n",
    "        p2=p2+1\n",
    "        cnt=cnt+1\n",
    "        last_choose=2\n",
    "    \n",
    "    while p1<c1 and p2<c2:\n",
    "          \n",
    "        while p2<c2 and p1<c1:\n",
    "            if last_choose==1 and hist2[p2][0] < tmphist[cnt-1][1]:\n",
    "                    tmphist[cnt-1][1]=max(hist2[p2][1] , tmphist[cnt-1][1])\n",
    "                    tmphist[cnt-1][2] = hist2[p2][2] + tmphist[cnt-1][2]\n",
    "                    p2=p2+1\n",
    "                    last_choose=2\n",
    "            elif last_choose==2 and hist1[p1][0] < tmphist[cnt-1][1]:\n",
    "                    tmphist[cnt-1][1]=max(hist1[p1][1] , tmphist[cnt-1][1])\n",
    "                    tmphist[cnt-1][2] = hist1[p1][2] + tmphist[cnt-1][2]\n",
    "                    p1=p1+1\n",
    "                    last_choose=1\n",
    "            else:\n",
    "                break\n",
    "            \n",
    "        \n",
    "        if p1<c1 and p2<c2 and hist1[p1][0] < hist2[p2][0]:\n",
    "            tmphist.append( [0,0,0] )\n",
    "            tmphist[cnt][0]= hist1[p1][0]\n",
    "            tmphist[cnt][1]= hist1[p1][1]\n",
    "            tmphist[cnt][2]= hist1[p1][2]\n",
    "            p1=p1+1\n",
    "            cnt=cnt+1  \n",
    "            last_choose =1\n",
    "                \n",
    "        elif p1<c1 and p2<c2 and hist1[p1][0] >= hist2[p2][0]:\n",
    "            tmphist.append( [0,0,0] )\n",
    "            tmphist[cnt][0]= hist2[p2][0]\n",
    "            tmphist[cnt][1]= hist2[p2][1]\n",
    "            tmphist[cnt][2]= hist2[p2][2]\n",
    "            p2=p2+1\n",
    "            cnt=cnt+1\n",
    "            last_choose =2\n",
    "            \n",
    "            \n",
    "    while p2<c2 and p1<c1:\n",
    "        if last_choose==1 and hist2[p2][0] < tmphist[cnt-1][1]:\n",
    "            tmphist[cnt-1][1]=max(hist2[p2][1] , tmphist[cnt-1][1])\n",
    "            tmphist[cnt-1][2] = hist2[p2][2] + tmphist[cnt-1][2]\n",
    "            p2=p2+1\n",
    "            last_choose=2\n",
    "        elif last_choose==2 and hist1[p1][0] < tmphist[cnt-1][1]:\n",
    "            tmphist[cnt-1][1]=max(hist1[p1][1] , tmphist[cnt-1][1])\n",
    "            tmphist[cnt-1][2] = hist1[p1][2] + tmphist[cnt-1][2]\n",
    "            p1=p1+1\n",
    "            last_choose=1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    while p1<c1:\n",
    "        \n",
    "        tmphist.append( [0,0,0] )\n",
    "        tmphist[cnt][0]= hist1[p1][0]\n",
    "        tmphist[cnt][1]= hist1[p1][1]\n",
    "        tmphist[cnt][2]= hist1[p1][2]\n",
    "        cnt=cnt+1\n",
    "        p1=p1+1\n",
    "        \n",
    "    while p2<c2:\n",
    "        \n",
    "        tmphist.append( [0,0,0] )\n",
    "        tmphist[cnt][0]= hist2[p2][0]\n",
    "        tmphist[cnt][1]= hist2[p2][1]\n",
    "        tmphist[cnt][2]= hist2[p2][2]\n",
    "        cnt=cnt+1\n",
    "        p2=p2+1\n",
    "    \n",
    "    return tmphist\n",
    "def local_merge_hist_list(hist1, hist2):\n",
    "\n",
    "    tmphist=[]\n",
    "    p1=0\n",
    "    p2=0\n",
    "    c1=len(hist1)\n",
    "    c2=len(hist2)\n",
    "    cnt=0\n",
    "    \n",
    "    #debug\n",
    "#     for i in range(0,c1-1):\n",
    "#         if hist1[i][0]> hist1[i+1][0]:\n",
    "#             print '(( individual histogram problem  ))',i,hist1[i][0], hist1[i+1][0]\n",
    "#         if hist1[i][1]> hist1[i+1][1]:\n",
    "#             print '[] individual histogram problem  []',i,hist1[i][1], hist1[i+1][1]\n",
    "        \n",
    "#     for i in range(0,c2-1):\n",
    "#         if hist2[i][0]> hist2[i+1][0]:\n",
    "#             print '(( individual histogram problem  ))', i,hist2[i][0], hist2[i+1][0]\n",
    "#         if hist2[i][1]> hist2[i+1][1]:\n",
    "#             print '[] individual histogram problem  []', i,hist2[i][1], hist2[i+1][1]\n",
    "#     print 'for debug:',c1,c2\n",
    "        \n",
    " \n",
    "    last_choose=0\n",
    "    \n",
    "    if p1<c1 and p2<c2 and hist1[p1][0] < hist2[p2][0]:\n",
    "        tmphist.append( [0,0,0] )\n",
    "        tmphist[cnt][0]= hist1[p1][0]\n",
    "        tmphist[cnt][1]= hist1[p1][1]\n",
    "        tmphist[cnt][2]= hist1[p1][2]\n",
    "        p1=p1+1\n",
    "        cnt=cnt+1\n",
    "        last_choose=1\n",
    "        \n",
    "    elif p1<c1 and p2<c2 and hist1[p1][0] >= hist2[p2][0] :\n",
    "        tmphist.append( [0,0,0] )\n",
    "        tmphist[cnt][0]= hist2[p2][0]\n",
    "        tmphist[cnt][1]= hist2[p2][1]\n",
    "        tmphist[cnt][2]= hist2[p2][2]\n",
    "        p2=p2+1\n",
    "        cnt=cnt+1\n",
    "        last_choose=2\n",
    "    \n",
    "    while p1<c1 and p2<c2:\n",
    "        \n",
    "#         if hist2[p2][1] <= tmphist[cnt-1][1]:\n",
    "#             tmphist[cnt-1][2]= tmphist[cnt-1][2] + hist2[p2][2]\n",
    "#             p2=p2+1\n",
    "        \n",
    "#         if hist1[p1][1] <= tmphist[cnt-1][1]:\n",
    "#             tmphist[cnt-1][2]= tmphist[cnt-1][2] + hist1[p1][2]\n",
    "#             p1=p1+1\n",
    "        \n",
    "#         if last_choose ==1:\n",
    "        while p2<c2 and hist2[p2][1] <= tmphist[cnt-1][1]:\n",
    "#                 tmphist[cnt-1][1]= tmphist[cnt-1][1], hist2[p\n",
    "            tmphist[cnt-1][2]= tmphist[cnt-1][2] + hist2[p2][2]\n",
    "            p2=p2+1\n",
    "#         elif last_choose ==2:        \n",
    "        while p1<c1 and hist1[p1][1] <= tmphist[cnt-1][1]:\n",
    "#                 tmphist[cnt-1][1]= max(tmphist[cnt-1][1],  hist1[p1][1])\n",
    "            tmphist[cnt-1][2]= tmphist[cnt-1][2] + hist1[p1][2]\n",
    "            p1=p1+1\n",
    "        \n",
    "        if p1<c1 and p2<c2 and hist1[p1][0] < hist2[p2][0]:\n",
    "            tmphist.append( [0,0,0] )\n",
    "            tmphist[cnt][0]= hist1[p1][0]\n",
    "            tmphist[cnt][1]= hist1[p1][1]\n",
    "            tmphist[cnt][2]= hist1[p1][2]\n",
    "            p1=p1+1\n",
    "            cnt=cnt+1  \n",
    "            last_choose =1\n",
    "                \n",
    "        elif p1<c1 and p2<c2 and hist1[p1][0] >= hist2[p2][0]:\n",
    "            tmphist.append( [0,0,0] )\n",
    "            tmphist[cnt][0]= hist2[p2][0]\n",
    "            tmphist[cnt][1]= hist2[p2][1]\n",
    "            tmphist[cnt][2]= hist2[p2][2]\n",
    "            p2=p2+1\n",
    "            cnt=cnt+1\n",
    "            last_choose =2\n",
    "                       \n",
    "    if last_choose ==1:\n",
    "        while p2<c2 and hist2[p2][1] <= tmphist[cnt-1][1]:\n",
    "            tmphist[cnt-1][2]= tmphist[cnt-1][2] + hist2[p2][2]\n",
    "            p2=p2+1\n",
    "    elif last_choose ==2:\n",
    "        while p1<c1 and hist1[p1][1] <= tmphist[cnt-1][1]:\n",
    "            tmphist[cnt-1][2]= tmphist[cnt-1][2] + hist1[p1][2]\n",
    "            p1=p1+1\n",
    "    \n",
    "    \n",
    "    while p1<c1:\n",
    "        \n",
    "        tmphist.append( [0,0,0] )\n",
    "        tmphist[cnt][0]= hist1[p1][0]\n",
    "        tmphist[cnt][1]= hist1[p1][1]\n",
    "        tmphist[cnt][2]= hist1[p1][2]\n",
    "        cnt=cnt+1\n",
    "        p1=p1+1\n",
    "        \n",
    "    while p2<c2:\n",
    "        \n",
    "        tmphist.append( [0,0,0] )\n",
    "        tmphist[cnt][0]= hist2[p2][0]\n",
    "        tmphist[cnt][1]= hist2[p2][1]\n",
    "        tmphist[cnt][2]= hist2[p2][2]\n",
    "        cnt=cnt+1\n",
    "        p2=p2+1\n",
    "    \n",
    "    return tmphist\n",
    "\n",
    "\n",
    "def split_onOneFeature_hist( values_hist,node_data_cnt, node_hist):\n",
    "    \n",
    "    left_count=0\n",
    "    right_count= node_data_cnt\n",
    "    \n",
    "#     if len(values_hist)<=1:\n",
    "#         return [-1,-1,-1,-1]\n",
    "\n",
    "    #this is a bit-record variable\n",
    "    leftSplit_valueSet = 0 \n",
    "    leftSplit_count = 0\n",
    "    leftSplit_hist= []\n",
    "    rightSplit_hist=[]\n",
    "        \n",
    "    bestLeft=(-1,-1)\n",
    "    bestRight=(-1,-1)  \n",
    "    bestSplitMetric =0 \n",
    "    \n",
    "    \n",
    "    #sort feature values according to median \n",
    "    sorted_value_map=[]\n",
    "    \n",
    "    for i in values_hist.keys():\n",
    "        currentVal=i\n",
    "        currentVal_hist_count= values_hist[i]['count_inFeatureValue']\n",
    "        currentVal_hist= values_hist[i]['hist_val_list']\n",
    "        \n",
    "        #debug\n",
    "        if len(currentVal_hist) > bin_num:\n",
    "            print '!!! histogrm bin number error !!!'\n",
    "        \n",
    "        sorted_value_map.append((currentVal,LAD_hist_list(currentVal_hist,currentVal_hist_count)))\n",
    "            \n",
    "    sorted_value_map=sorted(sorted_value_map, key= lambda val: val[1][0] )\n",
    "    values_cnt= len(sorted_value_map)\n",
    "    \n",
    "    #--------- initialize the value set ----------------------\n",
    "    \n",
    "    # check the number of feature values\n",
    "    if values_cnt <= 1:\n",
    "        return [-1,-1,-1,-1]\n",
    "    \n",
    "    current_feature_value = sorted_value_map[0][0] \n",
    "    currentVal_count= values_hist[current_feature_value]['count_inFeatureValue']\n",
    "    currentVal_hist= values_hist[current_feature_value]['hist_val_list']\n",
    "    \n",
    "    #debug\n",
    "    if len(currentVal_hist) > bin_num:\n",
    "        print '!!! histogrm bin number error !!!'\n",
    "\n",
    "    leftSplit_count = leftSplit_count+currentVal_count     \n",
    "    leftSplit_hist=local_merge_hist_list_no_overlapping( leftSplit_hist, currentVal_hist)\n",
    "    \n",
    "    # histograms for the right values\n",
    "    rightSplit_hist={}\n",
    "    tmp_value=0\n",
    "    for j in range(0, values_cnt-1 ):\n",
    "        tmp_value = sorted_value_map[values_cnt-1-j][0]\n",
    "        rightSplit_hist= local_merge_hist_list_no_overlapping( rightSplit_hist, values_hist[tmp_value]['hist_val_list'])      \n",
    "            \n",
    "    left = LAD_hist_list( leftSplit_hist, leftSplit_count)\n",
    "    right= LAD_hist_list( rightSplit_hist, (node_data_cnt - leftSplit_count)) \n",
    "        \n",
    "    #debug\n",
    "#     tmpcnt=0\n",
    "#     for ele in leftSplit_hist:\n",
    "#         tmpcnt= tmpcnt+ ele[2]\n",
    "#     if tmpcnt!= leftSplit_count:\n",
    "#         print '++++ problem in left split histogram', tmpcnt, leftSplit_count\n",
    "#     tmpcnt=0\n",
    "#     for ele in rightSplit_hist:\n",
    "#         tmpcnt= tmpcnt+ ele[2]\n",
    "#     if tmpcnt!= (node_data_cnt - leftSplit_count):\n",
    "#         print '???? problem in right split histogram', tmpcnt, (node_data_cnt - leftSplit_count)\n",
    "    \n",
    "    \n",
    "    leftMedian= left[0]\n",
    "    leftMetric= left[1]\n",
    "    rightMedian= right[0]\n",
    "    rightMetric= right[1]\n",
    "        \n",
    "#         debug\n",
    "#     if leftMedian<0 or rightMedian<0 or leftMetric <0 or rightMetric <0:\n",
    "#         print '$$$$ problem in MAD calculation',leftMedian,rightMedian,leftMetric,rightMetric,\\\n",
    "#         leftSplit_count,(node_data_cnt - leftSplit_count),\\\n",
    "#         current_feature_value\n",
    "            \n",
    "    current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "    bestSplitMetric=current_splitMetric     \n",
    "    bestLeft=left\n",
    "    bestRight=right\n",
    "    #bit variable to record which value is chosen for the left child\n",
    "    leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)\n",
    "    \n",
    "\n",
    "    #---------------------------------------------------------------\n",
    "\n",
    "    # scan the sorted feature values\n",
    "    for k in range(1,values_cnt-1):\n",
    "\n",
    "        current_feature_value=sorted_value_map[k][0]\n",
    "        currentVal_count= values_hist[current_feature_value]['count_inFeatureValue']\n",
    "        currentVal_hist= values_hist[current_feature_value]['hist_val_list']\n",
    "        \n",
    "        \n",
    "        #debug\n",
    "        if len(currentVal_hist) > bin_num:\n",
    "            print '!!! histogrm bin number error !!!'\n",
    "        \n",
    "\n",
    "#         histograms for the left values\n",
    "        leftSplit_count = leftSplit_count+currentVal_count     \n",
    "        leftSplit_hist=local_merge_hist_list_no_overlapping( leftSplit_hist, currentVal_hist)\n",
    "    \n",
    "#         histograms for the right values\n",
    "        rightSplit_hist={}\n",
    "        tmp_value=0\n",
    "        for j in range(0, values_cnt-1-k ):\n",
    "            tmp_value = sorted_value_map[values_cnt-1-j][0]\n",
    "            rightSplit_hist= local_merge_hist_list_no_overlapping( rightSplit_hist, values_hist[tmp_value]['hist_val_list'])      \n",
    "            \n",
    "        left = LAD_hist_list( leftSplit_hist, leftSplit_count)\n",
    "        right= LAD_hist_list( rightSplit_hist, (node_data_cnt - leftSplit_count)) \n",
    "        \n",
    "        #debug\n",
    "#         tmpcnt=0\n",
    "#         for ele in leftSplit_hist:\n",
    "#             tmpcnt= tmpcnt+ ele[2]\n",
    "#         if tmpcnt!= leftSplit_count:\n",
    "#             print '++++ problem in left split histogram', tmpcnt, leftSplit_count\n",
    "#         tmpcnt=0\n",
    "#         for ele in rightSplit_hist:\n",
    "#             tmpcnt= tmpcnt+ ele[2]\n",
    "#         if tmpcnt!= (node_data_cnt - leftSplit_count):\n",
    "#             print '???? problem in right split histogram', tmpcnt, (node_data_cnt - leftSplit_count)\n",
    "                 \n",
    "        leftMedian= left[0]\n",
    "        leftMetric= left[1]\n",
    "        rightMedian= right[0]\n",
    "        rightMetric= right[1]\n",
    "        \n",
    "#         #debug\n",
    "        if leftMetric <0 or rightMetric <0:\n",
    "            print '$$$$ problem in MAD calculation',leftMedian,rightMedian,leftMetric,rightMetric,\\\n",
    "            leftSplit_count,(node_data_cnt - leftSplit_count),\\\n",
    "            current_feature_value\n",
    "            \n",
    "            print 'problem histogram:', leftSplit_hist\n",
    "            print 'problem histogram:', rightSplit_hist,\n",
    "            \n",
    "            \n",
    "        current_splitMetric=1.0*leftSplit_count/node_data_cnt*leftMetric + \\\n",
    "                               1.0*(node_data_cnt - leftSplit_count)/node_data_cnt*rightMetric\n",
    "        \n",
    "        if current_splitMetric < bestSplitMetric:\n",
    "            \n",
    "            bestSplitMetric=current_splitMetric\n",
    "            bestLeft=left\n",
    "            bestRight=right\n",
    "            leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)        \n",
    "    \n",
    "    return [leftSplit_valueSet, bestLeft[0], bestRight[0], bestSplitMetric ]\n",
    "\n",
    "\n",
    "def find_bestSplit_hist(local_aggre_nodes, current_NumNodes, nodes_tree, \\\n",
    "                        nodes_tree_test):\n",
    "        \n",
    "    #debug \n",
    "#     tmpnode_cnt=[]\n",
    "#     presplit=[]\n",
    "    \n",
    "    local_node_idx=0\n",
    "    \n",
    "#------------- grow the unbalanced tree------------------------    \n",
    "    cur_node_num= len(nodes_tree)\n",
    "\n",
    "    for i in range(0, current_NumNodes):\n",
    "\n",
    "        #------------- grow the unbalanced tree ---------------------------------- \n",
    "        if cur_node_num >= 3:\n",
    "            parent_node = int(math.ceil(  (i + cur_node_num -2.0)*1.0/ 2 ))\n",
    "            parent_node_esti =nodes_tree_test[2* parent_node +1 ] \n",
    "            tmp_split_feature= nodes_tree[parent_node][0]\n",
    "            \n",
    "            if tmp_split_feature == -1:\n",
    "                nodes_tree.append(  (-1,  -1  )  )\n",
    "                nodes_tree_test.append( parent_node_esti  )\n",
    "                nodes_tree_test.append( parent_node_esti )\n",
    "                continue\n",
    "            else:\n",
    "                while i!= local_aggre_nodes[local_node_idx][0]:\n",
    "                    local_node_idx=local_node_idx+1\n",
    "                    \n",
    "#                 print '!!!  possible problem !!!!'\n",
    "        #-------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "#         if local_aggre_nodes[ current_idx_nodes ][0] != i:\n",
    "#             nodes_layer.append( (-1,-1) )\n",
    "#             nodes_tree.append( (-1,-1) )\n",
    "#             continue\n",
    " \n",
    "        node_count= local_aggre_nodes[local_node_idx][1]['count'] \n",
    "        node_hist=  local_aggre_nodes[local_node_idx][1]['hist_list']           \n",
    "        \n",
    "#         (median, meanAbsDevi)\n",
    "        \n",
    "#      ????\n",
    "        tmp_metric= LAD_hist_list( node_hist, node_count)\n",
    "        best_split_sofar= tmp_metric[1]\n",
    "        best_split=(-1,-1,-1,-1)\n",
    "        \n",
    "        #debug\n",
    "#         tmpnode_cnt.append(node_count)\n",
    "#         presplit.append( best_splitMetric_feature[1]/100000 )\n",
    "         \n",
    "        best_split_feature=-1\n",
    "        best_split_featureValueSet=-1\n",
    "\n",
    "        for j in range(0,numFeatures):\n",
    "                     \n",
    "            statisticToValues = local_aggre_nodes[local_node_idx][1][j]\n",
    "\n",
    "            split=split_onOneFeature_hist(statisticToValues,node_count, node_hist)\n",
    "            \n",
    "            #debug\n",
    "#             print split[1],split[2]\n",
    "#             [leftSplit_valueSet, bestLeft[0],bestRight[0], bestSplitMetric ]\n",
    "\n",
    "            #debug\n",
    "#             if len(nodes_tree) ==0:\n",
    "#                 print 'bug check: ', split\n",
    "    \n",
    "            if  split[0]!=-1 and  split[3] < best_split_sofar:\n",
    "                best_split=split\n",
    "                best_split_sofar=split[3]\n",
    "                best_split_feature = j\n",
    "                best_split_featureValueSet= split[0]\n",
    "         \n",
    "        \n",
    "        local_node_idx=local_node_idx+1\n",
    "        \n",
    "        #------------- grow the unbalanced tree ---------------------------------- \n",
    "        if best_split_feature == -1:\n",
    "            nodes_tree.append(  (-1,  -1  )  )\n",
    "            nodes_tree_test.append( tmp_metric[0]  )\n",
    "            nodes_tree_test.append( tmp_metric[0] )\n",
    "            #debug\n",
    "#             print 'chosen:', tmp_metric[0],tmp_metric[0]\n",
    "            continue\n",
    "        #-------------------------------------------------------------------------        \n",
    "\n",
    "        # split on each node\n",
    "        nodes_tree.append(  (best_split_feature,  best_split_featureValueSet  )      )\n",
    "        \n",
    "        # tree for predicting\n",
    "        nodes_tree_test.append(best_split[1])\n",
    "        nodes_tree_test.append(best_split[2])\n",
    "    \n",
    "#     \n",
    "    if local_node_idx != len(local_aggre_nodes):\n",
    "        print '!!!! debug !!!!: not all local nodes are processed ', local_node_idx,len(local_aggre_nodes)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#statistic_infor_check for distributed information collection\n",
    "\n",
    "def statistic_infor_check(local_aggre_nodes):\n",
    "    numNode = len(local_aggre_nodes)\n",
    "#     print numNode\n",
    "    tmpsum=0\n",
    "    for i in range(0, numNode):\n",
    "        tmpsum=tmpsum+local_aggre_nodes[i][1]['count']\n",
    "\n",
    "    if tmpsum != 200574:\n",
    "        print '########### problem in node count ################'\n",
    "\n",
    "    tmpsum=0\n",
    "    for i in range(0, numNode):\n",
    "        #check node hist\n",
    "        node_count = local_aggre_nodes[i][1]['count']\n",
    "            \n",
    "        #check node hist_list\n",
    "        tmpsum=0\n",
    "        for j in local_aggre_nodes[i][1]['hist_list']:\n",
    "            tmpsum = tmpsum + j[2]\n",
    "        if tmpsum != node_count:\n",
    "            print '$$$$ problem in the histogram_list of node', tmpsum, node_count\n",
    "\n",
    "            \n",
    "        #check feature values\n",
    "        for j in range(0, numFeatures):\n",
    "\n",
    "            tmpsum=0\n",
    "            \n",
    "            for k in local_aggre_nodes[i][1][j].keys():    \n",
    "               \n",
    "                value_cnt = local_aggre_nodes[i][1][j][k]['count_inFeatureValue']\n",
    "                value_hist= local_aggre_nodes[i][1][j][k]['hist_val_list']\n",
    "                tmpsum= tmpsum+value_cnt\n",
    "                if len(value_hist) > bin_num:\n",
    "                    print '???? bin num wrong', len(value_hist)\n",
    "                    \n",
    "                #check feature-value hist_list\n",
    "                tmpsum1=0\n",
    "                for m in value_hist:\n",
    "                    tmpsum1= tmpsum1 + m[2]\n",
    "                if tmpsum1 != value_cnt:\n",
    "                    print '++++ problem in feature-value histogram_list', tmpsum1, value_cnt\n",
    "                 \n",
    "            if tmpsum != node_count:\n",
    "                print '---- problem !!!! in feature:',j, tmpsum, node_count  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1658253 332190\n",
      "1658253\n",
      "312746\n"
     ]
    }
   ],
   "source": [
    "# training and testing process: data preparation\n",
    "\n",
    "# with outliers\n",
    "# tmpdta = dta.takeSample(False, 1000000, 12243)\n",
    "# dta_train_all = sc.parallelize(tmpdta).cache().sample(False, .1, 12345)\n",
    "# dta_test_all = sc.parallelize(tmpdta).cache().sample(False, .1, 43243)\n",
    "\n",
    "\n",
    "dta_train_all = dta.cache().sample(False, .5, 12345)\n",
    "dta_test_all = dta.cache().sample(False, .1, 43243)\n",
    "\n",
    "\n",
    "# configurate extraction\n",
    "# print 'number of feature-value combinations:',len(dta_train_all.map(lambda line:(line[1],line[2],line[3],line[4])).distinct().collect())\n",
    "print dta_train_all.count(), dta_test_all.count()\n",
    "\n",
    "\n",
    "dta_train = dta_train_all\n",
    "# .filter(lambda line: line[0]<100000 )\n",
    "dta_test = dta_test_all.filter(lambda line: line[0]<1000 )\n",
    "\n",
    "print dta_train.count()\n",
    "print dta_test.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "debug boundary intersects\n",
      "time consumption ratio 10 : 109.234496832 58.719725132\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-45ab9eb7d09f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     63\u001b[0m                               merge_parttion_combiner_hist(statis_partition_1,statis_partition_2 ))\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mlocal_aggre_nodes\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0maggre_nodes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;31m#debug\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m         \"\"\"\n\u001b[0;32m    756\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 757\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    758\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 536\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m    538\u001b[0m                 self.target_id, self.name)\n",
      "\u001b[1;32m/usr/local/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry)\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_give_back_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    471\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[1;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    432\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m                         \u001b[1;32mwhile\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m                             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m                                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training and testing process: main process\n",
    "\n",
    "#parameters\n",
    "maxdepth= 10\n",
    "numFeatures= 7\n",
    "trimm_ratio = 0.00\n",
    "# tmse (trimmed mse), lad, ma d, mse\n",
    "loss_func= 'tmse'\n",
    "bin_num=700\n",
    "\n",
    "# def tree_test_mapFunc_median(line):\n",
    "#     tmpnode = search_nodeToData(line[1:numFeatures+1], node_split)\n",
    "#     return (leaf_nodes[ tmpnode ]  - line[0])*(leaf_nodes[ tmpnode ]  - line[0])\n",
    "\n",
    "# def tree_test_mapFunc(line):\n",
    "#     tmpnode = search_nodeToData(line[1:numFeatures+1], node_split)\n",
    "#     return (leaf_nodes[ tmpnode ]  - line[0])*(leaf_nodes[ tmpnode ]  - line[0])\n",
    "    \n",
    "# def tree_test( testData_rdd ):\n",
    "#     err_rdd=testData_rdd.map( lambda line: tree_test_mapFunc(line))\n",
    "#     err_sum = err_rdd.reduce( lambda a,b: a+b)\n",
    "#     return err_sum / testData_rdd.count()\n",
    "\n",
    "# variables for the tree structure \n",
    "node_split=[]\n",
    "node_test=[]\n",
    "\n",
    "test_err = []\n",
    "train_err = []\n",
    "run_time=[]\n",
    "\n",
    "tree_history_split=[]\n",
    "tree_history_esti=[]\n",
    "tree_history_leaf=[]\n",
    "tree_history_runtime=[]\n",
    "\n",
    "#ini parameter on the cluster\n",
    "currentNode_split_fromMaster = sc.broadcast([])\n",
    "\n",
    "#prepare feature value set\n",
    "# feature_valueSet = data_featureValues_collect( dta_train )\n",
    "# feature_valueList=[]\n",
    "# for i in range(0, len(feature_valueSet)):\n",
    "#     feature_valueList.append( ( i, list(feature_valueSet[i][1]) ) )\n",
    "\n",
    "#     tune the starting depth\n",
    "for i in range(maxdepth,maxdepth+1):\n",
    "    \n",
    "    start = time.time() \n",
    "    \n",
    "    del node_split[:]\n",
    "    del node_test[:]\n",
    "    currentNode_split_fromMaster = sc.broadcast([])\n",
    "    \n",
    "    for cur_depth in range(0,i):\n",
    "        \n",
    "        start_cluster = time.time() \n",
    "        \n",
    "        current_NumNodes= int( math.pow(2,  cur_depth)) \n",
    "        statis_partitions = dta_train.mapPartitions( partition_combiner_hist )\n",
    "        \n",
    "        aggre_nodes =statis_partitions.reduceByKey(lambda statis_partition_1, statis_partition_2: \n",
    "                              merge_parttion_combiner_hist(statis_partition_1,statis_partition_2 ))\n",
    "        \n",
    "        local_aggre_nodes= aggre_nodes.collect()\n",
    "        \n",
    "        #debug\n",
    "#         statistic_infor_check(local_aggre_nodes)\n",
    "\n",
    "        #debug\n",
    "        print len(local_aggre_nodes) , current_NumNodes\n",
    "        \n",
    "        end_cluster = time.time()\n",
    "        \n",
    "        find_bestSplit_hist( sorted(local_aggre_nodes,key= lambda val:val[0])\\\n",
    "                                    ,current_NumNodes, node_split, node_test)\n",
    "        \n",
    "        end_local= time.time()\n",
    "        \n",
    "        \n",
    "        #debug\n",
    "#         statistic_infor_check(local_aggre_nodes)\n",
    "        \n",
    "        currentNode_split_fromMaster = sc.broadcast(node_split)\n",
    "        print \"time consumption ratio\", i,\":\",  end_cluster-start_cluster, end_local-end_cluster\n",
    "    \n",
    "    \n",
    "    end = time.time()\n",
    "    leaf_nodes = node_test[ len(node_test)-  (int)( math.pow(2,i)): len(node_test)  ]\n",
    "    \n",
    "    tmp_test_err= 0 \n",
    "#     tree_test( dta_test )\n",
    "    tmp_train_err= 0\n",
    "#     tree_test( dta_train )\n",
    "#     test_err.append( tmp_test_err)\n",
    "#     train_err.appeJinfeind( tmp_train_err )\n",
    "\n",
    "    elapsed = end-start\n",
    "    run_time.append(elapsed)\n",
    "\n",
    "    tree_history_split.append(copy.deepcopy(node_split) )\n",
    "    tree_history_esti.append( copy.deepcopy(node_test)  )\n",
    "    tree_history_leaf.append( copy.deepcopy(leaf_nodes)  )\n",
    "    tree_history_runtime.append( copy.deepcopy(run_time)  )\n",
    "        \n",
    "    print \"error at tree height\", i,\":\",  tmp_test_err, tmp_train_err\n",
    "    print \"running time at tree height\", i,\":\",  elapsed,  end_cluster-start\n",
    "    print \"number of leaf nodes at tree height\", i,\":\",  len(leaf_nodes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328.0, 10, 10, 2, 2, 8, 1, 3)\n",
      "1023\n",
      "2046\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "print dta_test.first()\n",
    "err_rdd=dta_test.map(lambda line:search_nodeToData(line[1:numFeatures+1], node_split) ) \n",
    "print len(node_split)\n",
    "print len(node_test)\n",
    "print len(leaf_nodes)\n",
    "\n",
    "# print node_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training and testing process: extract all the feature-value combinations \n",
    "# in the training data set\n",
    "\n",
    "total_featureVal_set=[]\n",
    "for i in range(0, numFeatures):\n",
    "    featureValues = dta_train.map(lambda line: line[1+i]).distinct().collect()\n",
    "    total_featureVal_set.append( featureValues)\n",
    "\n",
    "print total_featureVal_set    \n",
    "    \n",
    "print 'done'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1023\n",
      "1\n",
      "2046\n",
      "1\n",
      "1024\n",
      "[[1088.9344470500946]]\n"
     ]
    }
   ],
   "source": [
    "#---------------- LAD -------------------  data backup\n",
    "lad_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "lad_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "lad_tree_history_leaf =  copy.deepcopy( tree_history_leaf)\n",
    "lad_tree_history_runtime =  copy.deepcopy( tree_history_runtime)\n",
    "\n",
    "# lad_tree_history_split = copy.deepcopy( node_split)\n",
    "# lad_tree_history_esti =  copy.deepcopy( node_test)\n",
    "# lad_tree_history_leaf =  copy.deepcopy( leaf_nodes)\n",
    "# lad_tree_history_runtime =  copy.deepcopy( elapsed)\n",
    "\n",
    "\n",
    "print len(lad_tree_history_split)\n",
    "print len(lad_tree_history_split[0])\n",
    "print len(lad_tree_history_esti)\n",
    "print len(lad_tree_history_esti[0])\n",
    "\n",
    "print len(lad_tree_history_leaf)\n",
    "print len(lad_tree_history_leaf[0])\n",
    "\n",
    "print tree_history_runtime\n",
    "# print lad_tree_history_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# LAD result statistic: level-wise node infor.\n",
    "\n",
    "# lad_tree_history_split = copy.deepcopy( tree_history_split)\n",
    "# lad_tree_history_esti =  copy.deepcopy( tree_history_esti)\n",
    "\n",
    "print 'test error:',test_err\n",
    "print 'training error:',train_err\n",
    "print len(lad_tree_history_split)\n",
    "\n",
    "recog_conf=bfs_tree(lad_tree_history_split[ 0 ], total_featureVal_set, maxdepth-1,  lad_tree_history_esti[0], dta_train )  \n",
    "\n",
    "print len(recog_conf[0])\n",
    "print recog_conf[1]\n",
    "print recog_conf[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(76.02111852585, 37.4890427123), (93.05228825615, 54.8788496765), (6.97792332123, 92.7170374082), (87.9832876272, 71.3106791845), (80.4815428933, 8.29185638119), (77.9332755778, 81.5703974275), (42.1156898056, 69.4028103194), (83.9826375749, 17.6303557067), (32.1034566391, 18.1404172704), (10.0091745604, 79.2345371935)]\n"
     ]
    }
   ],
   "source": [
    "# LAD result statistic : individual test error\n",
    "\n",
    "pre=0            \n",
    "end_bound= pre+ (int)( math.pow(2,maxdepth)) \n",
    "test_leaf_nodes = tree_esti[ pre: end_bound  ]\n",
    "pre= end_bound        \n",
    "test_tree_split= tree_split[ 0:(int)( math.pow(2,i)-1)  ]\n",
    "    \n",
    "#     print len(test_tree_split)\n",
    "#     print len(test_leaf_nodes)\n",
    "\n",
    "print dta_test.map( lambda line: test_error_indiviMapFunc(line)).take(10)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test error at depth 1 : 467294.113528\n",
      "test error at depth 2 : 92001.8209507\n",
      "test error at depth 3 : 30066.62828\n",
      "test error at depth 4 : 37606.6286098\n",
      "test error at depth 5 : 52745.3160839\n",
      "test error at depth 6 : 48944.2660277\n",
      "test error at depth 7 : 52834.6009954\n",
      "test error at depth 8 : 126743.661072\n",
      "test error at depth 9 : 105180.958891\n",
      "test error at depth 10 : 158450.125081\n",
      "[467294.11352820997, 92001.820950687994, 30066.628280033197, 37606.628609800711, 52745.316083868696, 48944.266027665806, 52834.600995356239, 126743.66107203568, 105180.9588912968, 158450.12508092084]\n"
     ]
    }
   ],
   "source": [
    "# LAD result statistic : test error\n",
    "\n",
    "pre=0\n",
    "tree_esti = lad_tree_history_esti[0]\n",
    "tree_split =  lad_tree_history_split[ 0 ]\n",
    "err_depth=[]\n",
    "\n",
    "for i in range(1,maxdepth+1):\n",
    "            \n",
    "    end_bound= pre+ (int)( math.pow(2,i)) \n",
    "    test_leaf_nodes = tree_esti[ pre: end_bound  ]\n",
    "    pre= end_bound        \n",
    "    test_tree_split= tree_split[ 0:(int)( math.pow(2,i)-1)  ]\n",
    "    \n",
    "#     print len(test_tree_split)\n",
    "#     print len(test_leaf_nodes)\n",
    "\n",
    "    err_depth.append(test_error_tree( i, dta_test)  ) \n",
    "    \n",
    "\n",
    "print err_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "test error at depth 1 : 209723.230094\n",
    "test error at depth 2 : 81272.2607548\n",
    "test error at depth 3 : 65770.5823087\n",
    "test error at depth 4 : 52236.7565111\n",
    "test error at depth 5 : 42923.1526205\n",
    "test error at depth 6 : 33614.743367\n",
    "test error at depth 7 : 33243.0853692\n",
    "test error at depth 8 : 183156.136763\n",
    "test error at depth 9 : 45159.4615624\n",
    "test error at depth 10 : 91668.5891443\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bin=200\n",
    "\n",
    "test error at depth 1 : 13269.3523689\n",
    "test error at depth 2 : 6648.07724833\n",
    "test error at depth 3 : 5191.94339212\n",
    "test error at depth 4 : 5262.53792793\n",
    "test error at depth 5 : 3411.40979594\n",
    "test error at depth 6 : 3406.03050674\n",
    "test error at depth 7 : 3404.55134591\n",
    "test error at depth 8 : 3404.49915034\n",
    "test error at depth 9 : 3404.05074894\n",
    "test error at depth 10 : 3407.21308932\n",
    "test error at depth 11 : 3403.34222525\n",
    "test error at depth 12 : 3403.58541554\n",
    "test error at depth 13 : 3396.16384198\n",
    "    \n",
    "#bin 150, syndata_f6v5_6zeros_1percen\n",
    "test error at depth 1 : 5967.85959307\n",
    "test error at depth 2 : 6793.83095448\n",
    "test error at depth 3 : 8090.59117629\n",
    "test error at depth 4 : 5356.04267644\n",
    "test error at depth 5 : 3959.96802199\n",
    "test error at depth 6 : 3961.74647149\n",
    "test error at depth 7 : 3442.88848397\n",
    "test error at depth 8 : 3367.26442943\n",
    "test error at depth 9 : 3296.1163737\n",
    "test error at depth 10 : 3135.91342157   \n",
    "  bin 250  \n",
    "test error at depth 1 : 5071.93614349\n",
    "test error at depth 2 : 9295.06361711\n",
    "test error at depth 3 : 5664.24136607\n",
    "test error at depth 4 : 3932.5539618\n",
    "test error at depth 5 : 4077.58392782\n",
    "test error at depth 6 : 3422.34279944\n",
    "test error at depth 7 : 3387.27343053\n",
    "test error at depth 8 : 3341.40617714\n",
    "test error at depth 9 : 3261.99621439\n",
    "test error at depth 10 : 3109.7913964\n",
    "    \n",
    "    1 1\n",
    "time consumption ratio 10 : 29.0070030689 7.54022502899\n",
    "2 2\n",
    "time consumption ratio 10 : 34.2903339863 7.5352370739\n",
    "4 4\n",
    "time consumption ratio 10 : 41.3545651436 5.32629704475\n",
    "8 8\n",
    "time consumption ratio 10 : 60.5717840195 3.80291986465\n",
    "16 16\n",
    "time consumption ratio 10 : 74.6524348259 3.14331817627\n",
    "32 32\n",
    "time consumption ratio 10 : 90.6370639801 2.22257304192\n",
    "64 64\n",
    "time consumption ratio 10 : 138.396835804 1.66773104668\n",
    "128 128\n",
    "time consumption ratio 10 : 174.130022049 1.4323618412\n",
    "256 256\n",
    "time consumption ratio 10 : 217.209703207 1.43706798553\n",
    "512 512\n",
    "time consumption ratio 10 : 289.347126961 1.57845401764\n",
    "error at tree height 10 : 0 0\n",
    "running time at tree height 10 : 1185.33719301 1183.75290084\n",
    "number of leaf nodes at tree height 10 : 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#outlier detection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data inspection for the nodes of the trained tree\n",
    "\n",
    "def feature_value_extract( valueSet):\n",
    "    \n",
    "    tmpval=1\n",
    "    tmp_valSet= valueSet\n",
    "    valSet_list=[]\n",
    "    cnt=0\n",
    "    \n",
    "    while tmpval <= tmp_valSet:\n",
    "        if tmpval & tmp_valSet !=0:\n",
    "            valSet_list.append(cnt)\n",
    "        tmpval=tmpval<<1\n",
    "        cnt=cnt+1\n",
    "        \n",
    "    return valSet_list\n",
    "\n",
    "#????\n",
    "def training_data_summary( train_rdd ):\n",
    "\n",
    "#     always remove outliers before\n",
    "    train_rdd_clean= train_rdd.filter(lambda line: line[0]<1000)\n",
    "    distinct_featureVals = train_rdd_clean.map(lambda line: (line[1], line[2],line[3],line[4] ) ).distinct().collect()\n",
    "   \n",
    "    featureConfig_val={}\n",
    "    for i in distinct_featureVals:\n",
    "        \n",
    "        tmprdd=train_rdd_clean.filter(lambda line:(line[1], line[2],line[3],line[4])  ==i ).map(lambda line: line[0])\n",
    "        tmpval=tmprdd.reduce(lambda a,b:a+b)\n",
    "        tmpval= tmpval/tmprdd.count()*1.0\n",
    "        \n",
    "        featureConfig_val.update( {i:tmpval} )\n",
    "    \n",
    "#     print featureConfig_val\n",
    "    return featureConfig_val\n",
    "        \n",
    "\n",
    "def config_check( featureVal_set, feature_num):\n",
    "    \n",
    "    for i in range(0, feature_num):\n",
    "        if len(featureVal_set[i]) >1 :\n",
    "            return -1\n",
    "    return 1\n",
    "\n",
    "def lookup_featureConfig_val( configToVal_dict, value_set, feature_num):\n",
    "    \n",
    "    tmptuple=()\n",
    "    for i in range(0, feature_num):\n",
    "        tmptuple = tmptuple + ( value_set[i][0]  ,)\n",
    "#     print '++++++++++++', tmptuple\n",
    "    return configToVal_dict[tmptuple]\n",
    "\n",
    "\n",
    "\n",
    "def test_error_indiviMapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], test_tree_split)\n",
    "    return (test_leaf_nodes[ tmpnode ], line[0])\n",
    "    \n",
    "def test_error_mapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], test_tree_split)\n",
    "    \n",
    "#     return ( test_leaf_nodes[ tmpnode ],  line[0]   )\n",
    "    return (test_leaf_nodes[ tmpnode ]  - line[0])*(test_leaf_nodes[ tmpnode ]  - line[0])\n",
    "    \n",
    "def test_error_tree( current_depth, testRDD  ):\n",
    "    \n",
    "    test_cnt= testRDD.count()\n",
    "    err_rdd=testRDD.map( lambda line: test_error_mapFunc(line))\n",
    "    err_sum = err_rdd.reduce( lambda a,b: a+b)\n",
    "    \n",
    "    print 'test error at depth',current_depth, ':', err_sum*1.0/test_cnt\n",
    "    return err_sum*1.0/test_cnt\n",
    "    \n",
    "def bfs_tree(tree, total_featureVal_set, depth, tree_esti, trainRDD ):\n",
    "    \n",
    "    nodeNum=len(tree)   \n",
    "    if nodeNum == 0:\n",
    "        return\n",
    "    \n",
    "    feature_ConfigToVal = training_data_summary( trainRDD )\n",
    "    \n",
    "    interNode_cnt=0\n",
    "    non_leaf_cnt=0\n",
    "    recog_config_cnt=0\n",
    "    \n",
    "    qu=[]\n",
    "    current_depth=0\n",
    "    qu.append((0,total_featureVal_set,0))\n",
    "    \n",
    "    \n",
    "    recog_config_cnt_depth=[]\n",
    "    recog_conf=[]\n",
    "    \n",
    "    #debug\n",
    "    test_queue=[]\n",
    "    \n",
    "    while current_depth<= depth:\n",
    "        \n",
    "        print 'non_leaf_node   count at previous depth', non_leaf_cnt\n",
    "        \n",
    "        print 'depth', current_depth,':', len(qu)\n",
    "        tmpqu=[]\n",
    "        \n",
    "        non_leaf_cnt=0\n",
    "        \n",
    "        for i in qu:\n",
    "            current_nodeIdx= i[0] \n",
    "            current_featureVal_set= i[1]\n",
    "            \n",
    "            split_feature= tree[current_nodeIdx][0]\n",
    "            split_valueSet= tree[current_nodeIdx][1]\n",
    "                 \n",
    "            if split_feature == -1: \n",
    "                \n",
    "                if config_check( current_featureVal_set, numFeatures) ==1:\n",
    "                    recog_config_cnt= recog_config_cnt+1\n",
    "                    \n",
    "                    tmpval=lookup_featureConfig_val( feature_ConfigToVal, current_featureVal_set, featureNum)\n",
    "                    \n",
    "                    print current_nodeIdx,current_featureVal_set, 'has the estimate:',\\\n",
    "                    tree_esti[ current_nodeIdx*2+1],'true:',tmpval\n",
    "                    #result\n",
    "                    recog_conf.append( (tree_esti[ current_nodeIdx*2+1], tmpval) )\n",
    "                    \n",
    "                else:\n",
    "                    print current_nodeIdx,current_featureVal_set, 'has the estimate:',\\\n",
    "                    tree_esti[ current_nodeIdx*2+1],'!!! not seperated !!!'\n",
    "                    \n",
    "#                     print '!!! debug !!!', current_nodeIdx,tree[current_nodeIdx], current_featureVal_set\n",
    "                \n",
    "                interNode_cnt=interNode_cnt+1\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                non_leaf_cnt=non_leaf_cnt+1\n",
    "                \n",
    "                featureValue_list = feature_value_extract( split_valueSet)\n",
    "                \n",
    "                print current_nodeIdx, current_featureVal_set,'non-leaf node,', split_feature, featureValue_list\n",
    "                \n",
    "                \n",
    "            #debug\n",
    "#             if current_depth == 9:\n",
    "#                 print '+++ debug ++++',current_nodeIdx,tree[current_nodeIdx], current_featureVal_set, \\\n",
    "#                 featureValue_list,split_feature\n",
    "    \n",
    "            left_featureValue_set=[]\n",
    "            right_featureValue_set=[]\n",
    "            featureNum= len(current_featureVal_set)\n",
    "    \n",
    "            for i in range(0, numFeatures):\n",
    "                if i != split_feature:\n",
    "                    left_featureValue_set.append(  copy.deepcopy(current_featureVal_set[i])   )\n",
    "                    right_featureValue_set.append(  copy.deepcopy(current_featureVal_set[i])   )\n",
    "                else:\n",
    "                    left_featureValue_set.append(  copy.deepcopy(featureValue_list )   )\n",
    "                    right_featureValue_set.append(copy.deepcopy(list(set(current_featureVal_set[i] )-set(featureValue_list)))   )\n",
    "            \n",
    "            \n",
    "#             #debug\n",
    "#             if current_depth==8:\n",
    "#                 if left_featureValue_set== [[2], [11, 13], [103]] or \\\n",
    "#                 right_featureValue_set==right_featureValue_set:\n",
    "#                     print '????? debug ????', current_nodeIdx,left_featureValue_set,right_featureValue_set\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            tmpqu.append(  (current_nodeIdx*2+1,left_featureValue_set, current_nodeIdx)  )\n",
    "            tmpqu.append(  (current_nodeIdx*2+2,right_featureValue_set,current_nodeIdx) )\n",
    "        \n",
    "        qu=copy.deepcopy(tmpqu)\n",
    "        current_depth= current_depth+1\n",
    "        \n",
    "        #result\n",
    "        recog_config_cnt_depth.append( recog_config_cnt )\n",
    "    \n",
    "    \n",
    "# print out leaf nodes\n",
    "\n",
    "    print 'non_leaf_node count at previous depth', non_leaf_cnt\n",
    "    print 'bottom depth', current_depth,':', len(qu)\n",
    "    \n",
    "    for i in qu:\n",
    "        current_nodeIdx= i[0] \n",
    "        current_featureVal_set= i[1]\n",
    "        parent_nodeIdx= i[2]\n",
    "        \n",
    "#         debug \n",
    "#         print '----debug----', current_featureVal_set\n",
    "        \n",
    "        if 2*parent_nodeIdx+1 == current_nodeIdx: \n",
    "#         parent_nodeIdx == (current_nodeIdx*2+1):\n",
    "            tree_esti_val= tree_esti[ parent_nodeIdx*2]\n",
    "        else:\n",
    "            tree_esti_val= tree_esti[ parent_nodeIdx*2+1] \n",
    "        \n",
    "        if config_check( current_featureVal_set, featureNum) ==1:\n",
    "            recog_config_cnt= recog_config_cnt+1\n",
    "            tmpval=lookup_featureConfig_val( feature_ConfigToVal, current_featureVal_set, featureNum)\n",
    "            \n",
    "            #result\n",
    "            recog_conf.append( (tree_esti_val, tmpval) )\n",
    "            \n",
    "            print current_nodeIdx,current_featureVal_set, 'has the estimate:', tree_esti_val,\\\n",
    "                    'true:',tmpval\n",
    "        else:\n",
    "            print current_nodeIdx,current_featureVal_set, 'has the estimate:', tree_esti_val\n",
    "    \n",
    "    #result\n",
    "    recog_config_cnt_depth.append( recog_config_cnt )\n",
    "        \n",
    "    print 'inter-node number:', interNode_cnt\n",
    "    print ' *******  number of identified configurations:', recog_config_cnt\n",
    "    \n",
    "    tmpval=0\n",
    "    for i in range(0,recog_config_cnt):\n",
    "        tmpval=tmpval+ (recog_conf[i][0]-recog_conf[i][1])*(recog_conf[i][0]-recog_conf[i][1])\n",
    "    \n",
    "    return (recog_conf, recog_config_cnt_depth, tmpval*1.0/recog_config_cnt)\n",
    "\n",
    "# len( tree_history_split)\n",
    "# len(tree_history_esti)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write leaf-nodes results to txt file\n",
    "text_file = open(\"regTree_binNumFixed.txt\", 'a')\n",
    "text_file.write('\\n \\nleaf nodes at depth ')\n",
    "text_file.write(\"%f: \\n\" % maxdepth)\n",
    "for item in leaf_nodes:\n",
    "    text_file.write(\"%f  \" % item)\n",
    "\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#debug cluster side\n",
    "dta_train_all = dta.cache().sample(False, .2, 12345)\n",
    "dta_test_all = dta.cache().sample(False, .01, 43243)\n",
    "print dta_train_all.count(), dta_test_all.count()\n",
    "                \n",
    "dta_train= dta_train_all\n",
    "numFeatures=20\n",
    "bin_num=300\n",
    "\n",
    "\n",
    "#debug cluster side\n",
    "\n",
    "#statistic_infor_check for distributed information collection\n",
    "def statistic_infor_check(local_aggre_nodes):\n",
    "    \n",
    "    numNode = len(local_aggre_nodes)\n",
    "#     print numNode\n",
    "    tmpsum=0\n",
    "    for i in range(0, numNode):\n",
    "        tmpsum=tmpsum+local_aggre_nodes[i][1]['count']\n",
    "\n",
    "    if tmpsum != 400759:\n",
    "        print '########### problem in node count ################'\n",
    "\n",
    "    tmpsum=0\n",
    "    for i in range(0, numNode):\n",
    "        #check node hist\n",
    "        node_count = local_aggre_nodes[i][1]['count']\n",
    "            \n",
    "        #check node hist_list\n",
    "        tmpsum=0\n",
    "        for j in local_aggre_nodes[i][1]['hist_list']:\n",
    "            tmpsum = tmpsum + j[2]\n",
    "        if tmpsum != node_count:\n",
    "            print '$$$$ problem in the histogram_list of node', tmpsum, node_count\n",
    "\n",
    "            \n",
    "        #check feature values\n",
    "        for j in range(0, numFeatures):\n",
    "\n",
    "            tmpsum=0\n",
    "            \n",
    "            for k in local_aggre_nodes[i][1][j].keys():    \n",
    "               \n",
    "                value_cnt = local_aggre_nodes[i][1][j][k]['count_inFeatureValue']\n",
    "                value_hist= local_aggre_nodes[i][1][j][k]['hist_val_list']\n",
    "                tmpsum= tmpsum+value_cnt\n",
    "                if len(value_hist) > bin_num:\n",
    "                    print '???? bin num wrong', len(value_hist)\n",
    "                    \n",
    "                #check feature-value hist_list\n",
    "                tmpsum1=0\n",
    "                for m in value_hist:\n",
    "                    tmpsum1= tmpsum1 + m[2]\n",
    "                if tmpsum1 != value_cnt:\n",
    "                    print '++++ problem in feature-value histogram_list', tmpsum1, value_cnt\n",
    "                 \n",
    "            if tmpsum != node_count:\n",
    "                print '---- problem !!!! in feature:',j, tmpsum, node_count    \n",
    "                       \n",
    "currentNode_split_fromMaster = sc.broadcast([])\n",
    "node_split=[]\n",
    "\n",
    "runtime=[]\n",
    "\n",
    "\n",
    "for cur_depth in range(0,10):\n",
    "    \n",
    "    current_NumNodes= int( math.pow(2,  cur_depth)) \n",
    "#     del node_split[:]\n",
    "#     for i in range(0, current_NumNodes):\n",
    "#         node_split.append(i)\n",
    "#     currentNode_split_fromMaster = sc.broadcast(node_split)\n",
    "\n",
    "    \n",
    "    st=time.time()\n",
    "    \n",
    "    if cur_depth>=7:\n",
    "        statis_partitions = dta_train.mapPartitions( partition_combiner_hist )\n",
    "        aggre_nodes =statis_partitions.reduceByKey(lambda statis_partition_1, statis_partition_2: \n",
    "                              merge_parttion_combiner_hist(statis_partition_1,statis_partition_2 ))    \n",
    "        \n",
    "#         local_aggre_nodes= aggre_nodes.take(1)\n",
    "        local_aggre_nodes= aggre_nodes.collect()\n",
    "        statistic_infor_check(local_aggre_nodes)\n",
    "\n",
    "    \n",
    "    ed=time.time()\n",
    "    \n",
    "    print 'depth-',cur_depth,': done', ed-st\n",
    "    \n",
    "    \n",
    "    for i in range(0,current_NumNodes):\n",
    "        node_split.append(i)\n",
    "        \n",
    "    currentNode_split_fromMaster = sc.broadcast(node_split)\n",
    "    \n",
    "    runtime.append(ed-st)\n",
    "\n",
    "print 'done'\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
