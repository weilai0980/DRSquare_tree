{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os \n",
    "import sys \n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "from datetime  import datetime \n",
    "\n",
    "import numpy as np  # learn \n",
    "import pandas as pd # learn\n",
    "from pandas import *\n",
    "from numpy import *\n",
    "\n",
    "from scipy import stats # look at scipy\n",
    "from scipy import linalg\n",
    "from scipy import *\n",
    "\n",
    " \n",
    "import matplotlib as mplt # learn matplolib \n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "import seaborn as sns \n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc={\"figure.figsize\": (14, 6)})\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import sklearn as sk\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import random\n",
    "import time\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328.0, 10, 10, 2, 2, 8, 1, 3)\n",
      "3315067\n"
     ]
    }
   ],
   "source": [
    "#BT data loading\n",
    "\n",
    "# dta_RDD = sc.textFile(\"file:///home/tguo/data/tian-test/udpjitter-H1april2014_regTree.csv\")\n",
    "\n",
    "dta_RDD = sc.textFile(\"hdfs://computer61.ant-net/user/tguo/udpjitter-H1april2014_regTree_mllib.csv\")\n",
    "\n",
    "\n",
    "dta_splited = dta_RDD.map(lambda line: line.split(\",\")).map(lambda r:\n",
    "                                                                (float(r[0]),int( float(r[1])), \n",
    "                                                                 int(float(r[2])),int(float(r[3])),\n",
    "                                                                 int(float(r[4])),int(float(r[5])),\n",
    "                                                                 int(float(r[6])),int(float(r[7])) ))\n",
    "# dta_splited.first()\n",
    "\n",
    "dta= dta_splited\n",
    "dta.cache()\n",
    "print dta.first()\n",
    "print dta.count()\n",
    "\n",
    "\n",
    "# dta_filtered = dta.filter(lambda line: line[0] <2000 )\n",
    "# dta_filtered.cache()\n",
    "# dta_filtered.count()\n",
    "# print dta.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46.7446171548, 4, 13, 105)\n",
      "1000000\n"
     ]
    }
   ],
   "source": [
    "# synthetic data\n",
    "dta_RDD = sc.textFile(\"file:///home/tguo/tian_src/synthetic_data.txt\")\n",
    "dta_RDD.cache()\n",
    "\n",
    "dta_splited = dta_RDD.map(lambda line: line.split(\",\")).map(lambda r: ( float(r[3]),int(r[0]),int(r[1]),int(r[2])) )\n",
    "# ( float(r[2]),float(r[0]),float(r[1]) ) \n",
    "\n",
    "dta= dta_splited\n",
    "dta.cache()\n",
    "\n",
    "print 'original:',dta.first()\n",
    "print dta.count()\n",
    "\n",
    "# re-set index of categorical features\n",
    "\n",
    "# feature_dist=[]\n",
    "\n",
    "# tmp1 = dta.map(lambda line: line[1]).distinct().collect()\n",
    "# feature_dist.append(tmp1)\n",
    "\n",
    "# tmp1 = dta.map(lambda line: line[2]).distinct().collect()\n",
    "# feature_dist.append(tmp1)\n",
    "\n",
    "# tmp1 = dta.map(lambda line: line[3]).distinct().collect()\n",
    "# feature_dist.append(tmp1)\n",
    "\n",
    "# fea_cnt= len(feature_dist )\n",
    "# fea_map=[]\n",
    "\n",
    "# for i in range(0, fea_cnt):\n",
    "#     tmpcnt = len(feature_dist[i])\n",
    "#     val_map= dict( zip(feature_dist[i], range(0,tmpcnt)) )\n",
    "#     fea_map.append(val_map)\n",
    "\n",
    "# def reset_index( line ):\n",
    "#     tmp=[]\n",
    "#     tmp.append(line[0])\n",
    "#     for i in range(1,4):\n",
    "#         tmp.append(fea_map[i-1][ line[i] ] )\n",
    "#     return tmp\n",
    "\n",
    "# dta = dta.map( lambda line: reset_index(line)  )\n",
    "\n",
    "# print 'feature value re-indexed:',dta.first()\n",
    "# print dta.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [1, 10, 8], 12: [12, 25, 8], 100: [100, 100, 1]}\n"
     ]
    }
   ],
   "source": [
    "#debug: histogram operations\n",
    "bin_num =3\n",
    "\n",
    "tmphis1={  1:[1,10,8] , 20:[20,25,4],  12:[12,17,4]  }\n",
    "\n",
    "update_hist(tmphis1, 100)\n",
    "\n",
    "print tmphis1\n",
    "\n",
    "\n",
    "# update_hist(tmphis1, 0.5)\n",
    "\n",
    "# print tmphis1\n",
    "\n",
    "# update_hist(tmphis1, 600)\n",
    "\n",
    "# print tmphis1\n",
    "\n",
    "bin_num=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# basic functions in one-pass method  [on the cluster side]\n",
    "\n",
    "# assign each data instance to the node\n",
    "def search_nodeToData(features, tree):\n",
    "    \n",
    "    nodeNum=len(tree)\n",
    "    if nodeNum == 0:\n",
    "        return 0;\n",
    "    \n",
    "    current_nodeIdx=0\n",
    "    while current_nodeIdx< nodeNum:\n",
    "        split_feature= tree[current_nodeIdx][0]\n",
    "        split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "        if split_feature==-1:\n",
    "            return -1\n",
    "        \n",
    "        if (( 1<<features[split_feature]) &  split_valueSet) != 0 :\n",
    "            current_nodeIdx=current_nodeIdx*2+1\n",
    "        else:\n",
    "            current_nodeIdx=current_nodeIdx*2+2\n",
    "   \n",
    "    return current_nodeIdx - nodeNum\n",
    "\n",
    "def update_hist(hist, newY):\n",
    "#     hist_local= \n",
    "#     hist\n",
    "    numbins=len(hist)\n",
    "    \n",
    "    if newY in hist.keys():\n",
    "        hist[newY][2]= hist[newY][2] +1 \n",
    "    else:\n",
    "        hist.update({newY: [newY,newY,1]})\n",
    "        if numbins+1 > bin_num:\n",
    "            hist.update({newY: [newY,newY,1]}) \n",
    "            #data in each bin: left bound, right bound, count\n",
    "        \n",
    "            sorted_binIds=hist.keys()\n",
    "            # binId is the lower bound of the value range of the bin\n",
    "            sorted_binIds.sort()\n",
    "        \n",
    "            mindis= sorted_binIds[1]- sorted_binIds[0]\n",
    "            mindis_bin_left=sorted_binIds[0]\n",
    "            mindis_bin_right=sorted_binIds[1]\n",
    "            \n",
    "            for i in range(1,numbins+1):\n",
    "                tmp= sorted_binIds[i]- sorted_binIds[i-1]\n",
    "                if tmp < mindis:\n",
    "                    mindis=tmp\n",
    "                    mindis_bin_left= sorted_binIds[i-1]\n",
    "                    mindis_bin_right= sorted_binIds[i]       \n",
    "            \n",
    "#           print hist_local[mindis_bin_right][1],hist_local[mindis_bin_left][1]\n",
    "            hist[mindis_bin_left][1] = max(hist[mindis_bin_right][1],hist[mindis_bin_left][1] )                        \n",
    "            hist[mindis_bin_left][2] = hist[mindis_bin_left][2] + \\\n",
    "            hist[mindis_bin_right][2]\n",
    "            del hist[mindis_bin_right]\n",
    "            \n",
    "    return hist\n",
    "\n",
    "# def concatenate_hist(hist1, hist2):\n",
    "#     concat_hist={}\n",
    "#     tmp=[]\n",
    "    \n",
    "#     for i in hist1.keys():\n",
    "#         if i not in hist2.keys():\n",
    "#             concat_hist.update( {i:hist1[i]}  )\n",
    "#         else:\n",
    "#             del tmp[:]\n",
    "#             tmp.append(i)\n",
    "#             tmp.append( max( hist1[i][1], hist2[i][1]) )\n",
    "#             tmp.append(  hist1[i][2]+hist2[i][2]  )\n",
    "#             concat_hist.update( {i:tmp}  )\n",
    "    \n",
    "#     for i in hist2.keys():\n",
    "#         if i not in hist1.keys():\n",
    "#             concat_hist.update( {i:hist2[i]})\n",
    "#     return concat_hist\n",
    "\n",
    "# def concatenate_hist_memory(hist1, hist2):\n",
    "#     tmp=[]\n",
    "#     for i in hist1.keys(): \n",
    "#         if i not in hist2.keys():\n",
    "#             hist1.update( {i:hist1[i]}  )\n",
    "#         else:\n",
    "#             del tmp[:]\n",
    "#             tmp.append(i)\n",
    "#             tmp.append( max( hist1[i][1], hist2[i][1]) )\n",
    "#             tmp.append(  hist1[i][2]+hist2[i][2]  )\n",
    "#             hist1.update( {i:tmp}  )\n",
    "    \n",
    "#     for i in hist2.keys():\n",
    "#         if i not in hist1.keys():\n",
    "#             hist1.update( {i:hist2[i]})\n",
    "#     return hist1\n",
    "\n",
    "def merge_hist(hist1, hist2):\n",
    "    tmp=[]\n",
    "    concat_hist=hist1\n",
    "    \n",
    "    if len(hist1)==0:\n",
    "        return hist2\n",
    "    if len(hist2)==0:\n",
    "        return hist1\n",
    "\n",
    "    for i in concat_hist.keys():\n",
    "        if i in hist2.keys():\n",
    "            concat_hist.update( {i: [i,max( concat_hist[i][1], hist2[i][1]),concat_hist[i][2]+hist2[i][2]]})\n",
    "    \n",
    "    for i in hist2.keys():\n",
    "        if i not in concat_hist.keys():\n",
    "            concat_hist.update( {i:hist2[i]})\n",
    "\n",
    "    cnt_total= len(concat_hist)\n",
    "    if cnt_total <= bin_num:\n",
    "        return concat_hist\n",
    "    else:\n",
    "        bins=concat_hist.keys()\n",
    "        bins.sort()\n",
    "        \n",
    "        disl=[]\n",
    "        disr=[]\n",
    "        tmpdis=0\n",
    "        tmpleft=0\n",
    "        tmpright=0\n",
    "        \n",
    "        for i in range(0,cnt_total-1):\n",
    "            tmpleft= bins[i]\n",
    "            tmpright= bins[i+1]\n",
    "            tmpdis= tmpright-tmpleft \n",
    "            disl.append((tmpdis,tmpleft))\n",
    "            disr.append((tmpdis,tmpright))\n",
    "        disl.sort()\n",
    "        disr.sort()\n",
    "        \n",
    "        bin_num_toRemove=cnt_total - bin_num\n",
    "        disIdx=[]\n",
    "        cur=0\n",
    "        \n",
    "        for i in range(0,bin_num_toRemove):\n",
    "            while disl[cur][1] not in concat_hist.keys()  :\n",
    "                cur=cur+1\n",
    "            \n",
    "            tmphist1= concat_hist[disl[cur][1] ]\n",
    "            tmphist2= concat_hist[disr[cur][1] ]\n",
    "            tmpval1=tmphist1[1]\n",
    "            tmpval2=tmphist2[1]\n",
    "            tmphist1[1]= max(tmpval1, tmpval2)\n",
    "            tmphist1[2]=  tmphist1[2] + tmphist2[2]\n",
    "            del concat_hist[disr[cur][1]]\n",
    "            \n",
    "            cur=cur+1\n",
    "            \n",
    "    return concat_hist\n",
    "\n",
    "def partition_combiner_hist(list_dvAndfeatures):\n",
    "    \n",
    "    nodes_dict={}\n",
    "    tmpcnt=0\n",
    "    \n",
    "    for dvAndfeatures in list_dvAndfeatures:\n",
    "        \n",
    "        Y= dvAndfeatures[0]\n",
    "        \n",
    "        node= search_nodeToData(dvAndfeatures[1:numFeatures+1], \\\n",
    "                                currentNode_split_fromMaster.value)\n",
    "        \n",
    "        if node in nodes_dict.keys():\n",
    "            \n",
    "            # new added: s um of y in a node\n",
    "            nodes_dict[node]['sumY'] = nodes_dict[node]['sumY']+Y \n",
    "            nodes_dict[node]['count'] = nodes_dict[node]['count']+1 \n",
    "            nodes_dict[node]['hist']= update_hist(nodes_dict[node]['hist'], Y)\n",
    "            \n",
    "            for i in range(0,numFeatures):    \n",
    "                feature_val= dvAndfeatures[i+1]\n",
    "                \n",
    "                if feature_val not in nodes_dict[node][i]:\n",
    "                    nodes_dict[node][i].update( { feature_val: {}   }  )\n",
    "    \n",
    "                    nodes_dict[node][i][feature_val].update( {'count_inFeatureValue':1} )\n",
    "                    nodes_dict[node][i][feature_val].update( {'hist_inFeatureValue': {} } )\n",
    "                    nodes_dict[node][i][feature_val]['hist_inFeatureValue'].update({Y:[Y,Y,1]})\n",
    "                    \n",
    "                else:\n",
    "                    nodes_dict[node][i][feature_val]['count_inFeatureValue'] = \\\n",
    "                    nodes_dict[node][i][feature_val]['count_inFeatureValue']+1 \n",
    "                    nodes_dict[node][i][feature_val]['hist_inFeatureValue']=\\\n",
    "                    update_hist(nodes_dict[node][i][feature_val]['hist_inFeatureValue'], Y)\n",
    "                    \n",
    "            \n",
    "        else:\n",
    "            nodes_dict.update( {node: {}})  \n",
    "            \n",
    "            # new added: sum of y in a node\n",
    "            nodes_dict[node].update( { 'sumY': Y} )\n",
    "            nodes_dict[node].update( { 'count': 1} )\n",
    "            nodes_dict[node].update( { 'hist': {}} )\n",
    "            nodes_dict[node]['hist'].update({Y:[Y,Y,1]})            \n",
    "        \n",
    "            for i in range(0,numFeatures):\n",
    "                feature_val= dvAndfeatures[i+1]\n",
    "                \n",
    "                nodes_dict[node].update( { i: {}} )    \n",
    "                nodes_dict[node][i].update( { feature_val: {}   }  )\n",
    "             \n",
    "                nodes_dict[node][i][feature_val].update( {'count_inFeatureValue':1} )\n",
    "                nodes_dict[node][i][feature_val].update( {'hist_inFeatureValue': {} } )\n",
    "                nodes_dict[node][i][feature_val]['hist_inFeatureValue'].update({Y:[Y,Y,1]})    \n",
    "                        \n",
    "    return zip(nodes_dict.keys(), nodes_dict.values())\n",
    "\n",
    "def merge_parttion_combiner_hist(nodeToFeatureToValue_1,nodeToFeatureToValue_2 ):\n",
    "# optimization: calculate median and amd for feature-value    \n",
    "    \n",
    "    # new added: sum of Y in a node\n",
    "    nodeToFeatureToValue_1['sumY']= nodeToFeatureToValue_1['sumY']+\\\n",
    "    nodeToFeatureToValue_2['sumY']\n",
    "    \n",
    "    nodeToFeatureToValue_1['count']= nodeToFeatureToValue_1['count']+\\\n",
    "    nodeToFeatureToValue_2['count'] \n",
    "    \n",
    "    nodeToFeatureToValue_1['hist']=merge_hist(nodeToFeatureToValue_1['hist'],nodeToFeatureToValue_2['hist'])\n",
    "    \n",
    "    for i in range(0, numFeatures): #feature\n",
    "        for j in nodeToFeatureToValue_1[i].keys(): #feature value\n",
    "            \n",
    "            feature_val=j\n",
    "            \n",
    "            if feature_val in nodeToFeatureToValue_2[i].keys():               \n",
    "                nodeToFeatureToValue_1[i][j]['hist_inFeatureValue'] = \\\n",
    "                merge_hist(nodeToFeatureToValue_1[i][j]['hist_inFeatureValue'], \n",
    "                           nodeToFeatureToValue_2[i][j]['hist_inFeatureValue'])     \n",
    "                nodeToFeatureToValue_1[i][j]['count_inFeatureValue'] = nodeToFeatureToValue_1[i][j]['count_inFeatureValue']+nodeToFeatureToValue_2[i][j]['count_inFeatureValue']\n",
    "                \n",
    "    for i in range(0, numFeatures):\n",
    "        for j in nodeToFeatureToValue_2[i].keys():\n",
    "            \n",
    "            feature_val=j\n",
    "            \n",
    "            if feature_val not in nodeToFeatureToValue_1[i].keys():\n",
    "                nodeToFeatureToValue_1[i].update({feature_val: {} })\n",
    "                nodeToFeatureToValue_1[i][feature_val]= nodeToFeatureToValue_2[i][feature_val].copy()\n",
    "                              \n",
    "    return  nodeToFeatureToValue_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test: one-pass method\n",
    "dta_test = dta\n",
    "\n",
    "#tree=currentNode_split_fromMaster.value()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "statis_partitions = dta_test.mapPartitions( partition_combiner_hist )\n",
    "# local_statis_partition=statis_partitions.collect()\n",
    "aggre_nodes =statis_partitions.reduceByKey(lambda statis_partition_1, statis_partition_2: \n",
    "                              merge_parttion_combiner_hist(statis_partition_1,statis_partition_2 ))\n",
    "local_aggre_nodes= aggre_nodes.collect()\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "\n",
    "print elapsed,'sec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n",
      "(8.0, 77.0)\n"
     ]
    }
   ],
   "source": [
    "# test for absolute median deviation \n",
    "tmphis={  1:[1,10,8] , 20:[20,25,2],  12:[12,17,4]  }\n",
    "print MAD_hist(tmphis, 14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split decision  [on the local side]\n",
    "\n",
    "#implementation notes:\n",
    "# bit-operation to record the set of selected feature values in the left child\n",
    "\n",
    "#reset_hist function: merge bins if necessary to ensure the constant number of bins\n",
    "def reset_hist(hist):\n",
    "    cnt=len(hist)\n",
    "    if cnt > bin_num :\n",
    "        bin_ids= hist.keys()\n",
    "        sorted_bin_ids=sorted(bin_ids)\n",
    "        num_toMerge= cnt-bin_num\n",
    "        \n",
    "        mindis= sorted_bin_ids[1]-sorted_bin_ids[0]\n",
    "        tmpdis=[]\n",
    "        for i in range(2,cnt):\n",
    "            tmpdis.add( (sorted_bin_ids[i]-sorted_bin_ids[i-1],sorted_bin_ids[i-1],\\\n",
    "                        sorted_bin_ids[i]-sorted_bin_ids[i]) )\n",
    "             \n",
    "        sorted_dis= sorted(tmpdis, key = lambda line: line[2]  )\n",
    "        \n",
    "        for i in range(0, num_toMerge):\n",
    "            lower_bin=sorted_dis[i][0]\n",
    "            upper_bin= sorted_dis[i][1]\n",
    "            \n",
    "            hist[lower_bin][2]=hist[lower_bin][2]+hist[upper_bin][2]\n",
    "            hist[lower_bin][1]=max(hist[lower_bin][1], hist[upper_bin][1])\n",
    "            del hist[upper_bin]\n",
    "                \n",
    "#?? need improvments    \n",
    "def boundary_hist(l,r,cnt, l_toMinus, r_toMinus, cnt_toMinus):\n",
    "    if r>= r_toMinus:\n",
    "        return (l,r)\n",
    "    r-(r-l)/cnt\n",
    "      \n",
    "#?? need improvments        \n",
    "def minus_hist(hist1, hist2):\n",
    "    \n",
    "    # key-[left, right, count]\n",
    "    hist = copy.deepcopy(hist1)\n",
    "    # hist1.copy()\n",
    "    \n",
    "    for i in hist2.keys():\n",
    "        if i in hist.keys():\n",
    "            hist[i][2]= hist[i][2]-hist2[i][2]\n",
    "        \n",
    "    return hist\n",
    "    \n",
    "#MAD: median absolute median in a histogram   \n",
    "def MAD_hist(hist, cnt): \n",
    "\n",
    "    sorted_his= sorted(hist.items(), key= lambda line: line[0])\n",
    "    num_bins= len(hist)\n",
    "    \n",
    "    cntByBounds=0\n",
    "    curCnt=0 \n",
    "    midCnt= cnt/2.0\n",
    "    \n",
    "    mid_bin_idx=0\n",
    "    flag=0\n",
    "    \n",
    "    meanAbsDevi=0\n",
    "    median=0\n",
    "    \n",
    "    cntSum_beforeMid=0\n",
    "    \n",
    "    for i in range(0, num_bins): \n",
    "        \n",
    "        #debug\n",
    "#         print 'in mad:',sorted_his[i][1][2]\n",
    "        \n",
    "        if flag==0:\n",
    "            tmp_curCnt=curCnt+ sorted_his[i][1][2]\n",
    "            tmp_cntByBounds =cntByBounds - \\\n",
    "        (sorted_his[i][1][0]+sorted_his[i][1][1])/2.0*sorted_his[i][1][2]\n",
    "        \n",
    "            if tmp_curCnt >= midCnt:\n",
    "                flag=1\n",
    "                mid_bin_idx= i\n",
    "                \n",
    "                cntSum_beforeMid = curCnt\n",
    "            else:    \n",
    "                curCnt=tmp_curCnt\n",
    "                cntByBounds=tmp_cntByBounds\n",
    "        else:\n",
    "            curCnt= curCnt - sorted_his[i][1][2]\n",
    "            cntByBounds= cntByBounds + \\\n",
    "            (sorted_his[i][1][0]+sorted_his[i][1][1])/2.0*sorted_his[i][1][2]\n",
    "    \n",
    "    \n",
    "    if cnt == 1:\n",
    "        for tmpkey in hist.keys():\n",
    "            return (hist[tmpkey][0],0)\n",
    "    elif cnt ==0:\n",
    "        return (0,0)\n",
    "    else:        \n",
    "        \n",
    "        sample_inMedBin = midCnt - cntSum_beforeMid     \n",
    "        #debug\n",
    "#         print mid_bin_idx,num_bins,sorted_his[mid_bin_idx][1][0],sorted_his[mid_bin_idx][1][1],\\\n",
    "#         sorted_his[mid_bin_idx][1][2]\n",
    "        \n",
    "        median= sorted_his[mid_bin_idx][1][0]*1.0 + \\\n",
    "        (1.0*sorted_his[mid_bin_idx][1][1]-1.0*sorted_his[mid_bin_idx][1][0])/(sorted_his[mid_bin_idx][1][2]+1)*sample_inMedBin\n",
    "        meanAbsDevi =  cntByBounds + curCnt*median \n",
    "        \n",
    "        sample_val= sorted_his[ mid_bin_idx][1][0]*1.0 \n",
    "        sample_interval=1.0*(sorted_his[mid_bin_idx][1][1]-sorted_his[mid_bin_idx][1][0])/(sorted_his[mid_bin_idx][1][2]\\\n",
    "                                                                          +1.0)\n",
    "        \n",
    "        #debug\n",
    "        tmpMAD = meanAbsDevi\n",
    "        \n",
    "        for i in range(0, sorted_his[mid_bin_idx][1][2]):\n",
    "            sample_val= sample_val + sample_interval\n",
    "            meanAbsDevi=meanAbsDevi+ abs( sample_val-median)\n",
    "            \n",
    "            \n",
    "        #debug\n",
    "#         if meanAbsDevi <0:\n",
    "#             tmpsum=0\n",
    "#             for tmpkey in hist:\n",
    "#                 tmpsum= tmpsum+ hist[tmpkey][2]\n",
    "#             if tmpsum!=cnt:\n",
    "#                 print 'inside MAD calculation, number of data check:',tmpsum,cnt             \n",
    "#             print 'inside MAD calculation:', tmpMAD, cnt\n",
    "            \n",
    "        \n",
    "        return (median, meanAbsDevi)\n",
    "\n",
    "\n",
    "def local_merge_hist(hist1, hist2):\n",
    "    \n",
    "    local_hist= copy.deepcopy(hist1)\n",
    "    \n",
    "    for i in hist2.keys():\n",
    "        if i in local_hist.keys():\n",
    "            local_hist[i][2] = local_hist[i][2] +hist2[i][2]\n",
    "            local_hist[i][1]= max(local_hist[i][1], hist2[i][1])\n",
    "        else:\n",
    "            local_hist.update( {i: hist2[i]}  )\n",
    "    return local_hist\n",
    "    \n",
    "\n",
    "def split_onOneFeature_hist(feature_id, values,count_node,hist_node,meanAbsDevi_node):\n",
    "    \n",
    "    value_cnt= len(values)\n",
    "    left_count=0\n",
    "    right_count= count_node\n",
    "    \n",
    "    bestSplitMetric =  meanAbsDevi_node \n",
    "\n",
    "    #this is a bit-record variable\n",
    "    leftSplit_valueSet =0 \n",
    "    leftSplit_count=0\n",
    "    \n",
    "    leftSplit_hist= {}\n",
    "    rightSplit_hist={}\n",
    "        \n",
    "    bestLeft=(-1,-1)\n",
    "    bestRight=(-1,-1)  \n",
    "    \n",
    "    \n",
    "    #sort feature values according to median \n",
    "    sorted_value_map=[]\n",
    "    \n",
    "    for i in values.keys():\n",
    "        currentVal_hist_count= values[i]['count_inFeatureValue']\n",
    "        currentVal_hist= values[i]['hist_inFeatureValue']\n",
    "        sorted_value_map.append((i,MAD_hist(currentVal_hist,currentVal_hist_count)))\n",
    "            \n",
    "    sorted_value_map=sorted(sorted_value_map, key= lambda val: val[1][0] )\n",
    "    values_cnt= len(sorted_value_map)\n",
    "    \n",
    "    # scan the sorted feature values\n",
    "    for k in range(0,values_cnt-1):\n",
    "\n",
    "        current_feature_value=sorted_value_map[k][0]\n",
    "        currentVal_count= values[current_feature_value]['count_inFeatureValue']\n",
    "        currentVal_hist= values[current_feature_value]['hist_inFeatureValue']\n",
    "#         tmp_node_hist= copy.deepcopy(hist_node)\n",
    "\n",
    "#         histograms for the left values\n",
    "        leftSplit_count = leftSplit_count+currentVal_count     \n",
    "        leftSplit_hist=local_merge_hist( leftSplit_hist, currentVal_hist)\n",
    "    \n",
    "#         histograms for the right values\n",
    "        rightSplit_hist={}\n",
    "        tmp_value=0\n",
    "        for j in range(0, values_cnt-1-k ):\n",
    "            tmp_value = sorted_value_map[values_cnt-1-j][0]\n",
    "            rightSplit_hist= local_merge_hist( rightSplit_hist, values[tmp_value]['hist_inFeatureValue'])      \n",
    "            \n",
    "        left = MAD_hist( leftSplit_hist, leftSplit_count)\n",
    "        right= MAD_hist( rightSplit_hist, (count_node - leftSplit_count)) \n",
    "        \n",
    "        #debug\n",
    "#         tmpcnt=0\n",
    "#         for i in leftSplit_hist.keys():\n",
    "#             tmpcnt= tmpcnt+ leftSplit_hist[i][2]\n",
    "#         if tmpcnt!= leftSplit_count:\n",
    "#             print '++++ problem in left split histogram', tmpcnt, leftSplit_count\n",
    "#         tmpcnt=0\n",
    "#         for i in rightSplit_hist.keys():\n",
    "#             tmpcnt= tmpcnt+ rightSplit_hist[i][2]\n",
    "#         if tmpcnt!= (count_node - leftSplit_count):\n",
    "#             print '???? problem in right split histogram', tmpcnt, (count_node - leftSplit_count)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        leftMedian= left[0]\n",
    "        leftMetric= left[1]\n",
    "        \n",
    "        rightMedian= right[0]\n",
    "        rightMetric= right[1]\n",
    "        \n",
    "        #debug\n",
    "#         if leftMedian<0 or rightMedian<0 or leftMetric <=0 or rightMetric <=0:\n",
    "#             print '$$$$ problem in MAD calculation',leftMedian,rightMedian,leftMetric,rightMetric,feature_id,\\\n",
    "#             current_feature_value\n",
    "            \n",
    "        current_splitMetric=1.0*leftSplit_count/count_node*leftMetric + \\\n",
    "                               1.0*(count_node - leftSplit_count)/count_node*rightMetric\n",
    "        \n",
    "        if current_splitMetric < bestSplitMetric:\n",
    "            \n",
    "            bestSplitMetric=current_splitMetric  \n",
    "            \n",
    "            bestLeft=left\n",
    "            bestRight=right\n",
    "            \n",
    "            #bit variable to record which value is chosen for the left child\n",
    "            leftSplit_valueSet=leftSplit_valueSet+ (1<<current_feature_value)        \n",
    "    \n",
    "    return (leftSplit_valueSet, [bestLeft[0], bestRight[0], bestSplitMetric  ] )\n",
    "\n",
    "\n",
    "def find_bestSplit_hist(local_aggre_nodes, current_NumNodes, nodes_tree, \\\n",
    "                        nodes_tree_test):\n",
    "    \n",
    "    nodes_layer=[]\n",
    "    current_idx_nodes =0\n",
    "    \n",
    "    if local_aggre_nodes[ current_idx_nodes ][0] == -1:\n",
    "        current_idx_nodes=1\n",
    "    \n",
    "    #debug \n",
    "    tmpnode_cnt=[]\n",
    "    presplit=[]\n",
    "\n",
    "    for i in range(0, current_NumNodes):\n",
    "        if local_aggre_nodes[ current_idx_nodes ][0] != i:\n",
    "            nodes_layer.append( (-1,-1) )\n",
    "            nodes_tree.append( (-1,-1) )\n",
    "            continue\n",
    " \n",
    "        node_count= local_aggre_nodes[i][1]['count'] \n",
    "        node_hist=  local_aggre_nodes[i][1]['hist']           \n",
    "        \n",
    "        best_splitMetric_feature= MAD_hist( node_hist, node_count)\n",
    "        best_split_sofar= best_splitMetric_feature[1]\n",
    "        preSplitMetric_node= best_splitMetric_feature[1]\n",
    "        best_split=(-1,[-1,-1,-1])\n",
    "        \n",
    "        \n",
    "        #debug\n",
    "        tmpnode_cnt.append(node_count)\n",
    "        presplit.append( best_splitMetric_feature[1]/100000 )\n",
    "         \n",
    "        best_split_feature=-1\n",
    "        best_split_featureValueSet=-1\n",
    "\n",
    "        for j in range(0,numFeatures):\n",
    "                     \n",
    "            statisticToValues = local_aggre_nodes[i][1][j]\n",
    "           \n",
    "            split=split_onOneFeature_hist(j,statisticToValues,node_count, node_hist,preSplitMetric_node)\n",
    "            \n",
    "            if split[1][2] < best_split_sofar:\n",
    "                best_split=split\n",
    "                best_split_sofar=split[1][2]\n",
    "                best_split_feature = j\n",
    "                best_split_featureValueSet= split[0]\n",
    "   \n",
    "        \n",
    "        # for one-layer point assignment\n",
    "        nodes_layer.append( (best_split_feature,  best_split_featureValueSet  )  )\n",
    "        # for whole-tree point assignment\n",
    "        nodes_tree.append(  (best_split_feature,  best_split_featureValueSet  )      )\n",
    "        \n",
    "        # tree for predicting\n",
    "        nodes_tree_test.append(best_split[1][0])\n",
    "        nodes_tree_test.append(best_split[1][1])\n",
    "        \n",
    "        current_idx_nodes= current_idx_nodes+1\n",
    "    \n",
    "    #debug\n",
    "#     print tmpnode_cnt\n",
    "#     print presplit\n",
    "    \n",
    "    return nodes_layer\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#statistic_infor_check for distributed information collection\n",
    "\n",
    "def statistic_infor_check(local_aggre_nodes):\n",
    "    \n",
    "    numNode = len(local_aggre_nodes)\n",
    "#     print numNode\n",
    "    tmpsum=0\n",
    "    for i in range(0, numNode):\n",
    "        tmpsum=tmpsum+local_aggre_nodes[i][1]['count']\n",
    "        \n",
    "#     print tmpsum\n",
    "    if tmpsum != 3315067:\n",
    "        print 'problem in node count'\n",
    "\n",
    "    tmpsum=0\n",
    "    for i in range(0, numNode):\n",
    "        #check node hist\n",
    "        node_count = local_aggre_nodes[i][1]['count']\n",
    "        tmpsum=0\n",
    "        for j in local_aggre_nodes[i][1]['hist'].keys():\n",
    "            tmpsum= tmpsum+local_aggre_nodes[i][1]['hist'][j][2]\n",
    "        if tmpsum!= node_count:\n",
    "            print '$$$$ problem in the histogram of node'\n",
    "            \n",
    "        #check feature values\n",
    "        for j in range(0, numFeatures):\n",
    "            tmpsum=0\n",
    "            for k in local_aggre_nodes[i][1][j].keys():\n",
    "                value_hist= local_aggre_nodes[i][1][j][k]['hist_inFeatureValue']\n",
    "                value_cnt = local_aggre_nodes[i][1][j][k]['count_inFeatureValue']\n",
    "                tmpsum= tmpsum+value_cnt\n",
    "            \n",
    "#                 print len(value_hist)\n",
    "                if len(value_hist) > bin_num:\n",
    "                    print '???? bin num wrong', len(value_hist)\n",
    "            \n",
    "                #check feature-value hist\n",
    "                tmpsum1=0\n",
    "                for m in value_hist.keys():\n",
    "                    tmpsum1= tmpsum1 + value_hist[m][2]\n",
    "                if tmpsum1 != value_cnt:\n",
    "                    print '++++ problem in feature-value histogram', tmpsum1, value_cnt\n",
    "            \n",
    "            if tmpsum != node_count:\n",
    "                print '---- problem !!!! in feature:', tmpsum, node_count    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " #debug\n",
    "    nodeSplits_layer=find_bestSplit_hist( sorted(local_aggre_nodes,key= lambda val:val[0])\\\n",
    "                                    ,3,nodeSplits_tree,node_tree_test)\n",
    "# len(local_aggre_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -th level starts:\n",
      "current split decision: []\n",
      "0 -th level running time:  3890.58822012 sec 197.14286685 sec\n",
      "1 -th level starts:\n",
      "current split decision: [(6, 2)]\n",
      "1 -th level running time:  3801.04191589 sec 219.227946043 sec\n",
      "2 -th level starts:\n",
      "current split decision: [(6, 2), (5, 2570), (4, 1955842)]\n",
      "2 -th level running time:  3446.51540303 sec 232.52446413 sec\n"
     ]
    }
   ],
   "source": [
    "# training  process\n",
    "\n",
    "maxdepth=3\n",
    "numFeatures=7\n",
    "bin_num=800\n",
    "\n",
    "dta_train = dta\n",
    "nodeSplits_tree=[]\n",
    "node_tree_test=[]\n",
    "\n",
    "#ini parameter\n",
    "currentNode_split_fromMaster = sc.broadcast([])\n",
    "\n",
    "for i in range(0,maxdepth):\n",
    "    \n",
    "    start = time.time()\n",
    "    current_NumNodes= int( math.pow(2,  i))\n",
    "    \n",
    "    print i,'-th level starts:'\n",
    "    print 'current split decision:',currentNode_split_fromMaster.value\n",
    "    \n",
    "    \n",
    "      #----------------------- debug ---------------\n",
    "    \n",
    "#     error_partitions = dta_train.mapPartitions( error_partition_combiner_hist )\n",
    "\n",
    "#     error_nodes =error_partitions.reduceByKey(lambda statis_partition_1, \\\n",
    "#                                               statis_partition_2: \n",
    "#                               error_merge_parttion_combiner_hist(statis_partition_1,\\\n",
    "#                                                       statis_partition_2 ))\n",
    "    \n",
    "#     local_error_nodes= error_nodes.collect()\n",
    "    \n",
    "#     local_sorted_error_nodes = sorted(local_error_nodes,key= lambda val:val[0])\n",
    "    \n",
    "# #     print 'current number of nodes:', len(local_sorted_error_nodes)\n",
    "#     tmpres=[]\n",
    "#     tmpcnt=[]\n",
    "#     for k in range(0, len(local_sorted_error_nodes)):   \n",
    "            \n",
    "#         tmpvar=local_sorted_error_nodes[k][1]['sumYsquare']/local_sorted_error_nodes[k][1]['count']\\\n",
    "#         -(local_sorted_error_nodes[k][1]['sumY']/local_sorted_error_nodes[k][1]['count'])*\\\n",
    "#         (local_sorted_error_nodes[k][1]['sumY']/local_sorted_error_nodes[k][1]['count'])\n",
    "        \n",
    "#         tmpres.append( tmpvar/ 100000.0 )\n",
    "#         tmpcnt.append(local_sorted_error_nodes[k][1]['count'])\n",
    "        \n",
    "#     print 'before split:',tmpres\n",
    "#     print 'before split:', tmpcnt\n",
    "    \n",
    "    #---------------------------------------------\n",
    "    \n",
    "    statis_partitions = dta_train.mapPartitions( partition_combiner_hist )\n",
    "    \n",
    "    aggre_nodes =statis_partitions.reduceByKey(lambda statis_partition_1, statis_partition_2: \n",
    "                              merge_parttion_combiner_hist(statis_partition_1,statis_partition_2 ))\n",
    "\n",
    "    local_aggre_nodes= aggre_nodes.collect()\n",
    "\n",
    "    #debug\n",
    "#     statistic_infor_check(local_aggre_nodes)\n",
    "    \n",
    "    cluster_end = time.time()\n",
    "    \n",
    "    nodeSplits_layer=find_bestSplit_hist( sorted(local_aggre_nodes,key= lambda val:val[0])\\\n",
    "                                    ,current_NumNodes,nodeSplits_tree,node_tree_test)\n",
    "    \n",
    "    # could be optimized for performance\n",
    "    \n",
    "    #debug\n",
    "#     statistic_infor_check(local_aggre_nodes)\n",
    "    \n",
    "    \n",
    "    #for whole tree\n",
    "    currentNode_split_fromMaster = sc.broadcast(nodeSplits_tree)\n",
    "    end = time.time() \n",
    "\n",
    "    print i,'-th level running time: ', cluster_end - start,'sec', end- cluster_end, 'sec'\n",
    "    \n",
    "    \n",
    "    #----------------------- debug ---------------\n",
    "    \n",
    "#     error_partitions = dta_train.mapPartitions( error_partition_combiner )\n",
    "\n",
    "#     error_nodes =error_partitions.reduceByKey(lambda statis_partition_1, \\\n",
    "#                                               statis_partition_2: \n",
    "#                               error_merge_parttion_combiner(statis_partition_1,\\\n",
    "#                                                       statis_partition_2 ))\n",
    "    \n",
    "#     local_error_nodes= error_nodes.collect()\n",
    "   \n",
    "    \n",
    "#     local_sorted_error_nodes = sorted(local_error_nodes,key= lambda val:val[0])\n",
    "    \n",
    "# #     print 'current number of nodes:', len(local_sorted_error_nodes)\n",
    "#     tmpres=[]\n",
    "#     tmpcnt=[]\n",
    "    \n",
    "#     for k in range(0, len(local_sorted_error_nodes)):\n",
    "        \n",
    "#         tmpweight=0.0\n",
    "        \n",
    "#         if len(local_sorted_    nodeSplits_layer=find_bestSplit_hist( sorted(local_aggre_nodes,key= lambda val:val[0])\\\n",
    "#                                     ,3,nodeSplits_tree,node_tree_test)\n",
    "# len(local_aggre_nodes)error_nodes) > 1:\n",
    "#             if (k%2) == 0 :\n",
    "#                 tmpweight= local_sorted_error_nodes[k][1]['count']+ \\\n",
    "#                 local_sorted_error_nodes[k+1][1]['count']\n",
    "#             else:\n",
    "#                 tmpweight= local_sorted_error_nodes[k][1]['count']+ \\\n",
    "#                 local_sorted_error_nodes[k-1][1]['count']\n",
    "#             weight=1.0*local_sorted_error_nodes[k][1]['count']/tmpweight\n",
    "            \n",
    "#         else:\n",
    "#             weight=1.0    \n",
    "            \n",
    "# #       weight=1.0     \n",
    "            \n",
    "#         tmpvar=local_sorted_error_nodes[k][1]['sumYsquare']/local_sorted_error_nodes[k][1]['count']\\\n",
    "#         -(local_sorted_error_nodes[k][1]['sumY']/local_sorted_error_nodes[k][1]['count'])*\\\n",
    "#         (local_sorted_error_nodes[k][1]['sumY']/local_sorted_error_nodes[k][1]['count'])\n",
    "        \n",
    "#         tmpvar= weight*tmpvar/ 100000.0\n",
    "        \n",
    "#         tmpres.append( tmpvar )\n",
    "#         tmpcnt.append( local_sorted_error_nodes[k][1]['count'] )\n",
    "    \n",
    "#     reduced_error=[]\n",
    "#     for k in range(0, len(local_sorted_error_nodes),2):\n",
    "#         reduced_error.append( tmpres[k]+tmpres[k+1]    )\n",
    "        \n",
    "#     print 'after split:',reduced_error\n",
    "#     print 'after split:',tmpcnt\n",
    "    \n",
    "    #---------------------------------------------\n",
    "    \n",
    "    #for layer of nodes\n",
    "    #currentNode_split_fromMaster = sc.broadcast(nodeSplits_layer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error at tree height 1 : 1074453.77671 1028431.67887 47.7213981152\n",
      "error at tree height 2 : 1074614.6165 1028587.67404 91.3426940441\n",
      "error at tree height 3 : 1075137.30007 1028913.94437 131.446650028\n",
      "error at tree height 4 : 1074488.31044 1028295.12782 163.675576925\n",
      "error at tree height 5 : 1074212.84323 1027878.85149 188.23348999\n",
      "error at tree height 6 : 1073403.56541 1027236.53432 204.66207695\n",
      "error at tree height 7 : 1072303.28081 1026196.87638 213.713968992\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-2d42ca229424>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mlocal_aggre_nodes\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0maggre_nodes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mcluster_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mnodeSplits_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfind_bestSplit_hist\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_aggre_nodes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m                                    \u001b[1;33m,\u001b[0m\u001b[0mcurrent_NumNodes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnodeSplits_tree\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnode_tree_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;31m#for whole tree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mcurrentNode_split_fromMaster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnodeSplits_tree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-407b6bd5f72e>\u001b[0m in \u001b[0;36mfind_bestSplit_hist\u001b[1;34m(local_aggre_nodes, current_NumNodes, nodes_tree, nodes_tree_test)\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m         \u001b[0mnode_count\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mlocal_aggre_nodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'count'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m         \u001b[0mnode_hist\u001b[0m\u001b[1;33m=\u001b[0m  \u001b[0mlocal_aggre_nodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hist'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# training and testing process\n",
    "\n",
    "# with outliers\n",
    "tmpdta = dta.takeSample(False, 100000, 12243)\n",
    "dta_train = sc.parallelize(tmpdta).cache().sample(False, .7, 12345)\n",
    "dta_test = sc.parallelize(tmpdta).cache().sample(False, .3, 43243)\n",
    "\n",
    "\n",
    "maxdepth=10\n",
    "numFeatures=3\n",
    "bin_num=800\n",
    "\n",
    "def search_nodeToData(features, tree): \n",
    "    nodeNum=len(tree)\n",
    "    if nodeNum == 0:\n",
    "        return 0;\n",
    "    \n",
    "    current_nodeIdx=0\n",
    "    while current_nodeIdx< nodeNum:\n",
    "        split_feature= tree[current_nodeIdx][0]\n",
    "        split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "        if split_feature==-1:\n",
    "            return -1\n",
    "        \n",
    "        if (( 1<<features[split_feature]) &  split_valueSet) != 0 :\n",
    "            current_nodeIdx=current_nodeIdx*2+1\n",
    "        else:\n",
    "            current_nodeIdx=current_nodeIdx*2+2\n",
    "   \n",
    "    return current_nodeIdx - nodeNum\n",
    "\n",
    "def tree_test_mapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], nodeSplits_tree)\n",
    "    return (leaf_nodes[ tmpnode ]  - line[0])*(leaf_nodes[ tmpnode ]  - line[0])\n",
    "    \n",
    "def tree_test( testData_rdd ):\n",
    "    err_rdd=testData_rdd.map( lambda line: tree_test_mapFunc(line))\n",
    "    err_sum = err_rdd.reduce( lambda a,b: a+b)\n",
    "    return err_sum / testData_rdd.count()\n",
    "    \n",
    "    \n",
    "#ini parameter on the cluster\n",
    "currentNode_split_fromMaster = sc.broadcast([])\n",
    "test_err = []\n",
    "train_err = []\n",
    "run_time=[]\n",
    "\n",
    "for i in range(1,maxdepth):\n",
    "    nodeSplits_tree=[]\n",
    "    node_tree_test=[]\n",
    "    currentNode_split_fromMaster = sc.broadcast([])\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for cur_depth in range(0,i):\n",
    "        \n",
    "        current_NumNodes= int( math.pow(2,  cur_depth)) \n",
    "        statis_partitions = dta_train.mapPartitions( partition_combiner_hist )\n",
    "        aggre_nodes =statis_partitions.reduceByKey(lambda statis_partition_1, statis_partition_2: \n",
    "                              merge_parttion_combiner_hist(statis_partition_1,statis_partition_2 ))\n",
    "        local_aggre_nodes= aggre_nodes.collect()\n",
    "        cluster_end = time.time()\n",
    "        \n",
    "        print current_NumNodes, len(local_aggre_nodes)\n",
    "        \n",
    "        nodeSplits_layer=find_bestSplit_hist( sorted(local_aggre_nodes,key= lambda val:val[0])\\\n",
    "                                    ,current_NumNodes,nodeSplits_tree,node_tree_test)\n",
    "        #for whole tree\n",
    "        currentNode_split_fromMaster = sc.broadcast(nodeSplits_tree)\n",
    "    \n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    \n",
    "    leaf_nodes = node_tree_test[ len(node_tree_test)-  (int)( math.pow(2,i)): len(node_tree_test)  ]\n",
    "    tmp_test_err= tree_test( dta_test )\n",
    "    tmp_train_err= tree_test( dta_train )\n",
    "    \n",
    "    test_err.append( tmp_test_err)\n",
    "    train_err.append( tmp_train_err )\n",
    "    run_time.append(elapsed)\n",
    "    \n",
    "    print \"error at tree height\", i,\":\",  tmp_test_err, tmp_train_err, elapsed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1072303.27289\n"
     ]
    }
   ],
   "source": [
    "# both with outliers\n",
    "error at tree height 1 : 1074453.77671 1028431.67887 47.7213981152\n",
    "error at tree height 2 : 1074614.6165 1028587.67404 91.3426940441\n",
    "error at tree height 3 : 1075137.30007 1028913.94437 131.446650028\n",
    "error at tree height 4 : 1074488.31044 1028295.12782 163.675576925\n",
    "error at tree height 5 : 1074212.84323 1027878.85149 188.23348999\n",
    "error at tree height 6 : 1073403.56541 1027236.53432 204.66207695\n",
    "error at tree height 7 : 1072303.28081 1026196.87638 213.713968992\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# both without outliers \n",
    "error at tree height 1 : 761.33458883 764.324273897 48.9493060112\n",
    "error at tree height 2 : 737.88625241 739.985248741 93.26060009\n",
    "error at tree height 3 : 705.815687288 703.871007467 132.270705938\n",
    "error at tree height 4 : 650.272455937 649.6643822 162.958309174\n",
    "error at tree height 5 : 498.819425469 494.899825276 187.905719995\n",
    "error at tree height 6 : 184.136460443 178.974866547 202.772840977\n",
    "error at tree height 7 : 524.930194299 524.008347606 212.766065121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training and testing process\n",
    "\n",
    "# without outliers\n",
    "tmpdta = dta.filter(lambda line: line[0]<1000).takeSample(False, 100000, 12243)\n",
    "dta_train = tmpdta.sample(False, .7, 12345)\n",
    "dta_test = tmpdta.sample(False, .3, 43243)\n",
    "\n",
    "\n",
    "maxdepth=10\n",
    "numFeatures=3\n",
    "bin_num=800\n",
    "\n",
    "def search_nodeToData(features, tree): \n",
    "    nodeNum=len(tree)\n",
    "    if nodeNum == 0:\n",
    "        return 0;\n",
    "    \n",
    "    current_nodeIdx=0\n",
    "    while current_nodeIdx< nodeNum:\n",
    "        split_feature= tree[current_nodeIdx][0]\n",
    "        split_valueSet= tree[current_nodeIdx][1]\n",
    "        \n",
    "        if split_feature==-1:\n",
    "            return -1\n",
    "        \n",
    "        if (( 1<<features[split_feature]) &  split_valueSet) != 0 :\n",
    "            current_nodeIdx=current_nodeIdx*2+1\n",
    "        else:\n",
    "            current_nodeIdx=current_nodeIdx*2+2\n",
    "   \n",
    "    return current_nodeIdx - nodeNum\n",
    "\n",
    "def tree_test_mapFunc(line):\n",
    "    tmpnode = search_nodeToData(line[1:numFeatures+1], nodeSplits_tree)\n",
    "    return (leaf_nodes[ tmpnode ]  - line[0])*(leaf_nodes[ tmpnode ]  - line[0])\n",
    "    \n",
    "def tree_test( testData_rdd ):\n",
    "    err_rdd=testData_rdd.map( lambda line: tree_test_mapFunc(line))\n",
    "    err_sum = err_rdd.reduce( lambda a,b: a+b)\n",
    "    return err_sum / testData_rdd.count()\n",
    "    \n",
    "    \n",
    "#ini parameter on the cluster\n",
    "currentNode_split_fromMaster = sc.broadcast([])\n",
    "test_err = []\n",
    "train_err = []\n",
    "run_time=[]\n",
    "\n",
    "for i in range(7,maxdepth):\n",
    "    nodeSplits_tree=[]\n",
    "    node_tree_test=[]\n",
    "    currentNode_split_fromMaster = sc.broadcast([])\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for cur_depth in range(0,i):\n",
    "        \n",
    "        current_NumNodes= int( math.pow(2,  cur_depth)) \n",
    "        statis_partitions = dta_train.mapPartitions( partition_combiner_hist )\n",
    "        aggre_nodes =statis_partitions.reduceByKey(lambda statis_partition_1, statis_partition_2: \n",
    "                              merge_parttion_combiner_hist(statis_partition_1,statis_partition_2 ))\n",
    "        local_aggre_nodes= aggre_nodes.collect()\n",
    "        cluster_end = time.time()\n",
    "        nodeSplits_layer=find_bestSplit_hist( sorted(local_aggre_nodes,key= lambda val:val[0])\\\n",
    "                                    ,current_NumNodes,nodeSplits_tree,node_tree_test)\n",
    "        #for whole tree\n",
    "        currentNode_split_fromMaster = sc.broadcast(nodeSplits_tree)\n",
    "    \n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    \n",
    "    leaf_nodes = node_tree_test[ len(node_tree_test)-  (int)( math.pow(2,i)): len(node_tree_test)  ]\n",
    "    tmp_test_err= tree_test( dta_test )\n",
    "    tmp_train_err= tree_test( dta_train )\n",
    "    \n",
    "    test_err.append( tmp_test_err)\n",
    "    train_err.append( tmp_train_err )\n",
    "    run_time.append(elapsed)\n",
    "    \n",
    "    print \"error at tree height\", i,\":\",  tmp_test_err, tmp_train_err, elapsed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "error at tree height 1 : 756.01775353 994945.572004 587.657557011\n",
    "error at tree height 2 : 757.452416305 994500.945542 1171.5093379\n",
    "error at tree height 3 : 735.460751283 994502.186792 1758.78520989\n",
    "error at tree height 4 : 672.758858364 994615.267045 2336.85626101\n",
    "error at tree height 5 : 421.654094926 993851.332612 2904.57485199\n",
    "error at tree height 6 : 182.150690359 993910.166512 3450.21704102\n",
    "error at tree height 7 : 704.4328544785163  992832.543051533"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write leaf-nodes results to txt file\n",
    "text_file = open(\"regTree_binNumFixed.txt\", 'a')\n",
    "text_file.write('\\n \\nleaf nodes at depth ')\n",
    "text_file.write(\"%f: \\n\" % maxdepth)\n",
    "for item in leaf_nodes:\n",
    "    text_file.write(\"%f  \" % item)\n",
    "\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "521852\n",
      "1129558\n",
      "1148969\n",
      "514688\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "#debug for distributed information collection\n",
    "\n",
    "numNode = len(local_aggre_nodes)\n",
    "print numNode\n",
    "\n",
    "#check data on all nodes\n",
    "tmpsum=0\n",
    "for i in range(0, numNode):\n",
    "    print local_aggre_nodes[i][1]['count']\n",
    "    tmpsum=tmpsum+local_aggre_nodes[i][1]['count']\n",
    "    \n",
    "if tmpsum != 3315067:\n",
    "    print 'problem in node count'\n",
    "\n",
    "    \n",
    "    \n",
    "tmpsum=0\n",
    "for i in range(0, numNode):\n",
    " \n",
    "    #check node hist\n",
    "    node_count = local_aggre_nodes[i][1]['count']\n",
    "    tmpsum=0\n",
    "    for j in local_aggre_nodes[i][1]['hist'].keys():\n",
    "        tmpsum= tmpsum+local_aggre_nodes[i][1]['hist'][j][2]\n",
    "        \n",
    "    if tmpsum!= node_count:\n",
    "        print '$$$$$ problem in histogram of node'\n",
    "        \n",
    "    print len(local_aggre_nodes[i][1]['hist'])\n",
    "    \n",
    "    #check feature values\n",
    "    for j in range(0, numFeatures):\n",
    "        tmpsum=0\n",
    "        for k in local_aggre_nodes[i][1][j].keys():\n",
    "            value_hist= local_aggre_nodes[i][1][j][k]['hist_inFeatureValue']\n",
    "            value_cnt= local_aggre_nodes[i][1][j][k]['count_inFeatureValue']\n",
    "            tmpsum= tmpsum+value_cnt\n",
    "               \n",
    "            if len(value_hist) > bin_num:\n",
    "                print '?????? bin num wrong', len(value_hist),i,j,k\n",
    "            \n",
    "            \n",
    "            #check feature-value hist\n",
    "            tmpsum1=0\n",
    "            for m in value_hist.keys():\n",
    "                tmpsum1= tmpsum1 + value_hist[m][2]\n",
    "            \n",
    "            if tmpsum1 != value_cnt:\n",
    "                print '+++++++++++ problem in feature-value histogram', tmpsum1, value_cnt\n",
    "            \n",
    "            \n",
    "        if tmpsum != node_count:\n",
    "            print '------problem !!!! in feature:', tmpsum, node_count    \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
